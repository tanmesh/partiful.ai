{"docstore/data":{"14a64f1a-5cfe-428e-89c9-5ac4437e4541":{"indexId":"14a64f1a-5cfe-428e-89c9-5ac4437e4541","nodesDict":{"39bc813f-0812-45a8-a307-dc0f096c8c77":{"id_":"39bc813f-0812-45a8-a307-dc0f096c8c77","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"d79ab6ff-9a7f-4ad3-b3c1-93785db78021","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"smnBMqLDyib3z0LnFMjmrfuM1ecIH9UBvOZrCnVebd0=","text":"Multi Document Agents Setup and Download Data Building Multi Document Agents Running Example Queries Build Document Agent for each Document Build Retriever Enabled OpenAI Agent Define Baseline Vector Store Index  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this guide you learn towards setting up an agent that can effectively answer different types of questions over a larger set of documents These questions include the following QA over a specific doc QA comparing different docs Summaries over a specific doc Comparing summaries between different docs We do this with the following architecture setup a document agent over each Document each doc agent can do QA summarization within its doc setup a top level agent over this set of document agents Do tool retrieval and then do CoT over the set of tools to answer a question In this section we ll define imports and then download Wikipedia articles about different cities Each article is stored separately We load in 18 cities this is not quite at the level of hundreds of documents but its still large enough to warrant some top level document retrieval If you re opening this Notebook on colab you will probably need to install LlamaIndex Define Global LLM and Embeddings In this section we show you how to construct the multi document agent We first build a document agent for each document and then define the top level parent agent with an object index In this section we define document agents for each document We define both a vector index for semantic search and summary index for summarization for each document The two query engines are then converted into tools that are passed to an OpenAI function calling agent This document agent can dynamically choose to perform semantic search or summarization within a given document We create a separate document agent for each city We build a top level agent that can orchestrate across the different document agents to answer any user query This agent takes in all document agents as tools This specific agent RetrieverOpenAIAgent performs tool retrieval before tool use unlike a default agent that tries to put all tools in the prompt Here we use a top k retriever but we encourage you to customize the tool retriever method As a point of comparison we define a naive RAG pipeline which dumps all docs into a single vector index collection We set the top_k 4 Let s run some example queries ranging from QA summaries over a single document to QA summarization over multiple documents","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"59efa8a3-18cb-45c2-92ba-24e056b1aedc":{"id_":"59efa8a3-18cb-45c2-92ba-24e056b1aedc","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/agents.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"4745c382-bc68-47a3-95f7-21416bf7aa2a","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/agents.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"jSkwUuQ0t8jzKFEWxy3Jpwl/S5qAbRpUtv5+g4W35a0=","text":"Agents Agents LlamaIndex Resources LlamaHub   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes An agent is an automated reasoning and decision engine It takes in a user input query and can make internal decisions for executingthat query in order to return the correct result The key agent components can include but are not limited to Breaking down a complex question into smaller ones Choosing an external Tool to use coming up with parameters for calling the Tool Planning out a set of tasks Storing previously completed tasks in a memory module Research developments in LLMs e g ChatGPT Plugins LLM research ReAct Toolformer and LLM tooling LangChain Semantic Kernel have popularized the concept of agents LlamaIndex provides some amazing tools to manage and interact with your data within your LLM application And it is a core tool that you use while building an agent based app On one hand many components within LlamaIndex are agentic these make automated decisions to help a particular use case over your data This ranges from simple reasoning routing to reasoning loops with memory ReAct On the other hand LlamaIndex can be used as a core Tool within another agent framework If you ve built a RAG pipeline already and want to extend it with agentic behavior check out the below resources If you want to check out our standalone documentation hubs on agents and tools check out the following module guides We offer a collection of 40 agent tools for use with your agent in LlamaHub","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"7223b881-7ea0-40ba-bc55-4450610909d1":{"id_":"7223b881-7ea0-40ba-bc55-4450610909d1","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"5a0dc7eb-dc40-47f4-9bfd-815c4fa41be0","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"c53pQgKIvDkzxUGvspfNE5ukaYH8AkHKUTgn+5V4Xgg=","text":"RAG CLI Setup Usage Customization Create a LlamaIndex chat application Supported File Types  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes One common use case is chatting with an LLM about files you have saved locally on your computer We have written a CLI tool to help you do just that You can point the rag CLI tool to a set of files you ve saved locally and it will ingest those files into a local vector database that is then used for a Chat Q A repl within your terminal By default this tool uses OpenAI for the embeddings LLM as well as a local Chroma Vector DB instance Warning this means that by default the local data you ingest with this tool will be sent to OpenAI s API However you do have the ability to customize the models and databases used in this tool This includes the possibility of running all model execution locally See the Customization section below To set up the CLI tool make sure you ve installed the library pip install U llama index You will also need to install Chroma pip install U chromadb After that you can start using the tool Here are some high level steps to get you started Set the OPENAI_API_KEY environment variable By default this tool uses OpenAI s API As such you ll need to ensure the OpenAI API Key is set under the OPENAI_API_KEY environment variable whenever you use the tool Ingest some files Now you need to point the tool at some local files that it can ingest into the local vector database For this example we ll ingest the LlamaIndex README md file You can also specify a file glob pattern such as Ask a Question You can now start asking questions about any of the documents you d ingested in the prior step Open a Chat REPL You can even open a chat interface within your terminal Just run llamaindex cli rag chat and start asking questions about the files you ve ingested You can also create a full stack chat application with a FastAPI backend and NextJS frontend based on the files that you have selected To bootstrap the application make sure you have NodeJS and npx installed on your machine If not please refer to the LlamaIndex TS documentation for instructions Once you have everything set up creating a new application is easy Simply run the following command llamaindex cli rag create llama It will call our create llama tool so you will need to provide several pieces of information to create the app You can find more information about the create llama on npmjs create llama If you choose the option Generate code install dependencies and run the app 2 min all dependencies will be installed and the app will run automatically You can then access the application by going to this address http localhost 3000 Internally the rag CLI tool uses the SimpleDirectoryReader to parse the raw files in your local filesystem into strings This module has custom readers for a wide variety of file types Some of those may require that you pip install another module that is needed for parsing that particular file type If a file type is encountered with a file extension that the SimpleDirectoryReader does not have a custom reader for it will just read the file as a plain text file See the next section for information on how to add your own custom file readers customize other aspects of the CLI tool The rag CLI tool is highly customizable The tool is powered by combining the IngestionPipeline QueryPipeline modules within the RagCLI module To create your own custom rag CLI tool you can simply create a script that instantiates the RagCLI class with a IngestionPipeline QueryPipeline that you ve configured yourself From there you can simply run rag_cli_instance cli in your script to run the same ingestion and Q A commands against your own choice of embedding models LLMs vector DBs etc Here s some high level code to show the general setup From there you re just a few steps away from being able to use your custom CLI script Make sure to replace the python path at the top to the one your virtual environment is using run which python while your virtual environment is activated Let s say you saved your file at path to your script my_rag_cli py From there you can simply modify your shell s configuration file like bashrc or zshrc with a line like export PATH path to your script PATH After that do chmod x my_rag_cli py to give executable permissions to the file That s it You can now just open a new terminal session and run my_rag_cli py h You can now run the script with the same parameters but using your custom code configurations Note you can remove the py file extension from your my_rag_cli py file if you just want to run the command as my_rag_cli chat","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"b0b8a741-a250-4c78-a72e-ce5d11036536":{"id_":"b0b8a741-a250-4c78-a72e-ce5d11036536","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/root.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"ddf6a31c-3074-466b-a658-67fe834230fa","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/root.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"rIZhZfZ1NqgoPTICBq3AI/712WCzI7fnx8AVimKH9yg=","text":"Q A Types of question answering use cases Further examples What to do Where to search How to search  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes One of the most common use cases for an LLM application is to answer questions about a set of documents LlamaIndex has rich support for many forms of question answering Q A has all sorts of sub types such as Semantic search finding data that matches not just your query terms but your intent and the meaning behind your question This is sometimes known as top k search Example of semantic search Summarization condensing a large amount of data into a short summary relevant to your current question Example of summarization Over documents LlamaIndex can pull in unstructured text PDFs Notion and Slack documents and more and index the data within them Example of search over documents Building a multi document agent over the LlamaIndex docs Over structured data if your data already exists in a SQL database as JSON or as any number of other structured formats LlamaIndex can query the data in these sources Searching Pandas tables Text to SQL Combine multiple sources is some of your data in Slack some in PDFs some in unstructured text LlamaIndex can combine queries across an arbitrary number of sources and combine them Example of combining multiple sources Route across multiple sources given multiple data sources your application can first pick the best source and then route the question to that source Example of routing across multiple sources Multi document queries some questions have partial answers in multiple data sources which need to be questioned separately before they can be combined Example of multi document queries For further examples of Q A use cases see our Q A section in Putting it All Together","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"52df1666-9cf6-4cbe-9fea-6ee81ac860f5":{"id_":"52df1666-9cf6-4cbe-9fea-6ee81ac860f5","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/discover_llamaindex.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"9cf2f0cc-6f9e-4e01-b9de-d619c68ae614","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/discover_llamaindex.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"aUaqi/z3Bqh8JlSZlGNsI8KQByvD69HURhidu9z4ZP4=","text":"Discover LlamaIndex Video Series Bottoms Up Development Llama Docs Bot SubQuestionQueryEngine 10K Analysis Discord Document Management Joint Text to SQL and Semantic Search   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes If you like learning from videos now s a good time to check out our Discover LlamaIndex series If not we recommend heading on to our Understanding LlamaIndex tutorial This is a sub series within Discover LlamaIndex that shows you how to build a document chatbot from scratch We show you how to do this in a bottoms up fashion start by using the LLMs and data objects as independent modules Then gradually add higher level abstractions like indexing and advanced retrievers rerankers Full Repo Part 1 LLMs and Prompts Part 2 Documents and Metadata Part 3 Evaluation Part 4 Embeddings Part 5 Retrievers and Postprocessors This video covers the SubQuestionQueryEngine and how it can be applied to financial documents to help decompose complex queries into multiple sub questions Youtube Notebook This video covers managing documents from a source that is constantly updating i e Discord and how you can avoid document duplication and save embedding tokens Youtube Notebook and Supplementary Material Reference Docs This video covers the tools built into LlamaIndex for combining SQL and semantic search into a single unified query interface Youtube Notebook","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"27a8ef01-34bf-4686-9398-09f6f4ac1f85":{"id_":"27a8ef01-34bf-4686-9398-09f6f4ac1f85","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"4b07dedb-9b13-4b5e-bc96-70f4297f2ab3","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"sqJjLgiKc+t3ip0WiZtBFAfs1iVCDBiqi4yLm83wuqA=","text":"Agents Concept Usage Pattern Modules Reasoning Loop Tool Abstractions Blog Post Lower level API Step Wise Execution  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Data Agents are LLM powered knowledge workers in LlamaIndex that can intelligently perform various tasks over your data in both a read and write function They are capable of the following Perform automated search and retrieval over different types of data unstructured semi structured and structured Calling any external service API in a structured fashion and processing the response storing it for later In that sense agents are a step beyond our query engines in that they can not only read from a static source of data but can dynamically ingest and modify data from a variety of different tools Building a data agent requires the following core components A reasoning loop Tool abstractions A data agent is initialized with set of APIs or Tools to interact with these APIs can be called by the agent to return information or modify state Given an input task the data agent uses a reasoning loop to decide which tools to use in which sequence and the parameters to call each tool The reasoning loop depends on the type of agent We have support for the following agents OpenAI Function agent built on top of the OpenAI Function API a ReAct agent which works across any chat text completion endpoint a LLMCompiler Agent available as a LlamaPack source repo You can learn more about our Tool abstractions in our Tools section For full details please check out our detailed blog post By default our agents expose query and chat functions that will execute a user query end to end We also offer a lower level API allowing you to perform step wise execution of an agent This gives you much more control in being able to create tasks and analyze act upon the input output of each step within a task Check out our guide Data agents can be used in the following manner the example uses the OpenAI Function API See our usage pattern guide for more details Learn more about our different agent types and use cases in our module guides below Also take a look at our tools section","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"618b120e-aa18-4dc3-a597-e51fe00b815f":{"id_":"618b120e-aa18-4dc3-a597-e51fe00b815f","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_with_query_engine.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"929b9b4d-08a2-4be9-a56f-2e68bbc03b97","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_with_query_engine.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"ujUkicLWiosiE6LqvduSwNo+0Z86Hi4sZLjjV+kBB0Q=","text":"OpenAI Agent with Query Engine Tools Build Query Engine Tools Setup OpenAI Agent Let s Try It Out   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  If you re opening this Notebook on colab you will probably need to install LlamaIndex Download Data","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"acaac93a-55c9-424f-8c67-d925df5d26b1":{"id_":"acaac93a-55c9-424f-8c67-d925df5d26b1","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_parallel_function_calling.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"50b750c7-c9e6-43cf-bd67-fab68fcccd39","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_parallel_function_calling.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"zEf4w0brbY/5INsrZMBQrU+evb36PW/Dmn8cc+gNov0=","text":"Single Turn Multi Function Calling OpenAI Agents Setup Sync mode Async mode Example from OpenAI docs Conclusion   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  With the latest OpenAI API v 1 1 0 users can now execute multiple function calls within a single turn of User and Agent dialogue We ve updated our library to enable this new feature as well and in this notebook we ll show you how it all works NOTE OpenAI refers to this as Parallel function calling but the current implementation doesn t invoke parallel computations of the multiple function calls So it s parallelizable function calling in terms of our current implementation If you ve seen any of our previous notebooks on OpenAI Agents then you re already familiar with the cookbook recipe that we have to follow here But if not or if you fancy a refresher then all we need to do at a high level are the following steps Define a set of tools we ll use FunctionTool since Agents work with tools Define the LLM for the Agent Define a OpenAIAgent Here s an example straight from the OpenAI docs on Parallel function calling Their example gets this done in 76 lines of code whereas with the llama_index library you can get that down to about 18 lines All of the above function calls that the Agent has done above were in a single turn of dialogue between the Assistant and the User What s interesting is that an older version of GPT 3 5 is not quite advanced enough compared to is successor it will do the above task in 3 separate turns For the sake of demonstration here it is below And so as you can see the llama_index library can handle multiple function calls as well as a single function call within a single turn of dialogue between the user and the OpenAI agent","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"abd55638-e8de-4f52-95ed-4b3c51f4597d":{"id_":"abd55638-e8de-4f52-95ed-4b3c51f4597d","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/SQLRouterQueryEngine.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"89b1521e-2f49-4c85-93d5-76fb685aeb78","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/SQLRouterQueryEngine.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"kjQjUUrCwwA/DSwRQmk3A/AzoErDBNEGuCZGzyGiXKw=","text":"SQL Router Query Engine Setup Create Database Schema Test Data Load Data Build SQL Index Build Vector Index Define Query Engines Set as Tools Define Router Query Engine   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this tutorial we define a custom router query engine that can route to either a SQL database or a vector database If you re opening this Notebook on colab you will probably need to install LlamaIndex Here we introduce a toy scenario where there are 100 tables too big to fit into the prompt We introduce some test data into the city_stats table We first show how to convert a Document into a set of Nodes and insert into a DocumentStore","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"93564f63-bb7f-45a0-8e8a-d9d24c40a758":{"id_":"93564f63-bb7f-45a0-8e8a-d9d24c40a758","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/RouterQueryEngine.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"33447f73-2492-4819-abe9-726a105dda83","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/RouterQueryEngine.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"61vM70NI5ciOJfHN/uHRe+VJoPdKK0Jk/h4BAlb6xck=","text":"Router Query Engine Setup Global Models Load Data Define Summary Index and Vector Index over Same Data Define Query Engines and Set Metadata Define Router Query Engine PydanticSingleSelector LLMSingleSelector PydanticMultiSelector Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this tutorial we define a custom router query engine that selects one out of several candidate query engines to execute a query If you re opening this Notebook on colab you will probably need to install LlamaIndex We first show how to convert a Document into a set of Nodes and insert into a DocumentStore There are several selectors available each with some distinct attributes The LLM selectors use the LLM to output a JSON that is parsed and the corresponding indexes are queried The Pydantic selectors currently only supported by gpt 4 0613 and gpt 3 5 turbo 0613 the default use the OpenAI Function Call API to produce pydantic selection objects rather than parsing raw JSON For each type of selector there is also the option to select 1 index to route to or multiple Use the OpenAI Function API to generate parse pydantic objects under the hood for the router selector Use OpenAI or any other LLM to parse generated JSON under the hood to select a sub index for routing In case you are expecting queries to be routed to multiple indexes you should use a multi selector The multi selector sends to query to multiple sub indexes and then aggregates all responses using a summary index to form a complete answer","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"69c89dec-5e4d-4f49-8779-f55fda48f770":{"id_":"69c89dec-5e4d-4f49-8779-f55fda48f770","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/retrievers/router_retriever.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"2b8a5f3e-c814-46c1-ba89-ddb5bad69217","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/retrievers/router_retriever.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"AXIszHXYcsBCqZqn8KnmUKiWCgGo5HMbLhtUsscvbMk=","text":"Router Retriever Setup Download Data Load Data Define Selector Module for Routing PydanticSingleSelector PydanticMultiSelector  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this guide we define a custom router retriever that selects one or more candidate retrievers in order to execute a given query The router BaseSelector module uses the LLM to dynamically make decisions on which underlying retrieval tools to use This can be helpful to select one out of a diverse range of data sources This can also be helpful to aggregate retrieval results across a variety of data sources if a multi selector module is used This notebook is very similar to the RouterQueryEngine notebook If you re opening this Notebook on colab you will probably need to install LlamaIndex We first show how to convert a Document into a set of Nodes and insert into a DocumentStore There are several selectors available each with some distinct attributes The LLM selectors use the LLM to output a JSON that is parsed and the corresponding indexes are queried The Pydantic selectors currently only supported by gpt 4 0613 and gpt 3 5 turbo 0613 the default use the OpenAI Function Call API to produce pydantic selection objects rather than parsing raw JSON Here we use PydanticSingleSelector PydanticMultiSelector but you can use the LLM equivalents as well Node ID 7d07d325 489e 4157 a745 270e2066a643Similarity NoneText What I Worked On February 2021 Before college the two main things I worked on outside of schoo Node ID 01f0900b db83 450b a088 0473f16882d7Similarity NoneText showed Terry Winograd using SHRDLU I haven t tried rereading The Moon is a Harsh Mistress so I Node ID b2549a68 5fef 4179 b027 620ebfa6e346Similarity NoneText Science is an uneasy alliance between two halves theory and systems The theory people prove thi Node ID 4f1e9f0d 9bc6 4169 b3b6 4f169bbfa391Similarity NoneText been explored But all I wanted was to get out of grad school and my rapidly written dissertatio Node ID e20c99f9 5e80 4c92 8cc0 03d2a527131eSimilarity NoneText stop there of course or you get merely photographic accuracy and what makes a still life inter Node ID dbdf341a f340 49f9 961f 16b9a51eea2dSimilarity NoneText that big bureaucratic customers are a dangerous source of money and that there s not much overl Node ID ed341d3a 9dda 49c1 8611 0ab40d04f08aSimilarity NoneText about money because I could sense that Interleaf was on the way down Freelance Lisp hacking wor Node ID d69e02d3 2732 4567 a360 893c14ae157bSimilarity NoneText a web app is common now but at the time it wasn t clear that it was even possible To find out Node ID df9e00a5 e795 40a1 9a6b 8184d1b1e7c0Similarity NoneText have to integrate with any other software except Robert s and Trevor s so it was quite fun to wo Node ID 38f2699b 0878 499b 90ee 821cb77e387bSimilarity NoneText all too keenly aware of the near death experiences we seemed to have every few months Nor had I Node ID be04d6a9 1fc7 4209 9df2 9c17a453699aSimilarity NoneText for a second still life painted from the same objects which hopefully hadn t rotted yet Mean Node ID 42344911 8a7c 4e9b 81a8 0fcf40ab7690Similarity NoneText which I d created years before using Viaweb but had never used for anything In one day it got 30 Node ID 9ec3df49 abf9 47f4 b0c2 16687882742aSimilarity NoneText I didn t know but would turn out to like a lot a woman called Jessica Livingston A couple days Node ID d0cf6975 5261 4fb2 aae3 f3230090fb64Similarity NoneText of readers but professional investors are thinking Wow that means they got all the returns B Node ID 607d0480 7eee 4fb4 965d 3cb585fda62cSimilarity NoneText to the YC GDP but as YC grows this becomes less and less of a joke Now lots of startups get t Node ID 730a49c9 55f7 4416 ab91 1d0c96e704c8Similarity NoneText So this set me thinking It was true that on my current trajectory YC would be the last thing I Node ID edbe8c67 e373 42bf af98 276b559cc08bSimilarity NoneText operators you need The Lisp that John McCarthy invented or more accurately discovered is an an Node ID 175a4375 35ec 45a0 a90c 15611505096bSimilarity NoneText Like McCarthy s original Lisp it s a spec rather than an implementation although like McCarthy Node ID 0cb367f9 0aac 422b 9243 0eaa7be15090Similarity NoneText must tell readers things they don t already know and some people dislike being told such things Node ID 67afd4f1 9fa1 4e76 87ac 23b115823e6cSimilarity NoneText 1960 paper But if so there s no reason to suppose that this is the limit of the language that m Node ID 22d20835 7de6 4cf7 92de 2bee339f3157Similarity 0 8017176790752668Text that big bureaucratic customers are a dangerous source of money and that there s not much overl Node ID bf818c58 5d5b 4458 acbc d87cc67a36caSimilarity 0 7935885352785799Text So this set me thinking It was true that on my current trajectory YC would be the last thing I Node ID fbdd25ed 1ecb 4528 88da 34f581c30782Similarity NoneText So this set me thinking It was true that on my current trajectory YC would be the last thing I Node ID 4ce91b17 131f 4155 b7b5 8917cdc612b1Similarity NoneText to the YC GDP but as YC grows this becomes less and less of a joke Now lots of startups get t Node ID 9fe6c152 28d4 4006 8a1a 43bb72655438Similarity NoneText stop there of course or you get merely photographic accuracy and what makes a still life inter Node ID d11cd2e2 1dd2 4c3b 863f 246fe3856f49Similarity NoneText of readers but professional investors are thinking Wow that means they got all the returns B Node ID 2bfbab04 cb71 4641 9bd9 52c75b3a9250Similarity NoneText must tell readers things they don t already know and some people dislike being told such things Node ID 49882a2c bb95 4ff3 9df1 2a40ddaea408Similarity NoneText So this set me thinking It was true that on my current trajectory YC would be the last thing I Node ID d11aced1 e630 4109 8ec8 194e975b9851Similarity NoneText to the YC GDP but as YC grows this becomes less and less of a joke Now lots of startups get t Node ID 8aa6cc91 8e9c 4470 b6d5 4360ed13fefdSimilarity NoneText stop there of course or you get merely photographic accuracy and what makes a still life inter Node ID e37465de c79a 4714 a402 fbd5f52800a2Similarity NoneText must tell readers things they don t already know and some people dislike being told such things Node ID e0ac7fb6 84fc 4763 bca6 b68f300ec7b7Similarity NoneText of readers but professional investors are thinking Wow that means they got all the returns B Node ID 76d76348 52fb 49e6 95b8 2f7a3900fa1aSimilarity NoneText So this set me thinking It was true that on my current trajectory YC would be the last thing I Node ID 61e1908a 79d2 426b 840e 926df469ac49Similarity NoneText to the YC GDP but as YC grows this becomes less and less of a joke Now lots of startups get t Node ID cac03004 5c02 4145 8e92 c320b1803847Similarity NoneText stop there of course or you get merely photographic accuracy and what makes a still life inter Node I","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"1e8aa04f-a57d-44c7-a3ba-d59f05af21a0":{"id_":"1e8aa04f-a57d-44c7-a3ba-d59f05af21a0","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/retrievers/router_retriever.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"aaaa6c3a-e280-42d0-92a2-d58be98fd78c","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/retrievers/router_retriever.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"+s+rLwA/PkF6JIP1X/J84nPcUV/zpsFopVjABATYhG4=","text":"D f0d55e5e 5349 4243 ab01 d9dd7b12cd0aSimilarity NoneText of readers but professional investors are thinking Wow that means they got all the returns B Node ID 1516923c 0dee 4af2 b042 3e1f38de7e86Similarity NoneText must tell readers things they don t already know and some people dislike being told such things","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"8197173a-c449-4d8d-9dc0-1e647cc68e0e":{"id_":"8197173a-c449-4d8d-9dc0-1e647cc68e0e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"7a8662dc-82c3-4f1a-a682-2bb4c51807e3","metadata":{"url":"https://docs.llamaindex.ai/en/stable/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"esG0skGjdXp4RmP3VFB5O+cnXotQDPq6hUJyYdG+NAM=","text":"Welcome to LlamaIndex Why Context Augmentation Why LlamaIndex for Context Augmentation Who is LlamaIndex for Getting Started Ecosystem Community Associated projects  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes LlamaIndex is a data framework for LLM based applications which benefit from context augmentation Such LLM systems have been termed as RAG systems standing for Retrieval Augemented Generation LlamaIndex provides the essential abstractions to more easily ingest structure and access private or domain specific data in order to inject these safely and reliably into LLMs for more accurate text generation It s available in Python these docs and Typescript Tip Updating to LlamaIndex v0 10 0 Check out the migration guide LLMs offer a natural language interface between humans and data Widely available models come pre trained on huge amounts of publicly available data like Wikipedia mailing lists textbooks source code and more However while LLMs are trained on a great deal of data they are not trained on your data which may be private or specific to the problem you re trying to solve It s behind APIs in SQL databases or trapped in PDFs and slide decks You may choose to fine tune a LLM with your data but Training a LLM is expensive Due to the cost to train it s hard to update a LLM with latest information Observability is lacking When you ask a LLM a question it s not obvious how the LLM arrived at its answer Instead of fine tuning one can a context augmentation pattern called Retrieval Augmented Generation RAG to obtain more accurate text generation relevant to your specific data RAG involves the following high level steps Retrieve information from your data sources first Add it to your question as context and Ask the LLM to answer based on the enriched prompt In doing so RAG overcomes all three weaknesses of the fine tuning approach There s no training involved so it s cheap Data is fetched only when you ask for them so it s always up to date LlamaIndex can show you the retrieved documents so it s more trustworthy Firstly LlamaIndex imposes no restriction on how you use LLMs You can still use LLMs as auto complete chatbots semi autonomous agents and more see Use Cases on the left It only makes LLMs more relevant to you LlamaIndex provides the following tools to help you quickly standup production ready RAG systems Data connectors ingest your existing data from their native source and format These could be APIs PDFs SQL and much more Data indexes structure your data in intermediate representations that are easy and performant for LLMs to consume Engines provide natural language access to your data For example Query engines are powerful retrieval interfaces for knowledge augmented output Chat engines are conversational interfaces for multi message back and forth interactions with your data Data agents are LLM powered knowledge workers augmented by tools from simple helper functions to API integrations and more Application integrations tie LlamaIndex back into the rest of your ecosystem This could be LangChain Flask Docker ChatGPT or anything else LlamaIndex provides tools for beginners advanced users and everyone in between Our high level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code For more complex applications our lower level APIs allow advanced users to customize and extend any module data connectors indices retrievers query engines reranking modules to fit their needs To install the library pip install llama index We recommend starting at how to read these docs which will point you to the right place based on your experience level To download or contribute find LlamaIndex on Github https github com jerryjliu llama_index PyPi LlamaIndex https pypi org project llama index GPT Index duplicate https pypi org project gpt index Github https github com run llama LlamaIndexTS Docs https ts llamaindex ai LlamaIndex TS https www npmjs com package llamaindex Need help Have a feature suggestion Join the LlamaIndex community Twitter https twitter com llama_index Discord https discord gg dGcwcsnxhU LlamaHub https llamahub ai A large and growing collection of custom data connectors LlamaLab https github com run llama llama lab Ambitious projects built on top of LlamaIndex","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"d0e93c99-fbba-4303-a7da-214e5c655950":{"id_":"d0e93c99-fbba-4303-a7da-214e5c655950","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/llamahub_tools_guide.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"0dded53f-7ebb-48c0-b503-6535b5961c33","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/llamahub_tools_guide.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"HQzlp35t7+2Y6rHOHoqfH0d4SSH3M77q92XNjzgSPgc=","text":"LlamaHub Tools Guide Tool Specs Utility Tools OnDemandLoaderTool LoadAndSearchToolSpec  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes We offer a rich set of Tool Specs that are offered through LlamaHub These tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions We also provide a list of utility tools that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data Coming soon Oftentimes directly querying an API can return a massive volume of data which on its own may overflow the context window of the LLM or at the very least unnecessarily increase the number of tokens that you are using To tackle this we ve provided an initial set of utility tools in LlamaHub Tools utility tools are not conceptually tied to a given service e g Gmail Notion but rather can augment the capabilities of existing Tools In this particular case utility tools help to abstract away common patterns of needing to cache index and query data that s returned from any API request Let s walk through our two main utility tools below This tool turns any existing LlamaIndex data loader BaseReader class into a tool that an agent can use The tool can be called with all the parameters needed to trigger load_data from the data loader along with a natural language query string During execution we first load data from the data loader index it for instance with a vector store and then query it on demand All three of these steps happen in a single tool call Oftentimes this can be preferable to figuring out how to load and index API data yourself While this may allow for data reusability oftentimes users just need an ad hoc index to abstract away prompt window limitations for any API call A usage example is given below The LoadAndSearchToolSpec takes in any existing Tool as input As a tool spec it implements to_tool_list and when that function is called two tools are returned a load tool and then a search tool The load Tool execution would call the underlying Tool and the index the output by default with a vector index The search Tool execution would take in a query string as input and call the underlying index This is helpful for any API endpoint that will by default return large volumes of data for instance our WikipediaToolSpec will by default return entire Wikipedia pages which will easily overflow most LLM context windows Example usage is shown below","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"f48183d0-225d-4144-8a97-d0b5962fb2ff":{"id_":"f48183d0-225d-4144-8a97-d0b5962fb2ff","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/customization.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b124f2d8-b56a-4cf0-99f6-8bc1ce20b0e4","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/customization.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"mZ0z7y4W/2XX0HV9V0btPYFL1iDWncAo7gEQAD2fRz0=","text":"Customization Tutorial    Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Tip If you haven t already install LlamaIndex and complete the starter tutorial If you run into terms you don t recognize check out the high level concepts In this tutorial we start with the code you wrote for the starter example and show you the most common ways you might want to customize it for your use case I want to parse my documents into smaller chunks I want to use a different vector store First you can install the vector store you want to use For example to use chromadb as the vector store you can install it using pip To learn more about all integrations available checkout LlamaHub https llamahub ai _ Then you can use it in your code StorageContext defines the storage backend for where the documents embeddings and indexes are stored You can learn more about storage and how to customize it I want to retrieve more context when I query as_query_engine builds a default retriever and query engine on top of the index You can configure the retriever and query engine by passing in keyword arguments Here we configure the retriever to return the top 5 most similar documents instead of the default of 2 You can learn more about retrievers and query engines I want to use a different LLM You can learn more about customizing LLMs I want to use a different response mode You can learn more about query engines and response modes I want to stream the response back You can learn more about streaming responses I want a chatbot instead of Q A Learn more about the chat engine Next Steps want a thorough walkthrough of almost everything you can configure Get started with Understanding LlamaIndex want more in depth understanding of specific modules Check out the module guides in the left nav","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"3c4e45b0-a0a9-4cbb-a23b-354e408a61ad":{"id_":"3c4e45b0-a0a9-4cbb-a23b-354e408a61ad","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/RetrieverRouterQueryEngine.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"97353d28-02da-4e15-ae65-0297c15a08ad","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/RetrieverRouterQueryEngine.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"paJbpOrsGiTBMreyxG6OkcFMIi2DVsAeK/irrSYVxwU=","text":"Retriever Router Query Engine Setup Load Data Define Summary Index and Vector Index over Same Data Define Query Engine and Tool for these Indices Define Retrieval Augmented Router Query Engine   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this tutorial we define a router query engine based on a retriever The retriever will select a set of nodes and we will in turn select the right QueryEngine We use our new ToolRetrieverRouterQueryEngine class for this If you re opening this Notebook on colab you will probably need to install LlamaIndex Download Data We first show how to convert a Document into a set of Nodes and insert into a DocumentStore We define a Query Engine for each Index We then wrap these with our QueryEngineTool We define a router query engine that s augmented with a retrieval mechanism to help deal with the case when the set of choices is too large To do this we first define an ObjectIndex over the set of query engine tools The ObjectIndex is defined an underlying index data structure e g a vector index keyword index and can serialize QueryEngineTool objects to from our indices We then use our ToolRetrieverRouterQueryEngine class and pass in an ObjectRetriever over QueryEngineTool objects The ObjectRetriever corresponds to our ObjectIndex This retriever can then dyamically retrieve the relevant query engines during query time This allows us to pass in an arbitrary number of query engine tools without worrying about prompt limitations","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"202afe6f-8468-48c8-a026-b6c54dd908a1":{"id_":"202afe6f-8468-48c8-a026-b6c54dd908a1","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"8a151027-02bc-405f-be72-76b695e7fb7d","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"sRCP2qgZCrFyR6KKNxjzwGarJH95RqmyUvRDOGl17EY=","text":"Build your own OpenAI Agent Initial Setup Agent Definition Let s Try It Out Our Slightly Better OpenAIAgent Implementation Chat Async Chat Streaming Chat Async Streaming Chat Agent with Personality  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  With the new OpenAI API that supports function calling it s never been easier to build your own agent In this notebook tutorial we showcase how to write your own OpenAI agent in under 50 lines of code It is minimal yet feature complete with ability to carry on a conversation and use tools Let s start by importing some simple building blocks The main thing we need is the OpenAI API using our own llama_index LLM class a place to keep conversation history a definition for tools that our agent can use If you re opening this Notebook on colab you will probably need to install LlamaIndex Let s define some very simple calculator tools for our agent Now we define our agent that s capable of holding a conversation and calling tools in under 50 lines of code The meat of the agent logic is in the chat method At a high level there are 3 steps Call OpenAI to decide which tool if any to call and with what arguments Call the tool with the arguments to obtain an output Call OpenAI to synthesize a response from the conversation context and the tool output The reset method simply resets the conversation context so we can start another conversation We provide a slightly better OpenAIAgent implementation in LlamaIndex which you can directly use as follows In comparison to the simplified version above it implements the BaseChatEngine and BaseQueryEngine interface so you can more seamlessly use it in the LlamaIndex framework it supports multiple function calls per conversation turn it supports streaming it supports async endpoints it supports callback and tracing Here every LLM response is returned as a generator You can stream every incremental step or only the last response You can specify a system prompt to give the agent additional instruction or personality","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"55e135bc-e8ec-4b3a-86a8-f70b79b28278":{"id_":"55e135bc-e8ec-4b3a-86a8-f70b79b28278","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"edebb390-c99d-4373-b97b-675e68c03b5e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"AI6vE9951tW7V90grhITa1EiBkLrWktVFzFWQ/BDva8=","text":"HyDE Query Transform Download Data Example HyDE improves specific temporal queries Failure case 1 HyDE may mislead when query can be mis interpreted without context Failure case 2 HyDE may bias open ended queries Load documents build the VectorStoreIndex First we query without transformation The same query string is used for embedding lookup and also summarization Now we use HyDEQueryTransform to generate a hypothetical document and use it for embedding lookup In this example HyDE improves output quality significantly by hallucinating accurately what Paul Graham did after RISD see below and thus improving the embedding quality and final output Querying without transformation yields reasonable answer Querying without transformation yields a reasonable answer Querying with HyDEQueryTransform results in a more biased output Querying with HyDEQueryTransform results in nonsense In this example HyDE mis interprets Bel without document context see below resulting in a completely unrelated embedding string and poor retrieval outcome Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  If you re opening this Notebook on colab you will probably need to install LlamaIndex After going to RISD Paul Graham continued to pursue his passion for painting and art He took classes in the painting department at the Accademia di Belli Arti in Florence and he also took the entrance exam for the school He also continued to work on his book On Lisp and he took on consulting work to make money At the school Paul Graham and the other students had an arrangement where the faculty wouldn t require the students to learn anything and in return the students wouldn t require the faculty to teach anything Paul Graham was one of the few students who actually painted the nude model that was provided while the rest of the students spent their time chatting or occasionally trying to imitate things they d seen in American art magazines The model turned out to live just down the street from Paul Graham and she made a living from a combination of modelling and making fakes for a local antique dealer After going to RISD Paul Graham worked as a consultant for Interleaf and then co founded Viaweb with Robert Morris They created a software that allowed users to build websites via the web and received 10 000 in seed funding from Idelle s husband Julian They gave Julian 10 of the company in return for the initial legal work and business advice Paul Graham had a negative net worth due to taxes he owed so the seed funding was necessary for him to live on They opened for business in January 1996 with 6 stores Paul Graham then left Yahoo after his options vested and went back to New York He resumed his old life but now he was rich He tried to paint but he didn t have much energy or ambition He eventually moved back to Cambridge and started working on a web app for making web apps He recruited Dan Giffin and two undergrads to help him but he eventually realized he didn t want to run a company and decided to build a subset of the project as an open source project He and Dan worked on a new dialect of Lisp which he called Arc in a house he bought in Cambridge The subset he built as an open source project was the new Lisp whose After graduating from the Rhode Island School of Design RISD in 1985 Paul Graham went on to pursue a career in computer programming He worked as a software developer for several companies including Viaweb which he co founded in 1995 Viaweb was eventually acquired by Yahoo in 1998 and Graham used the proceeds to become a venture capitalist He founded Y Combinator in 2005 a startup accelerator that has helped launch over 2 000 companies including Dropbox Airbnb and Reddit Graham has also written several books on programming and startups and he continues to be an active investor in the tech industry Bel is a programming language that was written in Arc by Paul Graham over the course of four years March 26 2015 to October 12 2019 It is based on John McCarthy s original Lisp but with additional features added It is a spec expressed as code and is meant to be a formal model of computation an alternative to the Turing machine Bel is the pseudonym of Paul Graham the author of the context information who was in need of seed funding to live on and was part of a deal that became the model for Y Combinator s Bel is an ancient Semitic god originating from the Middle East He is often associated with the sun and is sometimes referred to as the Lord of Heaven Bel is also known as the god of fertility abundance and prosperity He is often depicted as a bull or a man with a bull s head In some cultures Bel is seen as a creator god responsible for the creation of the universe He is also associated with the underworld and is sometimes seen as a god of death Bel is also associated with justice and is often seen as a protector of the innocent Bel is an important figure in many religions including Judaism Christianity and Islam The author would likely say that art and engineering are two different disciplines that require different skills and approaches Art is more focused on expression and creativity while engineering is more focused on problem solving and technical knowledge The author also suggests that art school does not always provide the same level of rigor as engineering school and that painting students are often encouraged to develop a signature style rather than learn the fundamentals of painting Furthermore the author would likely point out that engineering can provide more financial stability than art as evidenced by the author s own experience of needing seed funding to live on while launching a company The author would likely say that art is a more lasting and independent form of work than engineering They mention that software written today will be obsolete in a couple decades and that systems work does not last In contrast they note that paintings can last hundreds of years and that it is possible to make a living as an artist They also mention that as an artist you can be truly independent and don t need to have a boss or research funding Furthermore they note that art can be a source of income for people who may not have access to traditional forms of employment such as the model in the example who was able to make a living from modelling and making fakes for a local antique dealer","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"d74c58a6-5e27-4e68-9cff-5597814d2139":{"id_":"d74c58a6-5e27-4e68-9cff-5597814d2139","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_cookbook.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"610f5d53-81a6-40a7-aace-ce08399bb843","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_cookbook.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"Vwke6zAfs0Z70qMScd2+jVv25I6Qlp7UOtOD8OmScO4=","text":"OpenAI Agent Query Engine Experimental Cookbook AutoRetrieval from a Vector Database Joint Text to SQL and Semantic Search Define Function Tool Initialize Agent Load and Index Structured Data Load and Index Unstructured Data Define Query Engines Tools Initialize Agent  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we try out the OpenAIAgent across a variety of query engine tools and datasets We explore how OpenAIAgent can compare replace existing workflows solved by our retrievers query engines Auto retrieval Joint SQL and vector search Our existing auto retrieval capabilities in VectorIndexAutoRetriever allow an LLM to infer the right query parameters for a vector database including both the query string and metadata filter Since the OpenAI Function API can infer function parameters we explore its capabilities in performing auto retrieval here If you re opening this Notebook on colab you will probably need to install LlamaIndex Here we define the function interface which is passed to OpenAI to perform auto retrieval We were not able to get OpenAI to work with nested pydantic objects or tuples as arguments so we converted the metadata filter keys and values into lists for the function API to work with Define AutoRetrieve Functions This is currently handled by our SQLAutoVectorQueryEngine Let s try implementing this by giving our OpenAIAgent access to two query tools SQL and Vector We load sample structured datapoints into a SQL db and index it We load unstructured data into a vector index backed by Pinecone","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"84a1476c-dd5c-4ec7-ae28-ac0c0c0353c2":{"id_":"84a1476c-dd5c-4ec7-ae28-ac0c0c0353c2","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_parallel_function_calling.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"a4e24e76-649c-4b2d-8f6d-90e0651a1d2b","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_parallel_function_calling.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"zEf4w0brbY/5INsrZMBQrU+evb36PW/Dmn8cc+gNov0=","text":"Single Turn Multi Function Calling OpenAI Agents Setup Sync mode Async mode Example from OpenAI docs Conclusion   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  With the latest OpenAI API v 1 1 0 users can now execute multiple function calls within a single turn of User and Agent dialogue We ve updated our library to enable this new feature as well and in this notebook we ll show you how it all works NOTE OpenAI refers to this as Parallel function calling but the current implementation doesn t invoke parallel computations of the multiple function calls So it s parallelizable function calling in terms of our current implementation If you ve seen any of our previous notebooks on OpenAI Agents then you re already familiar with the cookbook recipe that we have to follow here But if not or if you fancy a refresher then all we need to do at a high level are the following steps Define a set of tools we ll use FunctionTool since Agents work with tools Define the LLM for the Agent Define a OpenAIAgent Here s an example straight from the OpenAI docs on Parallel function calling Their example gets this done in 76 lines of code whereas with the llama_index library you can get that down to about 18 lines All of the above function calls that the Agent has done above were in a single turn of dialogue between the Assistant and the User What s interesting is that an older version of GPT 3 5 is not quite advanced enough compared to is successor it will do the above task in 3 separate turns For the sake of demonstration here it is below And so as you can see the llama_index library can handle multiple function calls as well as a single function call within a single turn of dialogue between the user and the OpenAI agent","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"b8815ac3-a0cb-48a3-8724-899749bb3372":{"id_":"b8815ac3-a0cb-48a3-8724-899749bb3372","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_assistant_query_cookbook.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"0882806c-4308-4386-9117-140e5cdeed41","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_assistant_query_cookbook.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"MQvot4SlEHnkDqg4u+o4bKka6pQbC2rJuWrFm+DCI8g=","text":"OpenAI Assistant Advanced Retrieval Cookbook Joint QA and Summarization AutoRetrieval from a Vector Database Joint Text to SQL and Semantic Search Load Data Setup Vector Summary Indexes Query Engines Tools Define Assistant Agent Define Function Tool Initialize Agent Load and Index Structured Data Load and Index Unstructured Data Define Query Engines Tools Initialize Agent Results A bit flaky Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we try out OpenAI Assistant API for advanced retrieval tasks by plugging in a variety of query engine tools and datasets The wrapper abstraction we use is our OpenAIAssistantAgent class which allows us to plug in custom tools We explore how OpenAIAssistant can complement replace existing workflows solved by our retrievers query engines through its agent execution function calling loop Joint QA Summarization Auto retrieval Joint SQL and vector search In this section we show how we can get the Assistant agent to both answer fact based questions and summarization questions This is something that the in house retrieval tool struggles to accomplish Our existing auto retrieval capabilities in VectorIndexAutoRetriever allow an LLM to infer the right query parameters for a vector database including both the query string and metadata filter Since the Assistant API can call functions infer function parameters we explore its capabilities in performing auto retrieval here If you re opening this Notebook on colab you will probably need to install LlamaIndex Here we define the function interface which is passed to OpenAI to perform auto retrieval We were not able to get OpenAI to work with nested pydantic objects or tuples as arguments so we converted the metadata filter keys and values into lists for the function API to work with This is currenty handled by our SQLAutoVectorQueryEngine Let s try implementing this by giving our OpenAIAssistantAgent access to two query tools SQL and Vector search We load sample structured datapoints into a SQL db and index it We load unstructured data into a vector index backed by Pinecone","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"a9b642b1-7a4d-43d1-9941-cff621af76aa":{"id_":"a9b642b1-7a4d-43d1-9941-cff621af76aa","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_assistant_agent.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"66cbfb97-6bcb-4709-aa37-d2b26290e24a","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_assistant_agent.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"bRv5Inl3K1wkk8wfiqE/50NAJKIG3TQ2PBfBi3PcGM0=","text":"OpenAI Assistant Agent Simple Agent no external tools Assistant with Built In Retrieval Assistant with Query Engine Tools Assistant Agent with your own Vector Store Retrieval API 1 Setup Load Data 2 Let s Try it Out  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This shows you how to use our agent abstractions built on top of the OpenAI Assistant API Here we show a simple example with the built in code interpreter Let s start by importing some simple building blocks If you re opening this Notebook on colab you will probably need to install LlamaIndex Let s test the assistant by having it use the built in OpenAI Retrieval tool over a user uploaded file Here we upload and pass in the file during assistant creation time The other option is you can upload pass the file id in for a message in a given thread with upload_files and add_message Here we showcase the function calling capabilities of the OpenAIAssistantAgent by integrating it with our query engine tools over different documents LlamaIndex has 35 vector database integrations Instead of using the in house Retrieval API you can use our assistant agent over any vector store Here is our full list of vector store integrations We picked one vector store Supabase using a random number generator","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"2b5e48bb-d70b-46e2-96a9-a347713cac99":{"id_":"2b5e48bb-d70b-46e2-96a9-a347713cac99","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_with_query_engine.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"f7c3de60-a1bb-4caa-8cb6-c389a80fe944","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_with_query_engine.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"ujUkicLWiosiE6LqvduSwNo+0Z86Hi4sZLjjV+kBB0Q=","text":"OpenAI Agent with Query Engine Tools Build Query Engine Tools Setup OpenAI Agent Let s Try It Out   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  If you re opening this Notebook on colab you will probably need to install LlamaIndex Download Data","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"ca92b63a-47cc-4d43-9b51-22f1efee9b8a":{"id_":"ca92b63a-47cc-4d43-9b51-22f1efee9b8a","metadata":{"url":"https://docs.llamaindex.ai/en/stable/#"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"d09eea58-8f24-4d8a-87c9-56545efd436b","metadata":{"url":"https://docs.llamaindex.ai/en/stable/#"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"7qEyAQXDeE8/n75yoqbcwtkg9B1g44+Tkd5uyO8tnWM=","text":"Welcome to LlamaIndex Why Context Augmentation Why LlamaIndex for Context Augmentation Who is LlamaIndex for Getting Started Ecosystem Community Associated projects  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes LlamaIndex is a data framework for LLM based applications which benefit from context augmentation Such LLM systems have been termed as RAG systems standing for Retrieval Augemented Generation LlamaIndex provides the essential abstractions to more easily ingest structure and access private or domain specific data in order to inject these safely and reliably into LLMs for more accurate text generation It s available in Python these docs and Typescript Tip Updating to LlamaIndex v0 10 0 Check out the migration guide LLMs offer a natural language interface between humans and data Widely available models come pre trained on huge amounts of publicly available data like Wikipedia mailing lists textbooks source code and more However while LLMs are trained on a great deal of data they are not trained on your data which may be private or specific to the problem you re trying to solve It s behind APIs in SQL databases or trapped in PDFs and slide decks You may choose to fine tune a LLM with your data but Training a LLM is expensive Due to the cost to train it s hard to update a LLM with latest information Observability is lacking When you ask a LLM a question it s not obvious how the LLM arrived at its answer Instead of fine tuning one can a context augmentation pattern called Retrieval Augmented Generation RAG to obtain more accurate text generation relevant to your specific data RAG involves the following high level steps Retrieve information from your data sources first Add it to your question as context and Ask the LLM to answer based on the enriched prompt In doing so RAG overcomes all three weaknesses of the fine tuning approach There s no training involved so it s cheap Data is fetched only when you ask for them so it s always up to date LlamaIndex can show you the retrieved documents so it s more trustworthy Firstly LlamaIndex imposes no restriction on how you use LLMs You can still use LLMs as auto complete chatbots semi autonomous agents and more see Use Cases on the left It only makes LLMs more relevant to you LlamaIndex provides the following tools to help you quickly standup production ready RAG systems Data connectors ingest your existing data from their native source and format These could be APIs PDFs SQL and much more Data indexes structure your data in intermediate representations that are easy and performant for LLMs to consume Engines provide natural language access to your data For example Query engines are powerful retrieval interfaces for knowledge augmented output Chat engines are conversational interfaces for multi message back and forth interactions with your data Data agents are LLM powered knowledge workers augmented by tools from simple helper functions to API integrations and more Application integrations tie LlamaIndex back into the rest of your ecosystem This could be LangChain Flask Docker ChatGPT or anything else LlamaIndex provides tools for beginners advanced users and everyone in between Our high level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code For more complex applications our lower level APIs allow advanced users to customize and extend any module data connectors indices retrievers query engines reranking modules to fit their needs To install the library pip install llama index We recommend starting at how to read these docs which will point you to the right place based on your experience level To download or contribute find LlamaIndex on Github https github com jerryjliu llama_index PyPi LlamaIndex https pypi org project llama index GPT Index duplicate https pypi org project gpt index Github https github com run llama LlamaIndexTS Docs https ts llamaindex ai LlamaIndex TS https www npmjs com package llamaindex Need help Have a feature suggestion Join the LlamaIndex community Twitter https twitter com llama_index Discord https discord gg dGcwcsnxhU LlamaHub https llamahub ai A large and growing collection of custom data connectors LlamaLab https github com run llama llama lab Ambitious projects built on top of LlamaIndex","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"390e3cbe-ee93-4be1-8cbf-159b57774d55":{"id_":"390e3cbe-ee93-4be1-8cbf-159b57774d55","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/agent_builder.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"212726ee-74e8-4417-ad76-01dbfdb0dcc7","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/agent_builder.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"Bceo5qrspccPBUVArSaWFlBPV/E/9guYR4JMBns4KaM=","text":"GPT Builder Demo Define Candidate Tools Define Meta Tools for GPT Builder Build Query Tool for Each Document Define Tool Retriever Load Data  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  Inspired by GPTs interface presented at OpenAI Dev Day 2023 Construct an agent with natural language Here you can build your own agent with another agent We also define a tool retriever to retrieve candidate tools In this setting we define tools as different Wikipedia pages Here we load wikipedia pages from different cities","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"93e3b2b7-4f2c-4257-830d-5a55960c8153":{"id_":"93e3b2b7-4f2c-4257-830d-5a55960c8153","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_retrieval.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"0c6bd133-b078-4fa8-b585-f9af135b9074","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_retrieval.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"pRlOiRApO9EaXBoSqx18xeYJ9z08AalgRaDmFuIVm/I=","text":"Retrieval Augmented OpenAI Agent Initial Setup Building an Object Index Our FnRetrieverOpenAIAgent Implementation   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this tutorial we show you how to use our FnRetrieverOpenAI implementationto build an agent on top of OpenAI s function API and store index an arbitrary number of tools Our indexing retrieval modules help to remove the complexity of having too many functions to fit in the prompt Let s start by importing some simple building blocks The main thing we need is the OpenAI API a place to keep conversation history a definition for tools that our agent can use If you re opening this Notebook on colab you will probably need to install LlamaIndex Let s define some very simple calculator tools for our agent We have an ObjectIndex construct in LlamaIndex that allows the user to use our index data structures over arbitrary objects The ObjectIndex will handle serialiation to from the object and use an underying index e g VectorStoreIndex SummaryIndex KeywordTableIndex as the storage mechanism In this case we have a large collection of Tool objects and we d want to define an ObjectIndex over these Tools The index comes bundled with a retrieval mechanism an ObjectRetriever This can be passed in to our agent so that it canperform Tool retrieval during query time We provide a FnRetrieverOpenAIAgent implementation in LlamaIndex which can take in an ObjectRetriever over a set of BaseTool objects During query time we would first use the ObjectRetriever to retrieve a set of relevant Tools These tools would then be passed into the agent more specifically their function signatures would be passed into the OpenAI Function calling API","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"8bb7f867-1f09-4c66-8846-01d24f73c7ac":{"id_":"8bb7f867-1f09-4c66-8846-01d24f73c7ac","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_context_retrieval.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"6575f716-a1c9-4cbc-aa80-6d73b33f0490","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_context_retrieval.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"5I9d8KxMIm8ZG0VkPhQVy51sAeiA+v0S4OBDgC3W7UY=","text":"Context Augmented OpenAI Agent Initial Setup Try Context Augmented Agent Use Uber 10 Q as context use Calculator as Tool  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this tutorial we show you how to use our ContextRetrieverOpenAIAgent implementationto build an agent on top of OpenAI s function API and store index an arbitrary number of tools Our indexing retrieval modules help to remove the complexity of having too many functions to fit in the prompt Here we setup a ContextRetrieverOpenAIAgent This agent will perform retrieval first before calling any tools This can help ground the agent s tool picking and answering capabilities in context If you re opening this Notebook on colab you will probably need to install LlamaIndex Download Data Here we augment our agent with context in different settings toy context we define some abbreviations that map to financial terms e g R Revenue We supply this as context to the agent","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"402376f4-1d38-4d5b-9e28-f27d9c5aa15b":{"id_":"402376f4-1d38-4d5b-9e28-f27d9c5aa15b","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/router/root.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b0d70703-3c71-40d5-9a49-ac2183b3a62c","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/router/root.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"hA4D5tfkr4ffetKyNQKZ0cYpa66p6f3EPXl+aFwDRyA=","text":"Routers Concept Usage Pattern Usage Pattern Defining a selector Using as a Query Engine Using as a Retriever Using selector as a standalone module   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Routers are modules that take in a user query and a set of choices defined by metadata and returns one or more selected choices They can be used on their own as selector modules or used as a query engine or retriever e g on top of other query engines retrievers They are simple but powerful modules that use LLMs for decision making capabilities They can be used for the following use cases and more Selecting the right data source among a diverse range of data sources Deciding whether to do summarization e g using summary index query engine or semantic search e g using vector index query engine Deciding whether to try out a bunch of choices at once and combine the results using multi routing capabilities The core router modules exist in the following forms LLM selectors put the choices as a text dump into a prompt and use LLM text completion endpoint to make decisions Pydantic selectors pass choices as Pydantic schemas into a function calling endpoint and return Pydantic objects A simple example of using our router module as part of a query engine is given below Defining a selector is at the core of defining a router You can easily use our routers as a query engine or a retriever In these cases the router will be responsiblefor selecting query engine s or retriever s to route the user query to We also highlight our ToolRetrieverRouterQueryEngine for retrieval augmented routing this is the casewhere the set of choices themselves may be very big and may need to be indexed NOTE this is a beta feature We also highlight using our router as a standalone module Some examples are given below with LLM and Pydantic based single multi selectors A RouterQueryEngine is composed on top of other query engines as tools Similarly a RouterRetriever is composed on top of other retrievers as tools An example is given below You can use the selectors as standalone modules Define choices as either a list of ToolMetadata or as a list of strings","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"6f09da19-8a31-407a-9478-4605dd53f232":{"id_":"6f09da19-8a31-407a-9478-4605dd53f232","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_context_retrieval.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"173f5b33-ddbc-4fb9-96c0-94249d21c7f8","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_context_retrieval.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"5I9d8KxMIm8ZG0VkPhQVy51sAeiA+v0S4OBDgC3W7UY=","text":"Context Augmented OpenAI Agent Initial Setup Try Context Augmented Agent Use Uber 10 Q as context use Calculator as Tool  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this tutorial we show you how to use our ContextRetrieverOpenAIAgent implementationto build an agent on top of OpenAI s function API and store index an arbitrary number of tools Our indexing retrieval modules help to remove the complexity of having too many functions to fit in the prompt Here we setup a ContextRetrieverOpenAIAgent This agent will perform retrieval first before calling any tools This can help ground the agent s tool picking and answering capabilities in context If you re opening this Notebook on colab you will probably need to install LlamaIndex Download Data Here we augment our agent with context in different settings toy context we define some abbreviations that map to financial terms e g R Revenue We supply this as context to the agent","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"a9fba1e9-fb02-4ae3-a449-a3312793669e":{"id_":"a9fba1e9-fb02-4ae3-a449-a3312793669e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/usage_pattern.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"dcceb1cc-b569-457d-ac46-fc94cea435a8","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/usage_pattern.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"vCaSyNKdiZUDEuL3a+DntLqIFH9gezXathKrw0yVzwE=","text":"Usage Pattern Getting Started Defining Tools Lower Level API Customizing your Agent Advanced Concepts for OpenAIAgent in beta Query Engine Tools Use other agents as Tools Function Retrieval Agents Context Retrieval Agents Query Planning  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes An agent is initialized from a set of Tools Here s an example of instantiating a ReActagent from a set of Tools An agent supports both chat and query endpoints inheriting from our ChatEngine and QueryEngine respectively Example usage To automatically pick the best agent depending on the LLM you can use the from_llm method to generate an agent It is easy to wrap query engines as tools for an agent as well Simply do the following A nifty feature of our agents is that since they inherit from BaseQueryEngine you can easily define other agents as toolsthrough our QueryEngineTool The OpenAIAgent and ReActAgent are simple wrappers on top of an AgentRunner interacting with an AgentWorker All agents can be defined this manner For example for the OpenAIAgent This is also the preferred format for custom agents Check out the lower level agent guide for more details If you wish to customize your agent you can choose to subclass the CustomSimpleAgentWorker and plug it into an AgentRunner see above Check out our Custom Agent Notebook Guide for more details You can also use agents in more advanced settings For instance being able to retrieve tools from an index during query time andbeing able to perform query planning over an existing set of Tools These are largely implemented with our OpenAIAgent classes which depend on the OpenAI Function API Supportfor our more general ReActAgent is something we re actively investigating NOTE these are largely still in beta The abstractions may change and become more general over time If the set of Tools is very large you can create an ObjectIndex to index the tools and then pass in an ObjectRetriever to the agent during query time to first dynamically retrieve the relevant tools before having the agent pick from the candidate tools We first build an ObjectIndex over an existing set of Tools We then define our FnRetrieverOpenAIAgent Our context augmented OpenAI Agent will always perform retrieval before calling any tools This helps to provide additional context that can help the agent better pick Tools versusjust trying to make a decision without any context OpenAI Function Agents can be capable of advanced query planning The trick is to provide the agentwith a QueryPlanTool if the agent calls the QueryPlanTool it is forced to infer a full Pydantic schema representing a queryplan over a set of subtools","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"373e9ffa-6bf0-4df8-864f-d59811cb0d46":{"id_":"373e9ffa-6bf0-4df8-864f-d59811cb0d46","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"029ae730-ab86-4c86-8000-c293b61395ea","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"mCnxMYGFd55+Iv5y3eIhPlFgrXqaXKKP/VclCM6f+Rs=","text":"Starter Tutorial Download data Set your OpenAI API key Load data and build an index Query your data Viewing Queries and Events Using Logging Storing your index   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Tip Make sure you ve followed the installation steps first This is our famous 5 lines of code starter example using OpenAI Want to use local models If you want to do our starter tutorial using only local models check out this tutorial instead This example uses the text of Paul Graham s essay What I Worked On This and many other examples can be found in the examples folder of our repo The easiest way to get it is to download it via this link and save it in a folder called data LlamaIndex uses OpenAI s gpt 3 5 turbo by default Make sure your API key is available to your code by setting it as an environment variable In MacOS and Linux this is the command and on Windows it is In the same folder where you created the data folder create a file called starter py file with the following This builds an index over the documents in the data folder which in this case just consists of the essay text but could contain many documents Your directory structure should look like this Add the following lines to starter py This creates an engine for Q A over your index and asks a simple question You should get back a response similar to the following The author wrote short stories and tried to program on an IBM 1401 Want to see what s happening under the hood Let s add some logging Add these lines to the top of starter py You can set the level to DEBUG for verbose output or use level logging INFO for less By default the data you just loaded is stored in memory as a series of vector embeddings You can save time and requests to OpenAI by saving the embeddings to disk That can be done with this line By default this will save the data to the directory storage but you can change that by passing a persist_dir parameter Of course you don t get the benefits of persisting unless you load the data So let s modify starter py to generate and store the index if it doesn t exist but load it if it does Now you can efficiently query to your heart s content But this is just the beginning of what you can do with LlamaIndex Next Steps learn more about the high level concepts tell me how to customize things curious about a specific module check out the guides on the left","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"7e66005f-4fb2-4d2e-b024-c316631c53ae":{"id_":"7e66005f-4fb2-4d2e-b024-c316631c53ae","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"189ed042-4cf4-4f2d-91b5-e441fa41ba68","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"sRCP2qgZCrFyR6KKNxjzwGarJH95RqmyUvRDOGl17EY=","text":"Build your own OpenAI Agent Initial Setup Agent Definition Let s Try It Out Our Slightly Better OpenAIAgent Implementation Chat Async Chat Streaming Chat Async Streaming Chat Agent with Personality  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  With the new OpenAI API that supports function calling it s never been easier to build your own agent In this notebook tutorial we showcase how to write your own OpenAI agent in under 50 lines of code It is minimal yet feature complete with ability to carry on a conversation and use tools Let s start by importing some simple building blocks The main thing we need is the OpenAI API using our own llama_index LLM class a place to keep conversation history a definition for tools that our agent can use If you re opening this Notebook on colab you will probably need to install LlamaIndex Let s define some very simple calculator tools for our agent Now we define our agent that s capable of holding a conversation and calling tools in under 50 lines of code The meat of the agent logic is in the chat method At a high level there are 3 steps Call OpenAI to decide which tool if any to call and with what arguments Call the tool with the arguments to obtain an output Call OpenAI to synthesize a response from the conversation context and the tool output The reset method simply resets the conversation context so we can start another conversation We provide a slightly better OpenAIAgent implementation in LlamaIndex which you can directly use as follows In comparison to the simplified version above it implements the BaseChatEngine and BaseQueryEngine interface so you can more seamlessly use it in the LlamaIndex framework it supports multiple function calls per conversation turn it supports streaming it supports async endpoints it supports callback and tracing Here every LLM response is returned as a generator You can stream every incremental step or only the last response You can specify a system prompt to give the agent additional instruction or personality","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"2e1ea686-d533-42e9-a747-073e1fec74a7":{"id_":"2e1ea686-d533-42e9-a747-073e1fec74a7","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_retrieval.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"fe95b995-308f-4354-8d2a-8b6f92dcdb5b","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_retrieval.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"pRlOiRApO9EaXBoSqx18xeYJ9z08AalgRaDmFuIVm/I=","text":"Retrieval Augmented OpenAI Agent Initial Setup Building an Object Index Our FnRetrieverOpenAIAgent Implementation   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this tutorial we show you how to use our FnRetrieverOpenAI implementationto build an agent on top of OpenAI s function API and store index an arbitrary number of tools Our indexing retrieval modules help to remove the complexity of having too many functions to fit in the prompt Let s start by importing some simple building blocks The main thing we need is the OpenAI API a place to keep conversation history a definition for tools that our agent can use If you re opening this Notebook on colab you will probably need to install LlamaIndex Let s define some very simple calculator tools for our agent We have an ObjectIndex construct in LlamaIndex that allows the user to use our index data structures over arbitrary objects The ObjectIndex will handle serialiation to from the object and use an underying index e g VectorStoreIndex SummaryIndex KeywordTableIndex as the storage mechanism In this case we have a large collection of Tool objects and we d want to define an ObjectIndex over these Tools The index comes bundled with a retrieval mechanism an ObjectRetriever This can be passed in to our agent so that it canperform Tool retrieval during query time We provide a FnRetrieverOpenAIAgent implementation in LlamaIndex which can take in an ObjectRetriever over a set of BaseTool objects During query time we would first use the ObjectRetriever to retrieve a set of relevant Tools These tools would then be passed into the agent more specifically their function signatures would be passed into the OpenAI Function calling API","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"21acece7-c84e-473c-a793-c4ec5c264379":{"id_":"21acece7-c84e-473c-a793-c4ec5c264379","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"52a1a9c4-8dfa-4fc5-9dd9-8bbc027075d3","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"sRCP2qgZCrFyR6KKNxjzwGarJH95RqmyUvRDOGl17EY=","text":"Build your own OpenAI Agent Initial Setup Agent Definition Let s Try It Out Our Slightly Better OpenAIAgent Implementation Chat Async Chat Streaming Chat Async Streaming Chat Agent with Personality  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  With the new OpenAI API that supports function calling it s never been easier to build your own agent In this notebook tutorial we showcase how to write your own OpenAI agent in under 50 lines of code It is minimal yet feature complete with ability to carry on a conversation and use tools Let s start by importing some simple building blocks The main thing we need is the OpenAI API using our own llama_index LLM class a place to keep conversation history a definition for tools that our agent can use If you re opening this Notebook on colab you will probably need to install LlamaIndex Let s define some very simple calculator tools for our agent Now we define our agent that s capable of holding a conversation and calling tools in under 50 lines of code The meat of the agent logic is in the chat method At a high level there are 3 steps Call OpenAI to decide which tool if any to call and with what arguments Call the tool with the arguments to obtain an output Call OpenAI to synthesize a response from the conversation context and the tool output The reset method simply resets the conversation context so we can start another conversation We provide a slightly better OpenAIAgent implementation in LlamaIndex which you can directly use as follows In comparison to the simplified version above it implements the BaseChatEngine and BaseQueryEngine interface so you can more seamlessly use it in the LlamaIndex framework it supports multiple function calls per conversation turn it supports streaming it supports async endpoints it supports callback and tracing Here every LLM response is returned as a generator You can stream every incremental step or only the last response You can specify a system prompt to give the agent additional instruction or personality","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"7b5baf74-ac52-43f3-97eb-13f6b3fcde94":{"id_":"7b5baf74-ac52-43f3-97eb-13f6b3fcde94","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/concepts.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"29ded040-2222-487f-a80c-e01f18792daf","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/concepts.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"8bSoSsJvNOb6aHyu07g3QEOF7HuTSB9UIAG93dITVhQ=","text":"High Level Concepts Retrieval Augmented Generation RAG Stages within RAG Important concepts within each step Loading stage Indexing Stage Querying Stage Putting it all together  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes This is a quick guide to the high level concepts you ll encounter frequently when building LLM applications Tip If you haven t install LlamaIndex and complete the starter tutorial before you read this It will help ground these steps in your experience LLMs are trained on enormous bodies of data but they aren t trained on your data Retrieval Augmented Generation RAG solves this problem by adding your data to the data LLMs already have access to You will see references to RAG frequently in this documentation In RAG your data is loaded and prepared for queries or indexed User queries act on the index which filters your data down to the most relevant context This context and your query then go to the LLM along with a prompt and the LLM provides a response Even if what you re building is a chatbot or an agent you ll want to know RAG techniques for getting data into your application  There are five key stages within RAG which in turn will be a part of any larger application you build These are Loading this refers to getting your data from where it lives whether it s text files PDFs another website a database or an API into your pipeline LlamaHub provides hundreds of connectors to choose from Indexing this means creating a data structure that allows for querying the data For LLMs this nearly always means creating vector embeddings numerical representations of the meaning of your data as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data Storing once your data is indexed you will almost always want to store your index as well as other metadata to avoid having to re index it Querying for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query including sub queries multi step queries and hybrid strategies Evaluation a critical step in any pipeline is checking how effective it is relative to other strategies or when you make changes Evaluation provides objective measures of how accurate faithful and fast your responses to queries are  There are also some terms you ll encounter that refer to steps within each of these stages Nodes and Documents A Document is a container around any data source for instance a PDF an API output or retrieve data from a database A Node is the atomic unit of data in LlamaIndex and represents a chunk of a source Document Nodes have metadata that relate them to the document they are in and to other nodes Connectors A data connector often called a Reader ingests data from different data sources and data formats into Documents and Nodes Indexes Once you ve ingested your data LlamaIndex will help you index the data into a structure that s easy to retrieve This usually involves generating vector embeddings which are stored in a specialized database called a vector store Indexes can also store a variety of metadata about your data Embeddings LLMs generate numerical representations of data called embeddings When filtering your data for relevance LlamaIndex will convert queries into embeddings and your vector store will find data that is numerically similar to the embedding of your query Retrievers A retriever defines how to efficiently retrieve relevant context from an index when given a query Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it s done Routers A router determines which retriever will be used to retrieve relevant context from the knowledge base More specifically the RouterRetriever class is responsible for selecting one or multiple candidate retrievers to execute a query They use a selector to choose the best option based on each candidate s metadata and the query Node Postprocessors A node postprocessor takes in a set of retrieved nodes and applies transformations filtering or re ranking logic to them Response Synthesizers A response synthesizer generates a response from an LLM using a user query and a given set of retrieved text chunks There are endless use cases for data backed LLM applications but they can be roughly grouped into three categories Query Engines A query engine is an end to end pipeline that allows you to ask questions over your data It takes in a natural language query and returns a response along with reference context retrieved and passed to the LLM Chat Engines A chat engine is an end to end pipeline for having a conversation with your data multiple back and forth instead of a single question and answer Agents An agent is an automated decision maker powered by an LLM that interacts with the world via a set of tools Agents can take an arbitrary number of steps to complete a given task dynamically deciding on the best course of action rather than following pre determined steps This gives it additional flexibility to tackle more complex tasks Next Steps Tell me how to customize things Continue learning with our understanding LlamaIndex guide Ready to dig deep Check out the module guides on the left","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"b036c883-479c-4c62-b27d-9308767054b9":{"id_":"b036c883-479c-4c62-b27d-9308767054b9","metadata":{"url":"https://docs.llamaindex.ai/en/stable/optimizing/agentic_strategies/agentic_strategies.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"379d0c63-9738-47a9-b21c-81649561f401","metadata":{"url":"https://docs.llamaindex.ai/en/stable/optimizing/agentic_strategies/agentic_strategies.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"NRnNlzHzMmPEUETDePC6tPsRyNs3UcYShJXNcFP4m38=","text":"Agentic strategies Simpler Agentic Strategies Data Agents   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes You can build agents on top of your existing LlamaIndex RAG pipeline to empower it with automated decision capabilities A lot of modules routing query transformations and more are already agentic in nature in that they use LLMs for decision making These include routing and query transformations This guides below show you how to deploy a full agent loop capable of chain of thought and query planning on top of existing RAG query engines as tools for more advanced decision making Make sure to check out our full module guide on Data Agents which highlight these use cases and much more Our lower level agent API shows you the internals of how an agent works with step wise execution Example guides below using OpenAI function calling","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"fd32d21f-ef8f-433f-8f88-780ef23c6774":{"id_":"fd32d21f-ef8f-433f-8f88-780ef23c6774","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_plan.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b40143b7-5738-4464-8f7f-8ffd04c55be7","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_plan.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"09XZB+GOh+TVtGWIR7egW7vU8hWaVVb3gPe2HnKzG5I=","text":"OpenAI Agent Query Planning Download Data Load data Build indices OpenAI Function Agent with a Query Plan Tool   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this demo we explore adding a QueryPlanTool to an OpenAIAgent This effectively enables the agentto do advanced query planning all through a single tool The QueryPlanTool is designed to work well with the OpenAI Function API The tool takes in a set of other tools as input The tool function signature contains of a QueryPlan Pydantic object which can in turn contain a DAG of QueryNode objects defining a compute graph The agent is responsible for defining this graph through the function signature when calling the tool The tool itself executes the DAG over any corresponding tools In this setting we use a familiar example Uber 10Q filings in March June and September of 2022 If you re opening this Notebook on colab you will probably need to install LlamaIndex We build a vector index query engine over each of the documents March June September Use OpenAIAgent built on top of the OpenAI tool use interface Feed it our QueryPlanTool which is a Tool that takes in other tools And the agent to generate a query plan DAG over these tools","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"41462b2d-0263-4ea8-ab2d-c3e8059fd118":{"id_":"41462b2d-0263-4ea8-ab2d-c3e8059fd118","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/root.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"06bcb5bc-0a77-4b09-b8c9-dd192e7838cd","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/root.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"e5jwlfjzADh1PeZ2HflseCDKAOTxVuMVpGyOKc7uCpA=","text":"Tools Concept Usage Pattern LlamaHub Tools Guide Blog Post  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Having proper tool abstractions is at the core of building data agents Defining a set of Tools is similar to defining any API interface with the exception that these Tools are meant for agent rather than human use We allow users to define both a Tool as well as a ToolSpec containing a series of functions under the hood A Tool implements a very generic interface simply define __call__ and also return some basic metadata name description function schema A Tool Spec defines a full API specification of any service that can be converted into a list of Tools We offer a few different types of Tools FunctionTool A function tool allows users to easily convert any user defined function into a Tool It can also auto infer the function schema QueryEngineTool A tool that wraps an existing query engine Note since our agent abstractions inherit from BaseQueryEngine these tools can also wrap other agents We offer a rich set of Tools and Tool Specs through LlamaHub For full details please check out our detailed blog post Our Tool Specs and Tools can be imported from the llama hub package To use with our agent See our Usage Pattern Guide for more details Check out our guide for a full overview of the Tools Tool Specs in LlamaHub","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"7121a8fb-e6bb-4748-b2eb-17221e978639":{"id_":"7121a8fb-e6bb-4748-b2eb-17221e978639","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/extraction.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b37ac0e3-a5ce-4c25-92d4-36d7df15db68","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/extraction.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"cBbp9U7eS+aCFdPeSc+XbY4U7oizGbOpInjRTVUAAa0=","text":"Structured Data Extraction Core Guides Misc Examples   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes LLMs are capable of ingesting large amounts of unstructured data and returning it in structured formats and LlamaIndex is set up to make this easy Using LlamaIndex you can get an LLM to read natural language and identify semantically important details such as names dates addresses and figures and return them in a consistent structured format regardless of the source format This can be especially useful when you have unstructured source material like chat logs and conversation transcripts Once you have structured data you can send them to a database or you can parse structured outputs in code to automate workflows Check out our Structured Output guide for a comprehensive overview of structured data extraction with LlamaIndex Do it in a standalone fashion Pydantic program or as part of a RAG pipeline We also have standalone output parsing modules that you can use yourself with an LLM prompt We also have multi modal structured data extraction Check it out Some additional miscellaneous examples highlighting use cases","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"3dc021a1-293a-4a84-a76e-a9de1f53d0da":{"id_":"3dc021a1-293a-4a84-a76e-a9de1f53d0da","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/pydantic_query_engine.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"ce38bf4a-5201-43fe-ab06-42831bc62afd","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/pydantic_query_engine.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"7BbvYtlAvh/6oH+l58J6i9VNliu6GCzrQedwVM5DMo4=","text":"Query Engine with Pydantic Outputs Setup Create the Index Query Engine OpenAI Create the Index Query Engine Non OpenAI Beta Accumulate Examples Beta Create our Pydanitc Output Object  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  Every query engine has support for integrated structured responses using the following response_modes in RetrieverQueryEngine refine compact tree_summarize accumulate beta requires extra parsing to convert to objects compact_accumulate beta requires extra parsing to convert to objects In this notebook we walk through a small example demonstrating the usage Under the hood every LLM response will be a pydantic object If that response needs to be refined or summarized it is converted into a JSON string for the next response Then the final response is returned as a pydantic object NOTE This can technically work with any LLM but non openai is support is still in development and considered beta If you re opening this Notebook on colab you will probably need to install LlamaIndex Download Data When using OpenAI the function calling API will be leveraged for reliable structured outputs When using an LLM that does not support function calling we rely on the LLM to write the JSON itself and we parse the JSON into the proper pydantic object Accumulate with pydantic objects requires some extra parsing This is still a beta feature but it s still possible to get accumulate pydantic objects In accumulate responses are separated by a default separator and prepended with a prefix","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"d03f7920-00cf-4a0d-a3a4-bc72665561c4":{"id_":"d03f7920-00cf-4a0d-a3a4-bc72665561c4","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/output_parser.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"83844ace-6e90-4f3e-8a48-756c3e576d8e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/output_parser.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"NlmnE99vH0g9FcKe0qg+GwsIOmLmhs98CWHHexhvqS8=","text":"Output Parsing Modules Guardrails Langchain Guides   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes LlamaIndex supports integrations with output parsing modules offeredby other frameworks These output parsing modules can be used in the following ways To provide formatting instructions for any prompt query through output_parser format To provide parsing for LLM outputs through output_parser parse Guardrails is an open source Python package for specification validation correction of output schemas See below for a code example Output Langchain also offers output parsing modules that you can use within LlamaIndex Output Examples","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"e0204140-f19b-4465-9dc6-f2ebc5d362ac":{"id_":"e0204140-f19b-4465-9dc6-f2ebc5d362ac","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/llm/llama_api.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"72cee319-4d09-4db6-8ef4-521dd54a2d3d","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/llm/llama_api.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"I57+knrrMVdiqVYMkCflWJmY9l2EjbPY3EKy/sGiTIU=","text":"Llama API Setup Basic Usage Function Calling Structured Data Extraction Call complete with a prompt Call chat with a list of messages  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  Llama API is a hosted API for Llama 2 with function calling support To start go to https www llama api com to obtain an API key If you re opening this Notebook on colab you will probably need to install LlamaIndex This is a simple example of parsing an output into an Album schema which can contain multiple songs Define output schema Define pydantic program llama API is OpenAI compatible Run program to get structured output","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"0b995285-1fc3-4123-97a0-c6e0c8d9ae3d":{"id_":"0b995285-1fc3-4123-97a0-c6e0c8d9ae3d","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_pydantic_program.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"6e138f19-5598-4dd3-9707-49bb80590e93","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_pydantic_program.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"a3LZvuWDFhu1rCJIGZWvZ6w0M7DchofzMRaaQb83b0M=","text":"Guidance Pydantic Program    Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  Generate structured data with guidance via LlamaIndex With guidance you can guarantee the output structure is correct by forcing the LLM to output desired tokens This is especialy helpful when you are using lower capacity model e g the current open source models which otherwise would struggle to generate valid output that fits the desired output schema If you re opening this Notebook on colab you will probably need to install LlamaIndex Define output schema Define guidance pydantic program Run program to get structured output Text highlighted in blue is variables specified by us text highlighted in green is generated by the LLM The output is a valid Pydantic object that we can then use to call functions APIs","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"9103e888-5a7d-4ada-969c-fe0ed6e01157":{"id_":"9103e888-5a7d-4ada-969c-fe0ed6e01157","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/modules.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"2a84d280-7768-46b9-9e83-685636f6cd74","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/modules.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"NRNzen7rQW8WLILAR3KbJILCEA51kkNsEdZXAfxoTeQ=","text":"Module Guides OpenAI Agent Beta OpenAI Assistant Agent ReAct Agent Additional Agents available on LlamaHub Custom Agents Lower Level Agent API   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes These guide provide an overview of how to use our agent classes For more detailed guides on how to use specific tools check out our tools module guides","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"26b217a0-1ecb-4078-8041-e5d1e6279279":{"id_":"26b217a0-1ecb-4078-8041-e5d1e6279279","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/usage_pattern.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"bab2d104-e8cf-4769-80ca-b6c1543411ce","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/usage_pattern.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"wGSEnAOmgjXSazanJCprTcGNHX9b3nmqCa69s+X6SRM=","text":"Usage Pattern Using with our Agents Using with LangChain   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes You can create custom LlamaHub Tool Specs and Tools or they can be imported from the llama hub package They can be plugged into our native agents or LangChain agents To use with our OpenAIAgent Full Tool details can be found on our LlamaHub page Each tool contains a Usage section showing how that tool can be used To use with a LangChain agent simply convert tools to LangChain tools with to_langchain_tool","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"571a7d55-e912-4db4-8d16-6f74218a3fac":{"id_":"571a7d55-e912-4db4-8d16-6f74218a3fac","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/installation.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"77110322-4226-4ba1-a5be-af5531f4622c","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/installation.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"8NtugY0jp6ImA1x0uq8dPRoz7pb1MxMO4QDR6FYQ/Lw=","text":"Installation and Setup Quickstart Installation from Pip Custom Installation from Pip Installation from Source Important OpenAI Environment Setup  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes The LlamaIndex ecosystem is structured using a collection of namespaced packages What this means for users is that LlamaIndex comes with a core starter bundle and additional integrations can be installed as needed A complete list of packages and available integrations is available in our temporary registry which will be moving to LlamaHub soon To get started quickly you can install with This is a starter bundle of packages containing llama index core llama index legacy temporarily included llama index llms openai llama index embeddings openai llama index program openai llama index question gen openai llama index agent openai llama index readers file llama index multi modal llms openai NOTE LlamaIndex may download and store local files for various packages NLTK HuggingFace Use the environment variable LLAMA_INDEX_CACHE_DIR to control where these files are saved By default we use the OpenAI gpt 3 5 turbo model for text generation and text embedding ada 002 for retrieval and embeddings In order to use this you must have an OPENAI_API_KEY set up as an environment variable You can obtain an API key by logging into your OpenAI account and and creating a new API key Tip You can also use one of many other available LLMs You mayneed additional environment keys tokens setup depending on the LLM provider Check out our OpenAI Starter Example If you aren t using OpenAI or want a more selective installation you can install individual packages as needed For example for a local setup with Ollama and HuggingFace embeddings the installation might look like Check out our Starter Example with Local Models A full guide to using and configuring LLMs is available here A full guide to using and configuring embedding models is available here Git clone this repository git clone https github com jerryjliu llama_index git Then do the following Install poetry this will help you manage package dependencies poetry shell this command creates a virtual environment which keeps installed packages contained to this project poetry install this will install the core starter package requirements Optional poetry install with dev docs this will install all dependencies needed for most local development From there you can install integrations as needed with pip For example","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"9434abd0-26ba-46f0-8d26-770432b63295":{"id_":"9434abd0-26ba-46f0-8d26-770432b63295","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/recursive_retriever_agents.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"18ef41f8-3216-4c83-a9d7-752b92895ad7","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/recursive_retriever_agents.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"MR4AZCSm6CDJJGt3J7puf0m/8mwpqh4jV1alXqpf8X4=","text":"Recursive Retriever Document Agents Setup and Download Data Build Document Agent for each Document Build Composable Retriever over these Agents Running Example Queries   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This guide shows how to combine recursive retrieval and document agents for advanced decision making over heterogeneous documents There are two motivating factors that lead to solutions for better retrieval Decoupling retrieval embeddings from chunk based synthesis Oftentimes fetching documents by their summaries will return more relevant context to queries rather than raw chunks This is something that recursive retrieval directly allows Within a document users may need to dynamically perform tasks beyond fact based question answering We introduce the concept of document agents agents that have access to both vector search and summary tools for a given document In this section we ll define imports and then download Wikipedia articles about different cities Each article is stored separately If you re opening this Notebook on colab you will probably need to install LlamaIndex Define LLM Service Context Callback Manager In this section we define document agents for each document First we define both a vector index for semantic search and summary index for summarization for each document The two query engines are then converted into tools that are passed to an OpenAI function calling agent This document agent can dynamically choose to perform semantic search or summarization within a given document We create a separate document agent for each city Now we define a set of summary nodes where each node links to the corresponding Wikipedia city article We then define a composable retriever query engine on top of these Nodes to route queries down to a given node which will in turn route it to the relevant document agent","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"569095df-53e8-47ac-a62b-0f103c13609c":{"id_":"569095df-53e8-47ac-a62b-0f103c13609c","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_transformations/SimpleIndexDemo-multistep.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"284a4820-bd33-4c43-96e6-9e9a8853c170","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_transformations/SimpleIndexDemo-multistep.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"H0sfNoPaxU1nDL73FWwf35+PAb5KKgBcMunQ12cUXSw=","text":"Multi Step Query Engine Download Data Load documents build the VectorStoreIndex Query Index   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  We have a multi step query engine that s able to decompose a complex query into sequential subquestions Thisguide walks you through how to set it up If you re opening this Notebook on colab you will probably need to install LlamaIndex In the first batch of the accelerator program started by the author the participants included the founders of Reddit Justin Kan and Emmett Shear who later founded Twitch Aaron Swartz who had helped write the RSS spec and later became a martyr for open access and Sam Altman who later became the second president of YC","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"4a26f274-1db9-4e66-9dc9-e4d537ca050c":{"id_":"4a26f274-1db9-4e66-9dc9-e4d537ca050c","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/pydantic_tree_summarize.html#download-data"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"faf40df3-d623-425c-951a-9b33b1f849a4","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/pydantic_tree_summarize.html#download-data"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"xlBM8IigYgPvq6okZa9EcPJiZA3qyMLlq08JQyFE2cY=","text":"Pydantic Tree Summarize Download Data Load Data Summarize Create pydantic model to structure response Inspect the response  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we demonstrate how to use tree summarize with structured outputs Specifically tree summarize is used to output pydantic objects Here we see the response is in an instance of our Biography class","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"ae5cc4cb-f0ee-470b-a722-54f1edbf5051":{"id_":"ae5cc4cb-f0ee-470b-a722-54f1edbf5051","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/LangchainOutputParserDemo.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"84cc20df-728f-4bfe-adc8-2f33192d3c55","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/LangchainOutputParserDemo.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"ahiOp9N30NkF/1uhDjVpekotWEeW7Bs37qKwJklq8A8=","text":"Langchain Output Parsing Load documents build the VectorStoreIndex Define Query Langchain Output Parser Query Index   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  Download Data Define custom QA and Refine Prompts","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"2a5c2eeb-b9b3-4520-96e2-ce88a7d843a2":{"id_":"2a5c2eeb-b9b3-4520-96e2-ce88a7d843a2","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/GuardrailsDemo.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"db6509a1-ba8d-4014-bd18-4d0d95fc3652","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/GuardrailsDemo.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"JMcrjKXYPnQt+UchIzAgp39IJ5ckhA9zL/EQCWp9MNk=","text":"Guardrails Output Parsing Download Data Load documents build the VectorStoreIndex Define Query Guardrails Spec Query Index   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  If you re opening this Notebook on colab you will probably need to install LlamaIndex Define custom QA and Refine Prompts Define Guardrails Spec","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"94e8263d-6d91-4eb4-8310-77e11bb9433e":{"id_":"94e8263d-6d91-4eb4-8310-77e11bb9433e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/pydantic_program.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"09630e66-b6e6-4002-a3ea-e09ac4e0d9b3","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/pydantic_program.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"oE9Lq0vEy481223qc6j56Xd8Li5NM20VkD0DIOxBqbM=","text":"Pydantic Program LLM Text Completion Pydantic Programs LLM Function Calling Pydantic Programs Prepackaged Pydantic Programs   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes A pydantic program is a generic abstraction that takes in an input string and converts it to a structured Pydantic object type Because this abstraction is so generic it encompasses a broad range of LLM workflows The programs are composable and be for more generic or specific use cases There s a few general types of Pydantic Programs LLM Text Completion Pydantic Programs These convert input text into a user specified structured object through a text completion API output parsing LLM Function Calling Pydantic Program These convert input text into a user specified structured object through an LLM function calling API Prepackaged Pydantic Programs These convert input text into prespecified structured objects","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"08383e7b-c0ec-4b67-b525-d683c493c088":{"id_":"08383e7b-c0ec-4b67-b525-d683c493c088","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/openai_pydantic_program.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"33d5c373-466e-4776-85f3-64a5a44dfb27","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/openai_pydantic_program.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"G4TPSnLq8Gm4CpvOwPfQjeG5HTHmYw6QX+FpIIyjqzU=","text":"OpenAI Pydantic Program Extraction into Album Extracting List of Album with Parallel Function Calling Extraction into Album Streaming Extraction into DirectoryTree object Without docstring in Model With docstring in Model  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This guide shows you how to generate structured data with new OpenAI API via LlamaIndex The user just needs to specify a Pydantic object We demonstrate two settings Extraction into an Album object which can contain a list of Song objects Extraction into a DirectoryTree object which can contain recursive Node objects This is a simple example of parsing an output into an Album schema which can contain multiple songs If you re opening this Notebook on colab you will probably need to install LlamaIndex Define output schema without docstring Define openai pydantic program Run program to get structured output Run program to get structured output The output is a valid Pydantic object that we can then use to call functions APIs With the latest parallel function calling feature from OpenAI we can simultaneously extract multiple structured data from a single prompt To do this we need to pick one of the latest models e g gpt 3 5 turbo 1106 and set allow_multiple to True in our OpenAIPydanticProgram if not it will only return the first object and raise a warning The output is a list of valid Pydantic object We also support streaming a list of objects through our stream_list function Full credits to this idea go to openai_function_call repo https github com jxnl openai_function_call tree main examples streaming_multitask This is directly inspired by jxnl s awesome repo here https github com jxnl openai_function_call That repository shows how you can use OpenAI s function API to parse recursive Pydantic objects The main requirement is that you want to wrap a recursive Pydantic object with a non recursive one Here we show an example in a directory setting where a DirectoryTree object wraps recursive Node objects to parse a file structure The output is a full DirectoryTree structure with recursive Node objects","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"05f78519-9b53-435e-8f67-d10106dac806":{"id_":"05f78519-9b53-435e-8f67-d10106dac806","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_pydantic_program.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"bf62e97a-586b-419e-b4e9-90eb05c0171e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_pydantic_program.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"a3LZvuWDFhu1rCJIGZWvZ6w0M7DchofzMRaaQb83b0M=","text":"Guidance Pydantic Program    Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  Generate structured data with guidance via LlamaIndex With guidance you can guarantee the output structure is correct by forcing the LLM to output desired tokens This is especialy helpful when you are using lower capacity model e g the current open source models which otherwise would struggle to generate valid output that fits the desired output schema If you re opening this Notebook on colab you will probably need to install LlamaIndex Define output schema Define guidance pydantic program Run program to get structured output Text highlighted in blue is variables specified by us text highlighted in green is generated by the LLM The output is a valid Pydantic object that we can then use to call functions APIs","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"7eba3544-798b-4855-8097-0ea29c3946f9":{"id_":"7eba3544-798b-4855-8097-0ea29c3946f9","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_pydantic_program.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"4810c1d6-87aa-4950-8f5d-3c91134e0a5d","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_pydantic_program.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"a3LZvuWDFhu1rCJIGZWvZ6w0M7DchofzMRaaQb83b0M=","text":"Guidance Pydantic Program    Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  Generate structured data with guidance via LlamaIndex With guidance you can guarantee the output structure is correct by forcing the LLM to output desired tokens This is especialy helpful when you are using lower capacity model e g the current open source models which otherwise would struggle to generate valid output that fits the desired output schema If you re opening this Notebook on colab you will probably need to install LlamaIndex Define output schema Define guidance pydantic program Run program to get structured output Text highlighted in blue is variables specified by us text highlighted in green is generated by the LLM The output is a valid Pydantic object that we can then use to call functions APIs","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"a18cd4a5-0152-4c62-b73d-0294144de48d":{"id_":"a18cd4a5-0152-4c62-b73d-0294144de48d","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/GuardrailsDemo.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"29c033ea-9d47-4cc1-baf3-5d69a42f911c","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/GuardrailsDemo.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"JMcrjKXYPnQt+UchIzAgp39IJ5ckhA9zL/EQCWp9MNk=","text":"Guardrails Output Parsing Download Data Load documents build the VectorStoreIndex Define Query Guardrails Spec Query Index   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  If you re opening this Notebook on colab you will probably need to install LlamaIndex Define custom QA and Refine Prompts Define Guardrails Spec","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"0bae2684-0759-4758-bb5d-753009b7e95d":{"id_":"0bae2684-0759-4758-bb5d-753009b7e95d","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/reading.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"96d2b64c-4f63-489d-986e-4450d4d0896d","metadata":{"url":"https://docs.llamaindex.ai/en/stable/getting_started/reading.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"D6MO0+AnGfkLT8GJ+TwDjvbcReZi+b4iOgTDlbgroSU=","text":"How to read these docs Before you start Structure of these docs   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Welcome to the LlamaIndex documentation We ve tried hard to make these docs approachable regardless of your experience level with LlamaIndex and with LLMs and generative AI in general LlamaIndex is a Python library so you should have Python installed and a basic working understanding of how to write it If you prefer JavaScript we recommend trying out our TypeScript package Many of our examples are formatted as Notebooks by which we mean Jupyter style notebooks You don t have to have Jupyter installed you can try out most of our examples on a hosted service like Google Colab Our docs are structured so you should be able to roughly progress simply by moving down the sidebar on the left or just hitting the next link at the bottom of each page Getting started The section you re in right now We can get you going from knowing nothing about LlamaIndex and LLMs Install the library write your first demo in five lines of code learn more about the high level concepts of LLM applications and then see how you can customize the five line example to meet your needs Use cases If you re a dev trying to figure out whether LlamaIndex will work for your use case we have an overview of the types of things you can build Understanding LlamaIndex Once you ve completed the Getting Started section this is the next place to go In a series of bite sized tutorials we ll walk you through every stage of building a production LlamaIndex application and help you level up on the concepts of the library and LLMs in general as you go Optimizing Already got a working LlamaIndex application and looking to further refine it Our optimizing section will walk you through the first things you should try like your embedding model and chunk size through progressively more complex and subtle customizations all the way to fine tuning your model Module guides Arranged in the same order of building an LLM application as our Understanding section these are comprehensive lower level guides to the individual components of LlamaIndex and how to use them","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"5c83671d-1444-429c-a8ac-9e14e6d3d938":{"id_":"5c83671d-1444-429c-a8ac-9e14e6d3d938","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"421b7efd-f942-40c9-99cd-9510216cb669","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"fv//e8nQKpXdQBA5zG2khIdqj1dH9/OvFiY7A40ugqU=","text":"Lower Level Agent API High Level Agent Architecture Benefits Usage Pattern Additional Module Guides   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes We offer a lower level agent API that offers a host of capabilities beyond simply executing a user query end to end These capabilities let you step through and control the agent in a much more granular fashion The end goal is that you can create reliable agentic software systems over your data We took inspiration from the Agent Protocol the OpenAI Assistants API and of course a host of agent research papers NOTE This is still under development so interfaces may change In fact we d love to get your feedback on how to make this better Our agents are composed of AgentRunner objects that interact with AgentWorkers AgentRunners are orchestrators that store state including conversational memory create and maintain tasks run steps through each task and offer the user facing high level interface for users to interact with AgentWorkers control the step wise execution of a Task Given an input step an agent worker is responsible for generating the next step They can be initialized with parameters and act upon state passed down from the Task TaskStep objects but do not inherently store state themselves The outer AgentRunner is responsible for calling an AgentWorker and collecting aggregating the results Some auxiliary classes Task high level task takes in a user query passes along other info like memory TaskStep represents a single step Feed this in as input to AgentWorker get back a TaskStepOutput Completing a Task can involve multiple TaskStep TaskStepOutput Output from a given step execution Outputs whether or not a task is done  Here are some key benefits to using this lower level API Decouple task creation from execution control when you want to execute a given task Get greater debuggability into the execution of each step Get greater visibility view completed steps and next steps Coming Soon Steerability directly control modify intermediate steps by injecting human feedback Abandon task give up if a task has derailed throughout the course of execution without affecting the core agent memory Coming Soon Undoing a step Easier Customization it s easy to subclass implement new agent algorithms incl ReAct OpenAI but also plan solve LLMCompiler by implementing an AgentWorker You can either use an OpenAIAgent or ReActAgent or create your own via the AgentRunner and AgentWorker NOTE The older legacy implementations of OpenAIAgent and ReActAgent which did not allow for step wise execution are still available via Check out our lower level agent module guides for more details","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"8b362c6d-58b3-412e-837f-6589d79cfb47":{"id_":"8b362c6d-58b3-412e-837f-6589d79cfb47","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/pydantic_tree_summarize.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"7b6485e4-7caa-41db-9063-017a0cc8d782","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/pydantic_tree_summarize.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"E4ScJ785zwAiFZ0MTj5PQuzE5toBNFu0jwbgqgDDdFU=","text":"Pydantic Tree Summarize Download Data Load Data Summarize Create pydantic model to structure response Inspect the response  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we demonstrate how to use tree summarize with structured outputs Specifically tree summarize is used to output pydantic objects Here we see the response is in an instance of our Biography class","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"1fd07a5b-e2e4-4fa7-95b6-0355a2256656":{"id_":"1fd07a5b-e2e4-4fa7-95b6-0355a2256656","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/output_parser.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"7b932c8e-e090-4067-9ab3-61d9c1300d11","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/output_parser.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"NlmnE99vH0g9FcKe0qg+GwsIOmLmhs98CWHHexhvqS8=","text":"Output Parsing Modules Guardrails Langchain Guides   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes LlamaIndex supports integrations with output parsing modules offeredby other frameworks These output parsing modules can be used in the following ways To provide formatting instructions for any prompt query through output_parser format To provide parsing for LLM outputs through output_parser parse Guardrails is an open source Python package for specification validation correction of output schemas See below for a code example Output Langchain also offers output parsing modules that you can use within LlamaIndex Output Examples","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"092c7ac1-2e31-40e5-9bcf-8b4a27997006":{"id_":"092c7ac1-2e31-40e5-9bcf-8b4a27997006","metadata":{"url":"https://docs.llamaindex.ai/en/stable/#"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"71a1f5d3-9ffb-49bb-9519-e23fe3cfcf22","metadata":{"url":"https://docs.llamaindex.ai/en/stable/#"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"7qEyAQXDeE8/n75yoqbcwtkg9B1g44+Tkd5uyO8tnWM=","text":"Welcome to LlamaIndex Why Context Augmentation Why LlamaIndex for Context Augmentation Who is LlamaIndex for Getting Started Ecosystem Community Associated projects  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes LlamaIndex is a data framework for LLM based applications which benefit from context augmentation Such LLM systems have been termed as RAG systems standing for Retrieval Augemented Generation LlamaIndex provides the essential abstractions to more easily ingest structure and access private or domain specific data in order to inject these safely and reliably into LLMs for more accurate text generation It s available in Python these docs and Typescript Tip Updating to LlamaIndex v0 10 0 Check out the migration guide LLMs offer a natural language interface between humans and data Widely available models come pre trained on huge amounts of publicly available data like Wikipedia mailing lists textbooks source code and more However while LLMs are trained on a great deal of data they are not trained on your data which may be private or specific to the problem you re trying to solve It s behind APIs in SQL databases or trapped in PDFs and slide decks You may choose to fine tune a LLM with your data but Training a LLM is expensive Due to the cost to train it s hard to update a LLM with latest information Observability is lacking When you ask a LLM a question it s not obvious how the LLM arrived at its answer Instead of fine tuning one can a context augmentation pattern called Retrieval Augmented Generation RAG to obtain more accurate text generation relevant to your specific data RAG involves the following high level steps Retrieve information from your data sources first Add it to your question as context and Ask the LLM to answer based on the enriched prompt In doing so RAG overcomes all three weaknesses of the fine tuning approach There s no training involved so it s cheap Data is fetched only when you ask for them so it s always up to date LlamaIndex can show you the retrieved documents so it s more trustworthy Firstly LlamaIndex imposes no restriction on how you use LLMs You can still use LLMs as auto complete chatbots semi autonomous agents and more see Use Cases on the left It only makes LLMs more relevant to you LlamaIndex provides the following tools to help you quickly standup production ready RAG systems Data connectors ingest your existing data from their native source and format These could be APIs PDFs SQL and much more Data indexes structure your data in intermediate representations that are easy and performant for LLMs to consume Engines provide natural language access to your data For example Query engines are powerful retrieval interfaces for knowledge augmented output Chat engines are conversational interfaces for multi message back and forth interactions with your data Data agents are LLM powered knowledge workers augmented by tools from simple helper functions to API integrations and more Application integrations tie LlamaIndex back into the rest of your ecosystem This could be LangChain Flask Docker ChatGPT or anything else LlamaIndex provides tools for beginners advanced users and everyone in between Our high level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code For more complex applications our lower level APIs allow advanced users to customize and extend any module data connectors indices retrievers query engines reranking modules to fit their needs To install the library pip install llama index We recommend starting at how to read these docs which will point you to the right place based on your experience level To download or contribute find LlamaIndex on Github https github com jerryjliu llama_index PyPi LlamaIndex https pypi org project llama index GPT Index duplicate https pypi org project gpt index Github https github com run llama LlamaIndexTS Docs https ts llamaindex ai LlamaIndex TS https www npmjs com package llamaindex Need help Have a feature suggestion Join the LlamaIndex community Twitter https twitter com llama_index Discord https discord gg dGcwcsnxhU LlamaHub https llamahub ai A large and growing collection of custom data connectors LlamaLab https github com run llama llama lab Ambitious projects built on top of LlamaIndex","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"25061e5a-3529-4c3b-8cbd-2a103d222636":{"id_":"25061e5a-3529-4c3b-8cbd-2a103d222636","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/usage_pattern.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"a460aa38-db76-4cee-9647-72c2bc091efc","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/usage_pattern.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"wGSEnAOmgjXSazanJCprTcGNHX9b3nmqCa69s+X6SRM=","text":"Usage Pattern Using with our Agents Using with LangChain   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes You can create custom LlamaHub Tool Specs and Tools or they can be imported from the llama hub package They can be plugged into our native agents or LangChain agents To use with our OpenAIAgent Full Tool details can be found on our LlamaHub page Each tool contains a Usage section showing how that tool can be used To use with a LangChain agent simply convert tools to LangChain tools with to_langchain_tool","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"6a31557a-c484-46d5-822b-76c481aad377":{"id_":"6a31557a-c484-46d5-822b-76c481aad377","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/structured_outputs.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"0450820c-cdb5-4767-9ffd-a5150922352f","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/structured_outputs.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"1aLip7oARfbFHiwUbwG3DF+CTi0qSXI4iFmpzz2PBj8=","text":"Structured Outputs Anatomy of a Structured Output Function   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes The ability of LLMs to produce structured outputs are important for downstream applications that rely on reliably parsing output values LlamaIndex itself also relies on structured output in the following ways Document retrieval Many data structures within LlamaIndex rely on LLM calls with a specific schema for Document retrieval For instance the tree index expects LLM calls to be in the format ANSWER number Response synthesis Users may expect that the final response contains some degree of structure e g a JSON output a formatted SQL query etc LlamaIndex provides a variety of modules enabling LLMs to produce outputs in a structured format We provide modules at different levels of abstraction Output Parsers These are modules that operate before and after an LLM text completion endpoint They are not used with LLM function calling endpoints since those contain structured outputs out of the box Pydantic Programs These are generic modules that map an input prompt to a structured output represented by a Pydantic object They may use function calling APIs or text completion APIs output parsers These can also be integrated with query engines Pre defined Pydantic Program We have pre defined Pydantic programs that map inputs to specific output types like dataframes See the sections below for an overview of output parsers and Pydantic programs Here we describe the different components of an LLM powered structured output function The pipeline depends on whether you re using a generic LLM text completion API or an LLM function calling API  With generic completion APIs the inputs and outputs are handled by text prompts The output parser plays a role before and after the LLM call in ensuring structured outputs Before the LLM call the output parser canappend format instructions to the prompt After the LLM call the output parser can parse the output to the specified instructions With function calling APIs the output is inherently in a structured format and the input can take in the signature of the desired object The structured output just needs to be cast in the right object format e g Pydantic","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"2e5470c7-bd11-46f1-a8e4-d57e7fa70750":{"id_":"2e5470c7-bd11-46f1-a8e4-d57e7fa70750","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/multi_modal/image_to_image_retrieval.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"5907e9b1-4175-4df0-9ecd-5a6f14ea8c3f","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/multi_modal/image_to_image_retrieval.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"RkiPVNAuuUyGx0wW8au9buPHPhP+WJiAEuajw5Ut6mQ=","text":"Image to Image Retrieval using CLIP embedding and image correlation reasoning using GPT4V Download images and texts from Wikipedia Build Multi Modal index and Vector Store to index both text and images from Wikipedia Plot input query image Retrieve images from Multi Modal Index given the image query Using Image Query Engine Plot images from Wikipedia 1 Image to Image Retrieval Results 2 GPT4V Reasoning Retrieved Images based on Input Image  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we show how to build a Image to Image retrieval using LlamaIndex with GPT4 V and CLIP LlamaIndex Image to Image Retrieval Images embedding index CLIP embeddings from OpenAI for images Framework LlamaIndex Steps Download texts images pdf raw files from Wikipedia pages Build Multi Modal index and vetor store for both texts and images Retrieve relevant images given a image query using Multi Modal Retriever Using GPT4V for reasoning the correlations between the input image and retrieved images Inside Query Engine there are few steps Retrieve relevant images based on input image Compose the image_qa_template by using the promt text Sending top k retrieved images and image_qa_template for GPT4V to answer synthesis","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"fbb90e75-9122-4eaf-842f-f162e601e3ef":{"id_":"fbb90e75-9122-4eaf-842f-f162e601e3ef","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_forced_function_call.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"f3005e13-8cb0-4836-8838-a1abd6594361","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_forced_function_call.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"DtpDApI6y9Yi8QRneLiqrfcJPYDiIoFGkQaZARH0GHQ=","text":"OpenAI agent specifying a forced function call Auto function call Forced function call None function call   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  If you re opening this Notebook on colab you will probably need to install LlamaIndex The agent automatically selects the useful add tool The agent is forced to call the useless_tool before selecting the add tool The agent is forced to not use a tool","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"7dc5342d-be0d-4770-ba9a-332f8adf8c5f":{"id_":"7dc5342d-be0d-4770-ba9a-332f8adf8c5f","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_sub_question.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"02526326-f8ae-47ef-8f7d-47be1b8d19ea","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_sub_question.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"ZFzTc8yrirxiZrpqjXRdvpBuyng1VVavpd8uF04aRKc=","text":"Guidance for Sub Question Query Engine Guidance Question Generator Using Guidance Question Generator with Sub Question Query Engine Prepare data and base query engines Construct sub question query engine and run some queries  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we showcase how to use guidance to improve the robustness of our sub question query engine The sub question query engine is designed to accept swappable question generators that implement the BaseQuestionGenerator interface To leverage the power of guidance we implemented a new GuidanceQuestionGenerator powered by our GuidancePydanticProgram Unlike the default LLMQuestionGenerator guidance guarantees that we will get the desired structured output and eliminate output parsing errors If you re opening this Notebook on colab you will probably need to install LlamaIndex Let s test it out Download Data","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"f1ff7979-d455-415c-8a89-1302595d662b":{"id_":"f1ff7979-d455-415c-8a89-1302595d662b","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/agent_runner_rag.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b9e1ddcd-6c13-45f6-8d1b-894ea7f00932","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/agent_runner_rag.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"5oeuoOm5npZiO/HOIz/ZjJ+iFawriony1iHofvGL72Q=","text":"Controllable Agents for RAG Setup Data Setup Agent Run Some Queries Define Toolset Setup OpenAI Agent Out of the box Test Step Wise Execution Inspect Steps Tasks  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Adding agentic capabilities on top of your RAG pipeline can allow you to reason over much more complex questions But a big pain point for agents is the lack of steerability transparency An agent may tackle a user query through chain of thought planning which requires repeated calls to an LLM During this process it can be hard to inspect what s going on or stop correct execution in the middle This notebook shows you how to use our brand new lower level agent API which allows controllable step wise execution on top of a RAG pipeline We showcase this over Wikipedia documents Here we load a simple dataset of different cities from Wikipedia Define LLM Callback Manager In this section we define our tools and setup the agent Each tool here corresponds to a simple top k RAG pipeline over a single document Wikipedia page We setup an OpenAI Agent through its components an AgentRunner as well as an OpenAIAgentWorker We now demonstrate the capabilities of our step wise agent framework We show how it can handle complex queries both e2e as well as step by step We can then show how we can steer the outputs We now break this query down into steps We first create a task object from the user query We can then start running through steps or even interjecting our own This returns a Task object which contains the input additional state in extra_state and other fields Now let s try executing a single step of this task When we inspect the logs and the output we see that the first part was executed the demographics of Houston We can also take a look at the upcoming step NOTE Currently the input is not shown since execution of a step purely depends on internal memory This is something we re working on If you wanted to pause execution now you can you can take the intermediate results without completing the agent flow NOTE The memory of the agent agent memory isn t modified until the task is complete and committed so if you pause now the memory won t be committed This is good in case the execution fails Let s run the next two steps Since the steps look good we are now ready to call finalize_response get back our response This will also commit the task execution to the memory object present in our agent_runner We can inspect it We can inspect current and previous tasks and steps This gives you greater transparency into what the agent has processed","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"fc312125-dfde-47f6-ab84-1f08194f401a":{"id_":"fc312125-dfde-47f6-ab84-1f08194f401a","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/chatbots.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"99a83839-94d6-4a6b-9411-841bfe1e978e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/chatbots.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"cWAUhLBmHYszGJp7J9TNYlI5Eb2ifEAT4CDuymOB5C8=","text":"Chatbots External sources   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Chatbots are another extremely popular use case for LLMs Instead of a single question and answer a chatbot can handle multiple back and forth queries and answers getting clarification or answering follow up questions LlamaIndex gives you the tools to build knowledge augmented chatbots and agents Here are some relevant resources Building a chatbot tutorial create llama a command line tool that generates a full stack chatbot application for you SECinsights ai an open source application that uses LlamaIndex to build a chatbot that answers questions about SEC filings RAGs a project inspired by OpenAI s GPTs that lets you build a low code chatbot over your data using Streamlit Our OpenAI agents are all chat bots in nature Building a chatbot with Streamlit","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"bdcaad70-1df5-48ae-b208-16b4d42963e4":{"id_":"bdcaad70-1df5-48ae-b208-16b4d42963e4","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/usecases/email_data_extraction.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"17818c5d-a207-431b-aaf4-b3579519587f","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/usecases/email_data_extraction.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"uGBLtTyMAl3XKtkg6r4XCYspoW7McOoDTuA8mcUcKuU=","text":"Email Data Extraction Add required packages Enable Logging and Set up OpenAI API Key Set Up Expected JSON Output Definition JSON Schema Load content from eml msg file Use LLM function to extract content in JSON format For outlook message   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  OpenAI functions can be used to extract data from Email This is another example of getting structured data from unstructured conent using LLamaIndex The primary objective of this example is to transform raw email content into an easily interpretable JSON format exemplifying a practical application of language models in data extraction Extracted structued JSON data can then be used in any downstream application We will use a sample email as shown in below image This email mimics a typical daily communication sent by ARK Investment to its subscribers This sample email includes detailed information about trades under their Exchange Traded Funds ETFs By using this specific example we aim to showcase how we can effectively extract and structure complex financial data from a real world email scenario transforming it into a comprehensible JSON format  You will need following libraries along with LlamaIndex unstructured msg A package for handling unstructured data required to get content from eml and msg format In this step we set up logging to monitor the program s execution and debug if needed We also configure the OpenAI API key essential for utilizing OpenAI services Replace YOUR_KEY_HERE with your actual OpenAI API key Here we define a Python class named EmailData using the Pydantic library This class models the structure of the data we expect to extract from emails including sender receiver the date and time of the email etfs having list of shares traded under that ETF In this step we will use the UnstructuredReader from the llama hub to load the content of an eml email file or msg Outlook file This file s contents are then stored in a variable for further processing In the final step we utilize the llama_index package to create a prompt template for extracting insights from the loaded email An instance of the OpenAI model is used to interpret the email content and extract the relevant information based on our predefined EmailData schema The output is then converted to a dictionary format for easy viewing and processing","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"1aa2054a-e33b-4427-b4d4-0d361580f4de":{"id_":"1aa2054a-e33b-4427-b4d4-0d361580f4de","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_context_retrieval.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"6133e2a6-aff5-424e-91fa-6e5f2025a2b1","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_context_retrieval.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"5I9d8KxMIm8ZG0VkPhQVy51sAeiA+v0S4OBDgC3W7UY=","text":"Context Augmented OpenAI Agent Initial Setup Try Context Augmented Agent Use Uber 10 Q as context use Calculator as Tool  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this tutorial we show you how to use our ContextRetrieverOpenAIAgent implementationto build an agent on top of OpenAI s function API and store index an arbitrary number of tools Our indexing retrieval modules help to remove the complexity of having too many functions to fit in the prompt Here we setup a ContextRetrieverOpenAIAgent This agent will perform retrieval first before calling any tools This can help ground the agent s tool picking and answering capabilities in context If you re opening this Notebook on colab you will probably need to install LlamaIndex Download Data Here we augment our agent with context in different settings toy context we define some abbreviations that map to financial terms e g R Revenue We supply this as context to the agent","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"c7704be0-c189-41d5-8a21-21fa8781792c":{"id_":"c7704be0-c189-41d5-8a21-21fa8781792c","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/react_agent_with_query_engine.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"3c7d14a4-5e84-466c-ad2f-a68b2ab93eb3","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/react_agent_with_query_engine.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"tg/Z4cSZHHT9vp6uCGBatVye4qaAceG/AibuUCbDP4E=","text":"ReAct Agent with Query Engine RAG Tools Build Query Engine Tools Setup ReAct Agent Run Some Example Queries Compare gpt 3 5 turbo vs gpt 3 5 turbo instruct Taking a look at a turbo instruct agent Try more complex queries Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this section we show how to setup an agent powered by the ReAct loop for financial analysis The agent has access to two tools one to query the 2021 Lyft 10 K and the other to query the 2021 Uber 10 K We try two different LLMs gpt 3 5 turbo gpt 3 5 turbo instruct Note that you can plug in any LLM that exposes a text completion endpoint Download Data Here we setup two ReAct agents one powered by standard gpt 3 5 turbo and the other powered by gpt 3 5 turbo instruct You can optionally specify context which will be added to the core ReAct system prompt We run some example queries using the agent showcasing some of the agent s abilities to do chain of thought reasoning and tool use to synthesize the right answer We also show queries Async execution Here we try another query with async execution We compare the performance of the two agents in being able to answer some complex queries We compare gpt 3 5 turbo with gpt 3 5 turbo instruct agents on more complex queries Observation The turbo instruct agent seems to do worse on agent reasoning compared to the regular turbo model Of course this is subject to further observation","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"85de3b07-d6ad-44e6-b027-09060d68215a":{"id_":"85de3b07-d6ad-44e6-b027-09060d68215a","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/react_agent.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"4f5cb7e4-0180-4af0-94de-ff3864c9724e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/react_agent.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"dWn7yCJlK11WuzXR+aAzIfzVk7KMlZ2xTUHIa0dJoY0=","text":"ReAct Agent A Simple Intro with Calculator Tools Define Function Tools Run Some Queries View Prompts gpt 3 5 turbo gpt 4 Customizing the Prompt  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This is a notebook that showcases the ReAct agent over very simple calculator tools no fancy RAG pipelines or API calls We show how it can reason step by step over different tools to achieve the end goal If you re opening this Notebook on colab you will probably need to install LlamaIndex We setup some trivial multiply and add tools Note that you can define arbitrary functions and pass it to the FunctionTool which will process the docstring and parameter signature Let s take a look at the core system prompt powering the ReAct agent Within the agent the current conversation history is dumped below this line For fun let s try instructing the agent to output the answer along with reasoning in bullet points See Additional Rules section","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"60a5e198-03a9-4f2b-ba6a-df0d2ccaff98":{"id_":"60a5e198-03a9-4f2b-ba6a-df0d2ccaff98","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/LangchainOutputParserDemo.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"964090d7-bff8-48f0-abc7-a67bbe81a7bf","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/LangchainOutputParserDemo.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"ahiOp9N30NkF/1uhDjVpekotWEeW7Bs37qKwJklq8A8=","text":"Langchain Output Parsing Load documents build the VectorStoreIndex Define Query Langchain Output Parser Query Index   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  Download Data Define custom QA and Refine Prompts","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"fc432247-6718-4fe1-b322-453cb1c51646":{"id_":"fc432247-6718-4fe1-b322-453cb1c51646","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/multi_modal/multi_modal_retrieval.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"d067f7a4-8568-4dd2-b968-f9820366fd3c","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/multi_modal/multi_modal_retrieval.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"WV6zxQNvsLrhmb8ql0WtTX/RMOAzi85/y1hwpOvHtxE=","text":"Multi Modal Retrieval using GPT text embedding and CLIP image embedding for Wikipedia Articles Load and Download Multi Modal datasets including texts and images from Wikipedia Parse Wikipedia Images and texts Load into local folder Build Multi Modal Vector Store using Text and Image embeddings under different collections Get Multi Modal retrieval results for some example queries Plot downloaded Images from Wikipedia Build a separate CLIP image embedding index under a differnt collection wikipedia_img  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we show how to build a Multi Modal retrieval system using LlamaIndex Wikipedia Text embedding index Generate GPT text embeddings from OpenAI for texts Wikipedia Images embedding index CLIP embeddings from OpenAI for images Query encoder Encoder query text for text index using GPT embedding Encoder query text for image index using CLIP embedding Framework LlamaIndex Steps Download texts and images raw files for Wikipedia articles Build text index for vector store using GPT embeddings Build image index for vector store using CLIP embeddings Retrieve relevant text and image simultaneously using different query encoding embeddings and vector stores Parse wikipedia articles and save into local folder Node ID e30e1817 4e31 4047 be5d 37502560920cSimilarity 0 808149809808292Text BTS Korean RR Bangtan Sonyeondan lit Bulletproof Boy Scouts also known as the Bangtan Boys is a South Korean boy band formed in 2010 The band consists of Jin Suga J Hope RM Jimi Node ID 024f3296 37c8 46d5 a184 2f78c621a99fSimilarity 0 7987048642063129Text Fandom According to Kyung Hyun Kim BTS s rise was facilitated by a great increase in music video programming and consumption on YouTube and the coming of an idol empire including merchand Node ID c564ccf4 a94f 408f 8b21 224538dc2e94Similarity 0 7838098925118134Text History 2010 2014 Formation and early years BTS was formed in 2010 after Big Hit Entertainment CEO Bang Si hyuk wanted to form a hip hop group around RM Kim Nam joon an undergr Node ID e002927c 0bf5 482b a0a1 0ee2f3cd48f9Similarity 0 8675476190545354Text Vincent Willem van Gogh Dutch v ns nt l v x 30 March 1853 29 July 1890 was a Dutch Post Impressionist painter who is among the most famous and influential figures in the history Node ID 69ef1c64 a5b4 468c a58c 7d36151961a7Similarity 0 8661792475490765Text Flowers Van Gogh painted several landscapes with flowers including roses lilacs irises and sunflowers Some reflect his interests in the language of colour and also in Japanese ukiy Node ID f971a611 a8b9 48b4 a81b d3856438aab8Similarity 0 8616832203971132Text Portraits Van Gogh said portaiture was his greatest interest What I m most passionate about much much more than all the rest in my profession he wrote in 1890 is the portrait the Node ID 8c14be3e 345a 4764 9b64 dacff771bc04Similarity 0 8689195893277072Text Tourism and conventions Tourism is one of San Francisco s most important private sector industries accounting for more than one out of seven jobs in the city The city s frequent portraya Node ID 22aa7d86 017f 433d 98dc 4007d9f67c17Similarity 0 8452524742723133Text LGBT San Francisco has long had an LGBT friendly history It was home to the first lesbian rights organization in the United States Daughters of Bilitis the first openly gay person to ru Node ID 3846a17a 79d8 415e 9bcf 76c818b27203Similarity 0 8329496262980858Text Parks and recreation Several of San Francisco s parks and nearly all of its beaches form part of the regional Golden Gate National Recreation Area one of the most visited units of the Natio Node ID 214c61be dad6 403c b301 bc2320b87e7aSimilarity 0 7808396168295813Text The Tesla Model S is a battery electric full size luxury sedan with a liftback body style built by Tesla Inc since 2012 The Model S features a battery powered dual motor all wheel drive layout Node ID 15b737b4 90e3 443a 87aa 13a7d7e80b87Similarity 0 7807424063856144Text P100D The P100D outputs 439 kW 589 hp and 1 248 N m 920 lbf ft torque on a dynamometer As of March 2017 P100D was the world s quickest production vehicle with a NHRA rolling start to 6 Node ID e134452b 3031 47b0 a20c df4fe32f1bcfSimilarity 0 7754107325086438Text Recalls As of December 2021 Tesla had had seven Model S recalls On June 14 2013 Tesla recalled Model S vehicles manufactured between May 10 2013 and June 8 2013 due to improper meth Node ID ff85b136 08c8 465d 96f5 a554c65067d8Similarity 0 8461934674061043Text Personality Batman s primary character traits can be summarized as wealth physical prowess deductive abilities and obsession The details and tone of Batman comic books have varied ov Node ID 55f5b842 6fd0 4e45 aef2 27f74f670e82Similarity 0 8229623965891602Text Batman is a superhero appearing in American comic books published by DC Comics The character was created by artist Bob Kane and writer Bill Finger and debuted in the 27th issue of the comic book Node ID cb4755db 088e 46af 92b2 3a4a3649d9feSimilarity 0 8218281955244808Text Enemies Batman faces a variety of foes ranging from common criminals to outlandish supervillains Many of them mirror aspects of the Batman s character and development often having tragic","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"fdc945d2-7a93-4a53-9a11-7035885c170b":{"id_":"fdc945d2-7a93-4a53-9a11-7035885c170b","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/multi_modal/gpt4v_multi_modal_retrieval.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"5ca838ca-b2dd-4130-a466-a103145d63c7","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/multi_modal/gpt4v_multi_modal_retrieval.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"0crgN4dWkBNgtpJzeBtUEOjIVVB7Dc9A0gNNSzK/o3c=","text":"Advanced Multi Modal Retrieval using GPT4V and Multi Modal Index Retriever Download images from Tesla website for GPT4V image reasoning Generate image reasoning from GPT4V Multi Modal LLM Generating text pdf images data from raw files Wikipedia SEC files for Multi Modal Index Retrieval Build Multi modal index and vector store to index both text and images Retrieve and query texts and images from our Multi Modal Index Plot input images Using GPT4V to understand those input images 1 Retrieval Augmented Captioning 2 Multi Modal RAG Querying  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we show how to build a Multi Modal retrieval system using LlamaIndex with GPT4 V and CLIP LlamaIndex Multi Modal Retrieval Text embedding index Generate GPT text embeddings Images embedding index CLIP embeddings from OpenAI for images Encoding queries Encode query text for text index using ada Encode query text for image index using CLIP Framework LlamaIndex Steps Using Multi Modal LLM GPT4V class to undertand multiple images Download texts images pdf raw files from related Wikipedia articles and SEC 10K report Build Multi Modal index and vetor store for both texts and images Retrieve relevant text and image simultaneously using Multi Modal Retriver according to the image reasoning from Step 1 We show two examples leveraging multi modal retrieval Retrieval Augmented Captioning In the first example we perform multi modal retrieval based on an existing image caption to return more relevant context We can then continue to query the LLM for related vehicles Multi modal RAG Querying In the second example given a user query we first retrieve a mix of both text and images and feed it to an LLM for synthesis Node ID 8a67ab30 545c 46ee a25f 64c95a4571beSimilarity 0 7758026357212682Text Reception Consumer Reports wrote that the all wheel drive Model X 90D largely disappoints as rear doors are prone to pausing and stopping the second row seats that cannot be folded and the Node ID 5db1e928 197d 41d4 b1c1 34d2bcf1cc4dSimilarity 0 7712850768830459Text Design and technology Body and chassis The i3 was the first mass production car with most of its internal structure and body made of carbon fiber reinforced plastic CFRP BMW took Node ID 89e533c6 3e25 4933 b58a 7d42ac67e957Similarity 0 768609543932987Text Autoshift Introduced in mid 2021 the Plaid and Long Range versions of the Model S feature no steering column mounted shift stalk instead the Model S uses cameras to infer whether to shif Node ID c9dac736 51ce 429a 9b77 96c95a00d91fSimilarity 0 8241315758378377Text Models The Taycan is currently offered as a 4 door saloon model and a 4 door estate model the Taycan Cross Turismo Other planned variants include a two door coupe and convertible models wh Node ID 531c87f5 fcc4 453e a013 fa6c9a3a7d24Similarity 0 822575963523647Text The Porsche Taycan is a battery electric saloon and shooting brake produced by German automobile manufacturer Porsche The concept version of the Taycan named the Porsche Mission E debuted at the","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"8adcc881-cb97-413e-9f44-2bd06ecf6b9e":{"id_":"8adcc881-cb97-413e-9f44-2bd06ecf6b9e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_sub_question.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"6dd1592b-c2ad-437e-8ea2-bea0f1cca0f4","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_sub_question.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"ZFzTc8yrirxiZrpqjXRdvpBuyng1VVavpd8uF04aRKc=","text":"Guidance for Sub Question Query Engine Guidance Question Generator Using Guidance Question Generator with Sub Question Query Engine Prepare data and base query engines Construct sub question query engine and run some queries  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we showcase how to use guidance to improve the robustness of our sub question query engine The sub question query engine is designed to accept swappable question generators that implement the BaseQuestionGenerator interface To leverage the power of guidance we implemented a new GuidanceQuestionGenerator powered by our GuidancePydanticProgram Unlike the default LLMQuestionGenerator guidance guarantees that we will get the desired structured output and eliminate output parsing errors If you re opening this Notebook on colab you will probably need to install LlamaIndex Let s test it out Download Data","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"20ec45e6-9d2e-49ff-8b46-9b3615758590":{"id_":"20ec45e6-9d2e-49ff-8b46-9b3615758590","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/llamahub_tools_guide.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b78e90a4-3532-4ded-9ab3-29d33103fcac","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/llamahub_tools_guide.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"HQzlp35t7+2Y6rHOHoqfH0d4SSH3M77q92XNjzgSPgc=","text":"LlamaHub Tools Guide Tool Specs Utility Tools OnDemandLoaderTool LoadAndSearchToolSpec  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes We offer a rich set of Tool Specs that are offered through LlamaHub These tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions We also provide a list of utility tools that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data Coming soon Oftentimes directly querying an API can return a massive volume of data which on its own may overflow the context window of the LLM or at the very least unnecessarily increase the number of tokens that you are using To tackle this we ve provided an initial set of utility tools in LlamaHub Tools utility tools are not conceptually tied to a given service e g Gmail Notion but rather can augment the capabilities of existing Tools In this particular case utility tools help to abstract away common patterns of needing to cache index and query data that s returned from any API request Let s walk through our two main utility tools below This tool turns any existing LlamaIndex data loader BaseReader class into a tool that an agent can use The tool can be called with all the parameters needed to trigger load_data from the data loader along with a natural language query string During execution we first load data from the data loader index it for instance with a vector store and then query it on demand All three of these steps happen in a single tool call Oftentimes this can be preferable to figuring out how to load and index API data yourself While this may allow for data reusability oftentimes users just need an ad hoc index to abstract away prompt window limitations for any API call A usage example is given below The LoadAndSearchToolSpec takes in any existing Tool as input As a tool spec it implements to_tool_list and when that function is called two tools are returned a load tool and then a search tool The load Tool execution would call the underlying Tool and the index the output by default with a vector index The search Tool execution would take in a query string as input and call the underlying index This is helpful for any API endpoint that will by default return large volumes of data for instance our WikipediaToolSpec will by default return entire Wikipedia pages which will easily overflow most LLM context windows Example usage is shown below","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"ed0f6c6a-65f4-4a28-b824-799e66dccb52":{"id_":"ed0f6c6a-65f4-4a28-b824-799e66dccb52","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/openai_pydantic_program.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"013e7e99-6346-48ba-b2d4-904dbe44800d","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/openai_pydantic_program.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"G4TPSnLq8Gm4CpvOwPfQjeG5HTHmYw6QX+FpIIyjqzU=","text":"OpenAI Pydantic Program Extraction into Album Extracting List of Album with Parallel Function Calling Extraction into Album Streaming Extraction into DirectoryTree object Without docstring in Model With docstring in Model  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This guide shows you how to generate structured data with new OpenAI API via LlamaIndex The user just needs to specify a Pydantic object We demonstrate two settings Extraction into an Album object which can contain a list of Song objects Extraction into a DirectoryTree object which can contain recursive Node objects This is a simple example of parsing an output into an Album schema which can contain multiple songs If you re opening this Notebook on colab you will probably need to install LlamaIndex Define output schema without docstring Define openai pydantic program Run program to get structured output Run program to get structured output The output is a valid Pydantic object that we can then use to call functions APIs With the latest parallel function calling feature from OpenAI we can simultaneously extract multiple structured data from a single prompt To do this we need to pick one of the latest models e g gpt 3 5 turbo 1106 and set allow_multiple to True in our OpenAIPydanticProgram if not it will only return the first object and raise a warning The output is a list of valid Pydantic object We also support streaming a list of objects through our stream_list function Full credits to this idea go to openai_function_call repo https github com jxnl openai_function_call tree main examples streaming_multitask This is directly inspired by jxnl s awesome repo here https github com jxnl openai_function_call That repository shows how you can use OpenAI s function API to parse recursive Pydantic objects The main requirement is that you want to wrap a recursive Pydantic object with a non recursive one Here we show an example in a directory setting where a DirectoryTree object wraps recursive Node objects to parse a file structure The output is a full DirectoryTree structure with recursive Node objects","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"796b31dd-11c6-4dd9-95df-ea5cdd1febc9":{"id_":"796b31dd-11c6-4dd9-95df-ea5cdd1febc9","metadata":{"url":"https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"cbe31db7-c5f3-41d0-8891-84e2f19456d4","metadata":{"url":"https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"RthW1vSKrK+JlzU5Fz3CBNUbadj65QFZo0vefCoI8dI=","text":"Agents Native OpenAIAgent Agentic Components within LlamaIndex Using LlamaIndex as as Tool within an Agent Framework LangChain ChatGPT  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Putting together an agent in LlamaIndex can be done by defining a set of tools and providing them to our ReActAgent implementation We re using it here with OpenAI but it can be used with any sufficiently capable LLM These tools can be Python functions as shown above or they can be LlamaIndex query engines You can learn more in our Agent Module Guide We have an OpenAIAgent implementation built on the OpenAI API for function calling that allows you to rapidly build agents LlamaIndex provides core modules capable of automated reasoning for different use cases over your data which makes them essentially Agents Some of these core modules are shown below along with example tutorials SubQuestionQueryEngine for Multi Document Analysis Sub Question Query Engine Intro 10Q Analysis Uber 10K Analysis Uber and Lyft Query Transformations How To Multi Step Query Decomposition Notebook Routing Usage Router Query Engine Guide Notebook LLM Reranking Second Stage Processing How To LLM Reranking Guide Great Gatsby Chat Engines Chat Engines How To LlamaIndex can be used as as Tool within an agent framework including LangChain ChatGPT These integrations are described below We have deep integrations with LangChain LlamaIndex query engines can be easily packaged as Tools to be used within a LangChain agent and LlamaIndex can also be used as a memory module retriever Check out our guides tutorials below Resources LangChain integration guide Building a Chatbot Tutorial LangChain LlamaIndex OnDemandLoaderTool Tutorial LlamaIndex can be used as a ChatGPT retrieval plugin we have a TODO to develop a more general plugin as well Resources LlamaIndex ChatGPT Retrieval Plugin","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"ae8eeded-858a-4e48-9a23-126d2aaa4b05":{"id_":"ae8eeded-858a-4e48-9a23-126d2aaa4b05","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_sub_question.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"720b1a04-15df-4ae1-8693-b55be0e081d4","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_sub_question.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"ZFzTc8yrirxiZrpqjXRdvpBuyng1VVavpd8uF04aRKc=","text":"Guidance for Sub Question Query Engine Guidance Question Generator Using Guidance Question Generator with Sub Question Query Engine Prepare data and base query engines Construct sub question query engine and run some queries  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we showcase how to use guidance to improve the robustness of our sub question query engine The sub question query engine is designed to accept swappable question generators that implement the BaseQuestionGenerator interface To leverage the power of guidance we implemented a new GuidanceQuestionGenerator powered by our GuidancePydanticProgram Unlike the default LLMQuestionGenerator guidance guarantees that we will get the desired structured output and eliminate output parsing errors If you re opening this Notebook on colab you will probably need to install LlamaIndex Let s test it out Download Data","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"40a45f8e-c248-417f-af7f-41dc055fb1c9":{"id_":"40a45f8e-c248-417f-af7f-41dc055fb1c9","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/llm_program.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b4dc32d3-441b-458e-b855-9753c2c7d552","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/llm_program.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"wXGGFFhpb1LKtyoaTQhqu+K//ztsblKs8OVDHH6Pgx4=","text":"LLM Pydantic Program Extract into Album class Define a Custom Output Parser Initialize with Pydantic Output Parser  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This guide shows you how to generate structured data with our LLMTextCompletionProgram Given an LLM as well as an output Pydantic class generate a structured Pydantic object In terms of the target object you can choose to directly specify output_cls or specify a PydanticOutputParser or any other BaseOutputParser that generates a Pydantic object in the examples below we show you different ways of extracting into the Album object which can contain a list of Song objects This is a simple example of parsing an output into an Album schema which can contain multiple songs Just pass Album into the output_cls property on initialization of the LLMTextCompletionProgram If you re opening this Notebook on colab you will probably need to install LlamaIndex Define output schema Define LLM pydantic program Run program to get structured output The output is a valid Pydantic object that we can then use to call functions APIs The above is equivalent to defining a Pydantic output parser and passing that in instead of the output_cls directly Sometimes you may want to parse an output your own way into a JSON object","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"bb68f1c2-43d6-40ba-a96a-9123b29e57e1":{"id_":"bb68f1c2-43d6-40ba-a96a-9123b29e57e1","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/agent_runner.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"9272265b-a522-41c1-bb38-444f1fe918c6","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/agent_runner.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"59Ne0/oQ9forSOTGm0GZpJVrVoT+DeF/TMFj53b6x0Q=","text":"Step wise Controllable Agents High Level Agent Architecture Notebook Walkthrough Test OpenAI Agent Test ReAct Agent Test E2E Chat Test Step Wise Execution List Out Tasks  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes This notebook shows you how to use our brand new lower level agent API which supports a host of functionalities beyond simply executing a user query to help you create tasks iterate through steps and control the inputs for each step Our agents are composed of AgentRunner objects that interact with AgentWorkers AgentRunners are orchestrators that store state including conversational memory create and maintain tasks run steps through each task and offer the user facing high level interface for users to interact with AgentWorkers control the step wise execution of a Task Given an input step an agent worker is responsible for generating the next step They can be initialized with parameters and act upon state passed down from the Task TaskStep objects but do not inherently store state themselves The outer AgentRunner is responsible for calling an AgentWorker and collecting aggregating the results If you are building your own agent you will likely want to create your own AgentWorker See below for an example This notebook shows you how to run step wise execution and full execution with agents We show you how to do execution with OpenAIAgent function calling We show you how to do execution with ReActAgent There s two main ways to initialize the agent Option 1 Initialize OpenAIAgent This is a simple subclass of AgentRunner that bundles the OpenAIAgentWorker under the hood Option 2 Initialize AgentRunner with OpenAIAgentWorker Here you import the modules and compose your own agent NOTE The old OpenAIAgent can still be imported via from llama_index agent import OldOpenAIAgent Here we re demonstrate the end to end execution of a user task through the chat function This will iterate step wise until the agent is done with the current task Now let s show the lower level API in action We do the same thing but break this down into steps We do the same experiments but with ReAct There are 3 tasks corresponding to the three runs above","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"234f2c47-bf63-4d84-96cc-aeffe66907fd":{"id_":"234f2c47-bf63-4d84-96cc-aeffe66907fd","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/df_program.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"1a6a1870-7abe-4764-89c9-d11112463ac7","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/df_program.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"57cMoGYm54BJL0m1RISnK8I8ZHoNAqJUrjf5lG0R3jg=","text":"DataFrame Structured Data Extraction Build a DF Extractor Yourself Using OpenAIPydanticProgram Use our DataFrame Programs   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This demo shows how you can extract tabular DataFrames from raw text This was directly inspired by jxnl s dataframe example here https github com jxnl openai_function_call blob main auto_dataframe py We show this with different levels of complexity all backed by the OpenAI Function API more code How to build an extractor yourself using our OpenAIPydanticProgram less code Using our out of the box DFFullProgram and DFRowsProgram objects Our OpenAIPydanticProgram is a wrapper around an OpenAI LLM that supports function calling it will return structuredoutputs in the form of a Pydantic object We import our DataFrame and DataFrameRowsOnly objects To create an output extractor you just need to 1 specify the relevant Pydantic object and 2 Add the right prompt If you re opening this Notebook on colab you will probably need to install LlamaIndex We provide convenience wrappers for DFFullProgram and DFRowsProgram This allows a simpler object creation interface than specifying all details through the OpenAIPydanticProgram","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"b5657de9-b466-49d3-be1f-9bd99b52746f":{"id_":"b5657de9-b466-49d3-be1f-9bd99b52746f","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/query_pipeline_agent.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"4e9dc911-a085-46fb-a233-a21bf6a6abd5","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/query_pipeline_agent.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"+OQ49VjcbMkJFMrnEnKqHYadNhAzAMEIC1+Z1tAGhtc=","text":"Building an Agent around a Query Pipeline Setup Setup Text to SQL Query Engine Tool Setup ReAct Agent Pipeline Setup Simple Retry Agent Pipeline for Text to SQL Setup Data Setup Observability Define Agent Input Component Define Agent Prompt Define Agent Output Parser Tool Pipeline Stitch together Agent Query Pipeline Visualize Query Pipeline Setup Agent Worker around Text to SQL Query Pipeline Run the Agent Define Core Modules Visualize Query Pipeline Define Agent Worker  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes In this cookbook we show you how to build an agent around a query pipeline Agents offer the ability to do complex sequential reasoning on top of any query DAG that you have setup Conceptually this is also one of the ways you can add a loop to the graph We show you two examples of agents you can implement a full ReAct agent that can do tool picking a simple agent that adds a retry layer around a text to sql query engine We use the chinook database as sample data Source We setup Arize Phoenix for observability Now we setup a simple text to SQL tool given a query translate text to SQL execute against database and get back a result We now setup a ReAct pipeline for a single step using our Query Pipeline syntax This is a multi part process that does the following Takes in agent inputs Calls ReAct prompt using LLM to generate next action tool or returns a response If tool action is selected call tool pipeline to execute tool collect response If response is generated get response Throughout this we ll use a variety of agent specific query components Unlike normal query pipelines these are specifically designed for query pipelines that are used in a QueryPipelineAgentWorker An AgentInputComponent that allows you to convert the agent inputs Task state dictionary into a set of inputs for the query pipeline An AgentFnComponent a general processor that allows you to take in the current Task state as well as any arbitrary inputs and returns an output In this cookbook we define a function component to format the ReAct prompt However you can put this anywhere Not used in this notebook An CustomAgentComponent similar to AgentFnComponent you can implement _run_component to define your own logic with access to Task and state It is more verbose but more flexible than AgentFnComponent e g you can define init variables and callbacks are in the base class Note that any function passed into AgentFnComponent and AgentInputComponent MUST include task and state as input variables as these are inputs passed from the agent Note that the output of an agentic query pipeline MUST be Tuple AgentChatResponse bool You ll see this below Here we define the agent input component called at the beginning of every agent step Besides passing along the input we also do initialization state modification Here we define the agent component that generates a ReAct prompt and after the output is generated from the LLM parses into a structured object Once the LLM gives an output we have a decision tree If an answer is given then we re done Process the output If an action is given we need to execute the specified tool with the specified args and then process the output Tool calling can be done via the ToolRunnerComponent module This is a simple wrapper module that takes in a list of tools and can be executed with the specified tool name every tool has a name and tool action We implement this overall module OutputAgentComponent that subclasses CustomAgentComponent Note we also implement sub_query_components to pass through higher level callback managers to the tool runner submodule We can now stitch together the top level agent pipeline agent_input react_prompt llm react_output The last component is the if else component that calls sub components This is our way to setup an agent around a text to SQL Query Pipeline Let s try the agent on some sample queries Instead of the full ReAct pipeline that does tool picking let s try a much simpler agent pipeline that only does text to SQL with retry logic We try a simple text based retry prompt where given the user input and previous conversation history can generate a modified query that outputs the right result agent input retry prompt output processor including a validation prompt","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"a8b723fb-9aff-4fdf-9ab4-dd4d5390aee6":{"id_":"a8b723fb-9aff-4fdf-9ab4-dd4d5390aee6","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/root.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"2211935e-5128-43da-a890-4c05b0b8cf67","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/root.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"e5jwlfjzADh1PeZ2HflseCDKAOTxVuMVpGyOKc7uCpA=","text":"Tools Concept Usage Pattern LlamaHub Tools Guide Blog Post  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Having proper tool abstractions is at the core of building data agents Defining a set of Tools is similar to defining any API interface with the exception that these Tools are meant for agent rather than human use We allow users to define both a Tool as well as a ToolSpec containing a series of functions under the hood A Tool implements a very generic interface simply define __call__ and also return some basic metadata name description function schema A Tool Spec defines a full API specification of any service that can be converted into a list of Tools We offer a few different types of Tools FunctionTool A function tool allows users to easily convert any user defined function into a Tool It can also auto infer the function schema QueryEngineTool A tool that wraps an existing query engine Note since our agent abstractions inherit from BaseQueryEngine these tools can also wrap other agents We offer a rich set of Tools and Tool Specs through LlamaHub For full details please check out our detailed blog post Our Tool Specs and Tools can be imported from the llama hub package To use with our agent See our Usage Pattern Guide for more details Check out our guide for a full overview of the Tools Tool Specs in LlamaHub","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"db2fa420-c915-4045-a9c3-7cddaddbebcf":{"id_":"db2fa420-c915-4045-a9c3-7cddaddbebcf","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_plan.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"ec45a21e-eaa8-47e2-a9cb-fca1f5532210","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_plan.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"09XZB+GOh+TVtGWIR7egW7vU8hWaVVb3gPe2HnKzG5I=","text":"OpenAI Agent Query Planning Download Data Load data Build indices OpenAI Function Agent with a Query Plan Tool   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this demo we explore adding a QueryPlanTool to an OpenAIAgent This effectively enables the agentto do advanced query planning all through a single tool The QueryPlanTool is designed to work well with the OpenAI Function API The tool takes in a set of other tools as input The tool function signature contains of a QueryPlan Pydantic object which can in turn contain a DAG of QueryNode objects defining a compute graph The agent is responsible for defining this graph through the function signature when calling the tool The tool itself executes the DAG over any corresponding tools In this setting we use a familiar example Uber 10Q filings in March June and September of 2022 If you re opening this Notebook on colab you will probably need to install LlamaIndex We build a vector index query engine over each of the documents March June September Use OpenAIAgent built on top of the OpenAI tool use interface Feed it our QueryPlanTool which is a Tool that takes in other tools And the agent to generate a query plan DAG over these tools","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"e38c5ec4-26c7-4613-b62d-3abd05366436":{"id_":"e38c5ec4-26c7-4613-b62d-3abd05366436","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/openai_pydantic_program.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"68db437a-5ebe-401c-bdee-7f4510b0c8c1","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/openai_pydantic_program.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"G4TPSnLq8Gm4CpvOwPfQjeG5HTHmYw6QX+FpIIyjqzU=","text":"OpenAI Pydantic Program Extraction into Album Extracting List of Album with Parallel Function Calling Extraction into Album Streaming Extraction into DirectoryTree object Without docstring in Model With docstring in Model  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This guide shows you how to generate structured data with new OpenAI API via LlamaIndex The user just needs to specify a Pydantic object We demonstrate two settings Extraction into an Album object which can contain a list of Song objects Extraction into a DirectoryTree object which can contain recursive Node objects This is a simple example of parsing an output into an Album schema which can contain multiple songs If you re opening this Notebook on colab you will probably need to install LlamaIndex Define output schema without docstring Define openai pydantic program Run program to get structured output Run program to get structured output The output is a valid Pydantic object that we can then use to call functions APIs With the latest parallel function calling feature from OpenAI we can simultaneously extract multiple structured data from a single prompt To do this we need to pick one of the latest models e g gpt 3 5 turbo 1106 and set allow_multiple to True in our OpenAIPydanticProgram if not it will only return the first object and raise a warning The output is a list of valid Pydantic object We also support streaming a list of objects through our stream_list function Full credits to this idea go to openai_function_call repo https github com jxnl openai_function_call tree main examples streaming_multitask This is directly inspired by jxnl s awesome repo here https github com jxnl openai_function_call That repository shows how you can use OpenAI s function API to parse recursive Pydantic objects The main requirement is that you want to wrap a recursive Pydantic object with a non recursive one Here we show an example in a directory setting where a DirectoryTree object wraps recursive Node objects to parse a file structure The output is a full DirectoryTree structure with recursive Node objects","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"699adeaa-b4af-40da-b3ef-76a8fb2611fe":{"id_":"699adeaa-b4af-40da-b3ef-76a8fb2611fe","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/multimodal.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"206cb0f8-6be9-46a8-b689-ead77d81c5a5","metadata":{"url":"https://docs.llamaindex.ai/en/stable/use_cases/multimodal.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"4K5Yh5JwLn6gsD/dg/uFBk8CcxbYkqhUBHKaCeyYKhU=","text":"Multi modal Types of Multi modal Use Cases Evaluations and Comparisons Model Guides RAG Retrieval Augmented Generation Structured Outputs Retrieval Augmented Image Captioning Agents LLaVa 13 Fuyu 8B and MiniGPT 4 Multi Modal LLM Models Comparison for Image Reasoning Simple Evaluation of Multi Modal RAG  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes LlamaIndex offers capabilities to not only build language based applications but also multi modal applications combining language and images This space is actively being explored right now but there are some fascinating use cases popping up All the core RAG concepts indexing retrieval and synthesis can be extended into the image setting The input could be text or image The stored knowledge base can consist of text or images The inputs to response generation can be text or image The final response can be text or image Check out our guides below You can generate a structured output with the new OpenAI GPT4V via LlamaIndex The user just needs to specify a Pydantic object to define the structure of the output Check out the guide below Oftentimes understanding an image requires looking up information from a knowledge base A flow here is retrieval augmented image captioning first caption the image with a multi modal model then refine the caption by retrieving it from a text corpus Check out our guides below Here are some initial works demonstrating agentic capabilities with GPT 4V These sections show comparisons between different multi modal models for different use cases These notebooks show how to use different Multi Modal LLM models for image understanding reasoning The various model inferences are supported by Replicate or OpenAI GPT4 V API We compared several popular Multi Modal LLMs GPT4 V OpenAI API LLava 13B Replicate Fuyu 8B Replicate MiniGPT 4 Replicate CogVLM Replicate Check out our guides below In this notebook guide we ll demonstrate how to evaluate a Multi Modal RAG system As in the text only case we will consider the evaluation of Retrievers and Generators separately As we alluded in our blog on the topic of Evaluating Multi Modal RAGs our approach here involves the application of adapted versions of the usual techniques for evaluating both Retriever and Generator used for the text only case These adapted versions are part of the llama index library i e evaluation module and this notebook will walk you through how you can apply them to your evaluation use cases Here are notebook guides showing you how to interact with different multimodal model providers","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"641e60bb-aed3-4305-9dbf-4ff21f22c298":{"id_":"641e60bb-aed3-4305-9dbf-4ff21f22c298","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_assistant_agent.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"781f09ba-b63d-4488-ae3b-e46c0245467e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_assistant_agent.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"bRv5Inl3K1wkk8wfiqE/50NAJKIG3TQ2PBfBi3PcGM0=","text":"OpenAI Assistant Agent Simple Agent no external tools Assistant with Built In Retrieval Assistant with Query Engine Tools Assistant Agent with your own Vector Store Retrieval API 1 Setup Load Data 2 Let s Try it Out  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This shows you how to use our agent abstractions built on top of the OpenAI Assistant API Here we show a simple example with the built in code interpreter Let s start by importing some simple building blocks If you re opening this Notebook on colab you will probably need to install LlamaIndex Let s test the assistant by having it use the built in OpenAI Retrieval tool over a user uploaded file Here we upload and pass in the file during assistant creation time The other option is you can upload pass the file id in for a message in a given thread with upload_files and add_message Here we showcase the function calling capabilities of the OpenAIAssistantAgent by integrating it with our query engine tools over different documents LlamaIndex has 35 vector database integrations Instead of using the in house Retrieval API you can use our assistant agent over any vector store Here is our full list of vector store integrations We picked one vector store Supabase using a random number generator","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"2182594f-796c-4a53-9eaa-10af14decb68":{"id_":"2182594f-796c-4a53-9eaa-10af14decb68","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_with_query_engine.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"9d6f2d12-3da2-447f-9d54-9b247736204e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_with_query_engine.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"ujUkicLWiosiE6LqvduSwNo+0Z86Hi4sZLjjV+kBB0Q=","text":"OpenAI Agent with Query Engine Tools Build Query Engine Tools Setup OpenAI Agent Let s Try It Out   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  If you re opening this Notebook on colab you will probably need to install LlamaIndex Download Data","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"b0703a03-bb84-46a0-87f0-203c0a6eee55":{"id_":"b0703a03-bb84-46a0-87f0-203c0a6eee55","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/df_program.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b845440f-85ab-48f4-a9a9-cfbac8d1a66e","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/df_program.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"57cMoGYm54BJL0m1RISnK8I8ZHoNAqJUrjf5lG0R3jg=","text":"DataFrame Structured Data Extraction Build a DF Extractor Yourself Using OpenAIPydanticProgram Use our DataFrame Programs   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This demo shows how you can extract tabular DataFrames from raw text This was directly inspired by jxnl s dataframe example here https github com jxnl openai_function_call blob main auto_dataframe py We show this with different levels of complexity all backed by the OpenAI Function API more code How to build an extractor yourself using our OpenAIPydanticProgram less code Using our out of the box DFFullProgram and DFRowsProgram objects Our OpenAIPydanticProgram is a wrapper around an OpenAI LLM that supports function calling it will return structuredoutputs in the form of a Pydantic object We import our DataFrame and DataFrameRowsOnly objects To create an output extractor you just need to 1 specify the relevant Pydantic object and 2 Add the right prompt If you re opening this Notebook on colab you will probably need to install LlamaIndex We provide convenience wrappers for DFFullProgram and DFRowsProgram This allows a simpler object creation interface than specifying all details through the OpenAIPydanticProgram","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"f82a9197-52e9-42df-9dd9-a09569ed8df5":{"id_":"f82a9197-52e9-42df-9dd9-a09569ed8df5","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_retrieval_benchmark.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"a7b7c880-202f-4769-be7c-06ea3072096a","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_retrieval_benchmark.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"wsYBuGcrp0vj8eIyVvlgCrs/UM+ME0egG/hnubgJqt8=","text":"Benchmarking OpenAI Retrieval API through Assistant Agent Setup Data Define Eval Modules Construct Assistant with Built In Retrieval Benchmark Setup Golden Dataset Eval Modules Define Baseline Index RAG Pipeline Run Evals over Baseline Run Evals over Assistant API Get Results Option 1 Pull Existing Dataset Option 2 Generate New Dataset Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This guide benchmarks the Retrieval Tool from the OpenAI Assistant API by using our OpenAIAssistantAgent We run over the Llama 2 paper and compare generation quality against a naive RAG pipeline Here we load the Llama 2 paper and chunk it We setup evaluation modules including the dataset and evaluators Here we load in a golden dataset NOTE We pull this in from Dropbox For details on how to generate a dataset please see our DatasetGenerator module If you choose this option you can choose to generate a new dataset from scratch This allows you to play around with our DatasetGenerator settings to make sure it suits your needs We define two evaluation modules correctness and semantic similarity both comparing quality of predicted response with actual response Let s construct the assistant by also passing it the built in OpenAI Retrieval tool Here we upload and pass in the file during assistant creation time We run the agent over our evaluation dataset We benchmark against a standard top k RAG pipeline k 2 with gpt 4 turbo NOTE During our time of testing November 2023 the Assistant API is heavily rate limited and can take 1 2 hours to generate responses over 60 datapoints Here we see that our basic RAG pipeline does better Take these numbers with a grain of salt The goal here is to give you a script so you can run this on your own data That said it s surprising the Retrieval API doesn t give immediately better out of the box performance","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"c545eaad-f298-495e-84e2-b6da19365fda":{"id_":"c545eaad-f298-495e-84e2-b6da19365fda","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/query_engine.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"a6f78092-1149-4a8c-a7cf-9f5fea415414","metadata":{"url":"https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/query_engine.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"PsDwwAfGwAf6P4cPtrQIToZwxQS+8x5RlxqCgZybazU=","text":"Query Engines Pydantic Outputs Usage Pattern Modules   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Using index as_query_engine and it s underlying RetrieverQueryEngine we can support structured pydantic outputs without an additional LLM calls in contrast to a typical output parser Every query engine has support for integrated structured responses using the following response_modes in RetrieverQueryEngine refine compact tree_summarize accumulate beta requires extra parsing to convert to objects compact_accumulate beta requires extra parsing to convert to objects Under the hood this uses OpenAIPydanitcProgam or LLMTextCompletionProgram depending on which LLM you ve setup If there are intermediate LLM responses i e during refine or tree_summarize with multiple LLM calls the pydantic object is injected into the next LLM prompt as a JSON object First you need to define the object you want to extract Then you create your query engine Lastly you can get a response and inspect the output Detailed usage is available in the notebooks below","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"2fcc9c42-208c-464d-baf1-cfbdbe76d413":{"id_":"2fcc9c42-208c-464d-baf1-cfbdbe76d413","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_cookbook.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"56db4b28-0e3c-4bdf-8bae-54e392f01959","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_cookbook.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"Vwke6zAfs0Z70qMScd2+jVv25I6Qlp7UOtOD8OmScO4=","text":"OpenAI Agent Query Engine Experimental Cookbook AutoRetrieval from a Vector Database Joint Text to SQL and Semantic Search Define Function Tool Initialize Agent Load and Index Structured Data Load and Index Unstructured Data Define Query Engines Tools Initialize Agent  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we try out the OpenAIAgent across a variety of query engine tools and datasets We explore how OpenAIAgent can compare replace existing workflows solved by our retrievers query engines Auto retrieval Joint SQL and vector search Our existing auto retrieval capabilities in VectorIndexAutoRetriever allow an LLM to infer the right query parameters for a vector database including both the query string and metadata filter Since the OpenAI Function API can infer function parameters we explore its capabilities in performing auto retrieval here If you re opening this Notebook on colab you will probably need to install LlamaIndex Here we define the function interface which is passed to OpenAI to perform auto retrieval We were not able to get OpenAI to work with nested pydantic objects or tuples as arguments so we converted the metadata filter keys and values into lists for the function API to work with Define AutoRetrieve Functions This is currently handled by our SQLAutoVectorQueryEngine Let s try implementing this by giving our OpenAIAgent access to two query tools SQL and Vector We load sample structured datapoints into a SQL db and index it We load unstructured data into a vector index backed by Pinecone","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"c9242346-80f0-4a10-a0dc-429aa8cdbfb5":{"id_":"c9242346-80f0-4a10-a0dc-429aa8cdbfb5","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"22b90dbb-9935-4938-b89e-e5533a25fd9c","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"rozH0eBH15LGjn0ynWGUNmrwxX+WT3+kx1/zE0Zig9Y=","text":"Sub Question Query Engine Preparation Download Data Setup sub question query engine Run queries   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this tutorial we showcase how to use a sub question query engine to tackle the problem of answering a complex query using multiple data sources It first breaks down the complex query into sub questions for each relevant data source then gather all the intermediate reponses and synthesizes a final response If you re opening this Notebook on colab you will probably need to install LlamaIndex","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"37a1e7af-ba3a-4e4b-95a0-48fca8c4844c":{"id_":"37a1e7af-ba3a-4e4b-95a0-48fca8c4844c","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/evaporate_program.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"5c0bd955-9f5e-491a-91ec-d40c630f2c2d","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/output_parsing/evaporate_program.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"iqy5Hp/xfxKAN96W2PMBL4Imy0a8MgZi77uplyclC2c=","text":"Evaporate Demo Use DFEvaporateProgram Use MultiValueEvaporateProgram Bonus Use the underlying EvaporateExtractor Load data Parse Data Running the DFEvaporateProgram Fitting Functions Run Inference  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This demo shows how you can extract DataFrame from raw text using the Evaporate paper Arora et al https arxiv org abs 2304 09433 The inspiration is to first fit on a set of training text The fitting process uses the LLM to generate a set of parsing functions from the text These fitted functions are then applied to text during inference time If you re opening this Notebook on colab you will probably need to install LlamaIndex The DFEvaporateProgram will extract a 2D dataframe from a set of datapoints given a set of fields and some training data to fit some functions on Here we load a set of cities from Wikipedia Here we demonstrate how to extract datapoints with our DFEvaporateProgram Given a set of fields the DFEvaporateProgram can first fit functions on a set of training data and then run extraction over inference data In contrast to the DFEvaporateProgram which assumes the output obeys a 2D tabular format one row per node the MultiValueEvaporateProgram returns a list of DataFrameRow objects each object corresponds to a column and can contain a variable length of values This can help if we want to extract multiple values for one field from a given piece of text In this example we use this program to parse gold medal counts The underlying EvaporateExtractor offers some additional functionality e g actually helping to identify fields over a set of text Here we show how you can use identify_fields to determine relevant fields around a general topic field","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"2035a495-df40-47bd-8e63-0c82e7915e81":{"id_":"2035a495-df40-47bd-8e63-0c82e7915e81","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/custom_agent.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"a364a124-8174-4aea-8f32-9710f97ac1d9","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/custom_agent.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"cgJOtW1YIr/MeE2636FuRHqtcQFqXwDJchIKmW3RZzE=","text":"Building a Custom Agent Setup the Custom Agent Setup Data and Tools Build Custom Agent Try Out Some Queries Refresher Creating a Custom Agent Worker Subclass Setup SQL DB Tool Setup Vector Tools  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes In this cookbook we show you how to build a custom agent using LlamaIndex The easiest way to build a custom agent is to simply subclass CustomSimpleAgentWorker and implement a few required functions You have complete flexibility in defining the agent step wise logic This lets you add arbitrarily complex reasoning logic on top of your RAG pipeline We show you how to build a simple agent that adds a retry layer on top of a RouterQueryEngine allowing it to retry queries until the task is complete We build this on top of both a SQL tool and a vector index query tool Even if the tool makes an error or only answers part of the question the agent can continue retrying the question until the task is complete Here we setup the custom agent An agent in LlamaIndex consists of both an agent runner agent worker An agent runner is an orchestrator that stores state like memory whereas an agent worker controls the step wise execution of a Task Agent runners include sequential parallel execution More details can be found in our lower level API guide Most core agent logic e g ReAct function calling loops can be executed in the agent worker Therefore we ve made it easy to subclass an agent worker letting you plug it into any agent runner As mentioned above we subclass CustomSimpleAgentWorker This is a class that already sets up some scaffolding for you This includes being able to take in tools callbacks LLM and also ensures that the state steps are properly formatted In the meantime you mostly have to implement the following functions _initialize_state _run_step _finalize_task Some additional notes You can implement _arun_step as well if you want to support async chat in the agent You can choose to override __init__ as long as you pass all remaining args kwargs to super CustomSimpleAgentWorker is implemented as a Pydantic BaseModel meaning that you can define your own custom properties as well Here are the full set of base properties on each CustomSimpleAgentWorker that you need to can pass in when constructing your custom agent tools Sequence BaseTool tool_retriever Optional ObjectRetriever BaseTool llm LLM callback_manager CallbackManager verbose bool Note that tools and tool_retriever are mutually exclusive you can only pass in one or the either e g define a static list of tools or define a callable function that returns relevant tools given a user message You can call get_tools message str to return relevant tools given a message All of these properties are accessible via self when defining your custom agent Here we define some helper variables and methods E g the prompt template to use to detect errors as well as the response format in Pydantic We setup both a SQL Tool as well as vector index tools for each city","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"f418df5c-0766-41d4-8fad-813faf68298b":{"id_":"f418df5c-0766-41d4-8fad-813faf68298b","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_plan.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"ac7285a4-8147-41f2-aa10-2e6861710711","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_plan.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"09XZB+GOh+TVtGWIR7egW7vU8hWaVVb3gPe2HnKzG5I=","text":"OpenAI Agent Query Planning Download Data Load data Build indices OpenAI Function Agent with a Query Plan Tool   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this demo we explore adding a QueryPlanTool to an OpenAIAgent This effectively enables the agentto do advanced query planning all through a single tool The QueryPlanTool is designed to work well with the OpenAI Function API The tool takes in a set of other tools as input The tool function signature contains of a QueryPlan Pydantic object which can in turn contain a DAG of QueryNode objects defining a compute graph The agent is responsible for defining this graph through the function signature when calling the tool The tool itself executes the DAG over any corresponding tools In this setting we use a familiar example Uber 10Q filings in March June and September of 2022 If you re opening this Notebook on colab you will probably need to install LlamaIndex We build a vector index query engine over each of the documents March June September Use OpenAIAgent built on top of the OpenAI tool use interface Feed it our QueryPlanTool which is a Tool that takes in other tools And the agent to generate a query plan DAG over these tools","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"f931fc8e-be8b-4c81-b32f-92af97556a45":{"id_":"f931fc8e-be8b-4c81-b32f-92af97556a45","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_assistant_query_cookbook.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"41282fb1-cf40-4caf-812e-ec4583805927","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_assistant_query_cookbook.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"MQvot4SlEHnkDqg4u+o4bKka6pQbC2rJuWrFm+DCI8g=","text":"OpenAI Assistant Advanced Retrieval Cookbook Joint QA and Summarization AutoRetrieval from a Vector Database Joint Text to SQL and Semantic Search Load Data Setup Vector Summary Indexes Query Engines Tools Define Assistant Agent Define Function Tool Initialize Agent Load and Index Structured Data Load and Index Unstructured Data Define Query Engines Tools Initialize Agent Results A bit flaky Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  In this notebook we try out OpenAI Assistant API for advanced retrieval tasks by plugging in a variety of query engine tools and datasets The wrapper abstraction we use is our OpenAIAssistantAgent class which allows us to plug in custom tools We explore how OpenAIAssistant can complement replace existing workflows solved by our retrievers query engines through its agent execution function calling loop Joint QA Summarization Auto retrieval Joint SQL and vector search In this section we show how we can get the Assistant agent to both answer fact based questions and summarization questions This is something that the in house retrieval tool struggles to accomplish Our existing auto retrieval capabilities in VectorIndexAutoRetriever allow an LLM to infer the right query parameters for a vector database including both the query string and metadata filter Since the Assistant API can call functions infer function parameters we explore its capabilities in performing auto retrieval here If you re opening this Notebook on colab you will probably need to install LlamaIndex Here we define the function interface which is passed to OpenAI to perform auto retrieval We were not able to get OpenAI to work with nested pydantic objects or tuples as arguments so we converted the metadata filter keys and values into lists for the function API to work with This is currenty handled by our SQLAutoVectorQueryEngine Let s try implementing this by giving our OpenAIAssistantAgent access to two query tools SQL and Vector search We load sample structured datapoints into a SQL db and index it We load unstructured data into a vector index backed by Pinecone","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"adeec1f0-3f5b-424d-b536-34ca9caae31a":{"id_":"adeec1f0-3f5b-424d-b536-34ca9caae31a","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/agent_runner_rag_controllable.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"4b9ed22f-7019-4e67-93a0-10d2b09531ce","metadata":{"url":"https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/agent_runner_rag_controllable.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"JqMGdF7E3lADhfxcGynjAVjfEsLeZybP7dWqtOifRy4=","text":"Controllable Agents for RAG Setup Data Setup Agent Run Some Queries Setup Human In the Loop Chat Download Data Load data Build indices query engines tools Out of the box Test Step Wise Execution  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  Adding agentic capabilities on top of your RAG pipeline can allow you to reason over much more complex questions But a big pain point for agents is the lack of steerability transparency An agent may tackle a user query through chain of thought planning which requires repeated calls to an LLM During this process it can be hard to inspect what s going on or stop correct execution in the middle This notebook shows you how to use our brand new lower level agent API which allows controllable step wise execution on top of a RAG pipeline We showcase this over Wikipedia documents Here we load a simple dataset of different cities from Wikipedia In this section we define our tools and setup the agent We now demonstrate the capabilities of our step wise agent framework We show how it can handle complex queries both e2e as well as step by step We can then show how we can steer the outputs Calling chat will attempt to run the task end to end and we notice that it only ends up calling one tool The end to end chat didn t work Let s try to break it down step by step and inject our own feedback if things are going wrong This returns a Task object which contains the input additional state in extra_state and other fields Now let s try executing a single step of this task We run into the same issue The query finished even though we haven t analyzed the docs yet Can we add a user input Since the steps look good we are now ready to call finalize_response get back our response This will also commit the task execution to the memory object present in our agent_runner We can inspect it With these capabilities it s easy to setup human in the loop or LLM in the loop feedback when interacting with an agent especially for long running tasks We setup a double loop one for the task the user chatting with an agent and the other to control the intermediate executions","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"6d904ea1-4330-49c3-9648-4e619335ce97":{"id_":"6d904ea1-4330-49c3-9648-4e619335ce97","metadata":{"url":"https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/query_transformations.html"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"d2b022e0-e493-4ba0-933f-d83aa273b9b4","metadata":{"url":"https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/query_transformations.html"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"TrfROqc/gSmlrveVsDE+mn2+wKrb/7s614VHjifILvA=","text":"Query Transformations Use Cases HyDE Hypothetical Document Embeddings Multi Step Query Transformations   Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes LlamaIndex allows you to perform query transformations over your index structures Query transformations are modules that will convert a query into another query They can be single step as in the transformation is run once before the query is executed against an index They can also be multi step as in The query is transformed executed against an index The response is retrieved Subsequent queries are transformed executed in a sequential fashion We list some of our query transformations in more detail below Query transformations have multiple use cases Transforming an initial query into a form that can be more easily embedded e g HyDE Transforming an initial query into a subquestion that can be more easily answered from the data single step query decomposition Breaking an initial query into multiple subquestions that can be more easily answered on their own multi step query decomposition HyDE is a technique where given a natural language query a hypothetical document answer is generated first This hypothetical document is then used for embedding lookup rather than the raw query To use HyDE an example code snippet is shown below Check out our example notebook for a full walkthrough Multi step query transformations are a generalization on top of existing single step query transformation approaches Given an initial complex query the query is transformed and executed against an index The response is retrieved from the query Given the response along with prior responses and the query follow up questions may be asked against the index as well This technique allows a query to be run against a single knowledge source until that query has satisfied all questions An example image is shown below  Here s a corresponding example code snippet Check out our example notebook for a full walkthrough Examples","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"807e0b4d-39eb-4784-a070-ee0cff640c8e":{"id_":"807e0b4d-39eb-4784-a070-ee0cff640c8e","metadata":{"url":"https://docs.vectara.com//docs/api-reference/admin-apis/admin"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"ef653191-e778-41e2-af52-2e90d42c587e","metadata":{"url":"https://docs.vectara.com//docs/api-reference/admin-apis/admin"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"6WQPnrdXdCDKyAguHTYbmWtKw1WkBhJsRTcRqUEWDQI=","text":"Corpus Administration APIs Create Delete and Reset API Definitions Corpus Management API Definitions   The Vectara Console is a good way for you to get started with Vectara Onceyou re ready to integrate the platform more deeply into your application theCorpus Admin APIs allow you to programmatically manipulate corpora and performmany other operations within the system These APIs enable new workflows fororganizations like tracking usage of accounts and corpora Check out this blog post about managing multi tenancy for more details The interactive API Playground lets you experiment with these API endpoints The full definitions of the Create Reset and Delete gRPC APIs are coveredin admin proto The Corpus Management API definitions enable administrators to track usage oftheir accounts and corpora The REST APIs are programmatically derived from these gRPC definitions SeeREST APIs for more information on endpoints or expand thespecific API in the left navigation sidebar to find REST examples in variousprogramming languages For more information on the programmatic conversion see gRPC with REST and Open APIs Itgoes into detail about how gRPC services were made available in both gRPC andHTTP REST formats to provide flexibility to users and create a versatile APIframework","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"5cbbebb7-643a-49c9-aaee-f1c4fa046369":{"id_":"5cbbebb7-643a-49c9-aaee-f1c4fa046369","metadata":{"url":"https://docs.vectara.com//docs/api-reference/search-apis/search"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"8ca85f2e-767d-41c5-ab42-e64cd1c00989","metadata":{"url":"https://docs.vectara.com//docs/api-reference/search-apis/search"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"azKdKWL0uSDK5jHwIOn6h+3wdepJSvKxSx8cXQpCKy0=","text":"Search API Definition Query Request Body and Response Query Definition Corpus Key Definition REST Example gRPC Example Advanced Scenarios Retrieval Augmented Generation Summarization Grounded in Data Chat Conversation Located within the Summary Query API Endpoint Address Query Request Headers Query Request Query Service Query Request Search Multiple Corpora Corpus Key Summarization Request Example ResponseSet Attribute Batch Query and Response Search a Single Corpus Example Search Multiple Corpora Example The Search endpoint lets you perform a query while defining its parametersthat specify the query text pagination details metadata filters and othersettings that enable application builders to tailor their queries to specificuse cases After you index data into one or more corpora you can run queriesand display the results This page provides a detailed reference for howto run queries and also describes some of Vectara s capabilities in metadatafiltering reranking Retrieval Augmented Generation RAG and hybrid search Check out our interactive API Playground that lets you experiment with this REST endpoint to manage your corpus details The Query request body specifies different parameters that ask questions aboutthe data within corpora The Query request requires the following parameters The query response message encapsulates a single query result It is a subdocumentprovided at indexing time The text is the subdocument text the scoreindicates how well the text answers the query higher scores are better The metadata list holds any subdocument level metadata that was stored withthe item at indexing time The corpus_key indicates which corpus the resultcame from recall that a single query can execute against multiple corpora Finally the document_index points at a specific document within theenclosing response set s document array This is useful for retrieving thedocument id and document level metadata A single query consists of a query which is specified in plain text Forexample Where can I buy the latest iPhone Optionally the querycontext provides additional information that the system may use to refine theresults For example The Apple store near my house is closed due to Covid The start field controls the starting position within the list of results while num_results dictates how many results are returned Thus settingstart 5 and num_results 20 would return twenty results beginning at positionfive These fields are mainly used to provide pagination The corpusKey specifies a list of corpora against which to run thequery While it s most often the case that a query is run against a singlecorpus it s sometimes useful to run against several in parallel Finally the reranking configuration enables reranking of results tofurther increase relevance in certain scenarios For details about our Englishcross attentional Scale only and Maximal Marginal Relevance MMR rerankers see Reranking The corpusKey specifies the ID of the corpus being searched The metadata_filter allowsspecifying a predicate expression that restricts the search to a part of thecorpus The filter is written in a simplified SQL dialect and can referencemetadata that was marked as filterable during corpuscreation See the Filter Expressions Overview for adescription of their syntax and Corpus Administration to learn howreferenceable metadata is specified during corpus creation By default Vectara only uses its neural semantic retrieval model and does not attempt to use keyword matching To enable hybrid search with amix of both keyword and neural results edit the lambda value If the corpus specifies custom dimensions Scale only weights can beassigned to each dimension as well Finally it s possible to override the semantic interpretation of the querystring Usually the default settings for the corpus are sufficient In moreadvanced scenarios it s desirable to force it to be treated as a query or more rarely as a response To use Retrieval Augmented Generation RAG which Vectara also refers to as Grounded Generation our groundbreaking way of producing generativesummaries on top of your own data you can submit a SummarizationRequest alongside your query This produces a summary that attempts to answer the end user s question citing the results as references For more information read about Retrieval Augmented Generation The summary within the query can also contain a conversation from Vectara Chat whichincludes a conversationId You enable Vectara Chat by setting the store value to true To interact with the Query service via REST calls you need the followingheaders The Query request body specifies different parameters that ask questions aboutthe data within corpora The Query request requires the following parameters You can find the full Query gRPC definition at serving proto Fundamentally the system accepts a query and returns a response which containsa list of results However for efficiency one or more queries can be batchedinto a single request The request provides several fields and nested messages that configure thequery its context and how the results should be processed and presented The query contains the search terms that the system needs to match againstthe data Then ContextConfig specifies the amount of text or number ofsentences before and after the result snippet The corpus_key allows the query to be executed across multiple corpora The CorpusKey identifies a specific corpus or corpora to include in the query Specifying the customer_id is optional since it defaults to thecustomer attached to the gRPC request The full Query definition provides the detailed summary request When Vectara respondswith the list of results that most semantically answer the user it will alsothen produce a summary of the results with its sources cited For more detailson Retrieval Augmented Generation have a look at thechatbots and grounded generation overview The summary comes back in a format where the text contains a summary of therelevant results to the given search with those relevant results included ascited sources Vectara cites these by number format For example if the 1st result is in the summary it is cited as 1 The response set groups a list of responses sorted in order of score togetherwith a list of statuses and enclosing documents Since it s possible forseveral results to come from the same document the length of the document listmay be less than the length of the response list Attribute represents a named piece of metadata Both the name and itsvalue are string typed The batch query request and response messages simply aggregate severalindividual queries and response sets respectively The response sets will matchthe queries in both number and order For example the third response set inthe batch response will correspond with the third query in the batch request There are situations where searching multiple corpora simultaneously can bebeneficial To do this effectively you need two things The query body modification that s necessary is t","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"d0b0fc55-8541-47cb-917d-2394e1ccab85":{"id_":"d0b0fc55-8541-47cb-917d-2394e1ccab85","metadata":{"url":"https://docs.vectara.com//docs/api-reference/search-apis/search"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"03d6c32b-0fea-494f-a716-50096ed5410b","metadata":{"url":"https://docs.vectara.com//docs/api-reference/search-apis/search"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"wWqYsvhZ8SD3YTShS4iWNXi2KAsLBWNtmgL2EsvHMlE=","text":"hat corpusKey can take anarray of objects So if you re currently searching 1 corpus as follows As long as your API key has permissions to each of these corpora you can search multiple corpora at once as follows In this example the query returns results across the queriedcorpora The corpusKey is returned in the response for each documentif you need to use it in your application","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"1070cf64-3873-48c6-9667-16366cc66919":{"id_":"1070cf64-3873-48c6-9667-16366cc66919","metadata":{"url":"https://vectara.com/contact-us/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"cddb398b-b5a3-4cd2-b08e-0fe34d4b12e3","metadata":{"url":"https://vectara.com/contact-us/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"azjlm6TgsPbiP8UxsBZTN8a2+yxpSBUa+33ya+wJdFI=","text":"Talk to a GenAI Expert   PlatformSolutionsResourcesPricingCompany CONTACT US 2024 Vectara Inc All rights reserved Prior to founding Vectara Amr served as the VP of Developer Relations for Google Cloud Amr co founded Cloudera where he led the development of enterprise tools to ingest and extract value from big data Before Cloudera Amr served as VP of product intelligence engineering at Yahoo after Yahoo acquired his first startup Aptiva a search engine company Amr holds a PhD in Electrical Engineering from Stanford University and a MA from Cairo University Prior to founding Vectara Amin served as a senior engineer at Google Research for 10 years Amin led the development of question answering and neural information retrieval systems that have been launched into several Google products His 20 years of search industry experience include working with Fortune 500 companies startups and state and federal governments Amin holds a BS in Computer Science and Mathematics from Bowling Green State University","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"423a5788-d6c5-42db-ad39-d09633eade1d":{"id_":"423a5788-d6c5-42db-ad39-d09633eade1d","metadata":{"url":"https://vectara.com/legal/security-at-vectara/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"8ba7ac4b-8bed-416a-a4a8-03d4eb40d749","metadata":{"url":"https://vectara.com/legal/security-at-vectara/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"2G1oaNYK+kzrwwqGkl3HQmi77OOY/CB3neNkbsplAWs=","text":"Security at Vectara   Vectara s Trust Center Security Matters to UsKey Security FeaturesInfrastructure SecurityData ManagementPhysical and Environmental SecuritySoftware SecurityPayment ProcessingOutsourced ProcessingPenetration TestingPreventing Unauthorized Product UseIncident DetectionIncident ResponseGeneral Security QuestionsPlatformSolutionsResourcesPricingCompany Visit Vectara s Trust Center at Trust Vectara com for additional information on Vectara s security announcements compliance certifications security tests and additional information Security is one of our biggest priorities here at Vectara On this page we have provided information about the security of your data our general security practices and how you can reach a member of the security team if you have questions that haven t been answered below The Vectara platform safeguards customer data using a variety of controls Authentication We implement a uniform password policy for our customer products Customers who interact with the products via the user interface must authenticate before accessing non public customer data We make HTTPS encryption also referred to as SSL or TLS available on every one of its login interfaces and for free for every customer Our HTTPS implementation uses industry standard algorithms and certificates We store user passwords using well validated technical approaches and policies that follow industry standard practices for security We have implemented technologies to ensure that stored data is encrypted at rest Authorization Customer Data is stored in multi tenant storage systems accessible to Customers via only application user interfaces and application programming interfaces Customers are not allowed direct access to the underlying application infrastructure The authorization model in each of our products is designed to ensure that only the appropriately assigned individuals can access relevant features views and configuration options Authorization to data sets is performed through validating the user s permissions against the attributes associated with each data set Application Programming Interface API access Public product APIs may be accessed using an API key or through other secure access methods Role based access control Vectara allows you to assign role specific access and permission to user entities in Vectara products Data encryption We use strong encryption standards to protect your data both when it s in transit and within the Vectara network as well as when it is at rest within the Vectara cloud Logs System monitoring and key activities related to billing security access and account management are securely logged We contract our digital hardware to cloud vendors that adhere to the applicable data regulations and compliances Our infrastructure runs on data centers provided by Amazon Web Services AWS which is SOC2 and PCI Level 1 certified among others AWS has a number of security and privacy focused features that we leverage wherever applicable The infrastructure providers use commercially reasonable efforts to ensure a minimum of 99 95 uptime The providers maintain a minimum of N 1 redundancy to power network and HVAC services Our infrastructure runs on stable regularly patched versions of operating system images with carefully configured security groups isolated VPC environments with well defined network segmentation role based access control and advanced web application firewall protection All customer data is stored in securely configured data storage tiers All customer data is encrypted at the server side before and during storage Optionally client managed encryption keys are supported Data can be stored with at least dual redundancy and with regular backups Different Vectara plans have different security features which are outlined on the Vectara pricing page We maintain all internal testing and validation data in a production stack equivalent internal stack Vectara does not distribute actual customer data for internal testing or validation purposes We rely on Amazon Web Services AWS to manage the physical and environmental security of our data centers Our internal security program covers physical security at our offices For more details please review AWS control and security measures Our security team sets architectural guidelines conducts code reviews and reviews deployment of software systems that can interface with customer data Our developers are trained with specific attention toward security Our code review processes look for any code that could potentially violate security policies We process all payments using Stripe which has been certified as a PCI Level 1 Service Provider Vectara hosts our Service with outsourced cloud infrastructure providers Additionally we maintain contractual relationships with vendors in order to provide the Service in accordance with our DPA We rely on contractual agreements and privacy policies in order to protect data processed or stored by these vendors In addition to our regular security reviews we partner with trusted third party security companies to perform annual penetration tests across our product ecosystem In addition to the elastic scaling capacity of our compute instances to mitigate interruptions at the application layer we implement industry standard access controls and detection capabilities for the internal networks that support Vectara products Vectara designed our infrastructure to log extensive information about the system behavior traffic received system authentication and other application requests Internal systems aggregated log data and alert appropriate employees of malicious unintended or anomalous activities Our personnel including security operations and support personnel are responsive to known incidents We have policies and procedures to address service availability integrity security privacy and confidentiality issues Our stated processes include If you have general security questions or concerns please email us at security vectara com 2024 Vectara Inc All rights reserved Prior to founding Vectara Amr served as the VP of Developer Relations for Google Cloud Amr co founded Cloudera where he led the development of enterprise tools to ingest and extract value from big data Before Cloudera Amr served as VP of product intelligence engineering at Yahoo after Yahoo acquired his first startup Aptiva a search engine company Amr holds a PhD in Electrical Engineering from Stanford University and a MA from Cairo University Prior to founding Vectara Amin served as a senior engineer at Google Research for 10 years Amin led the development of question answering and neural information retrieval systems that have been launched into several Google products His 20 years of search industry experience include working with Fortune 500 companies startups and state and federal governments Amin holds a BS in Computer Science and Mathematics from Bowling Green State University","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"99283d28-25fa-4184-b725-3fce464c6753":{"id_":"99283d28-25fa-4184-b725-3fce464c6753","metadata":{"url":"https://vectara.com/careers/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"748de255-f8c4-430b-9f82-ca775a8915ba","metadata":{"url":"https://vectara.com/careers/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"K/jFo+tGPdxgKMdWgqDuYSimIkScbPBYlM9mFFDYD+c=","text":"Careers and CultureOur MissionWe are looking for people whoBenefitsOpen roles at VectaraEqual OpportunityAre you a Builder Are Strategic Analytic ThinkersBelieve in the Power of TechnologyAre Builders Generous pay equity grantsMedical Dental VisionHSAs and FSAsPaid Time OffDevelopmentFun PlatformSolutionsResourcesPricingCompany Until now the world of search has focused on keyword based search Vectara is building the next generation of search technologies Our pioneering semantic search platform harnesses the power of neural networks and large language models to serve a big and growing list of applications for natural language processing We are a team that believes in product led growth and the power of inbound marketing We value the opportunity to serve developers product teams and the applications and sites they create People at Vectara are passionate about helping customers take advantage of breakthroughs in applied AI to discover and learn more about their business and the world around them Become a part of a team that is changing the world of search and the way that engineers interface with data Help us rethink how and where natural language processing is applied within applications sites and enterprises across the globe Vectara s mission is to help the world find meaning through search The company uses the latest innovations in artificial intelligence and neural network technologies for natural language processing to deliver unparalleled search relevance To accomplish this mission we are building a world class team of computer scientists product and business leaders Everyone at Vectara is directly responsible and chartered with connecting the user with the magic of our product People at Vectara relish the idea of thinking in modern unconventional ways to show off how Vectara can solve some of the most persistent and difficult problems humans face in finding and using information to make better decisions faster We think constantly about the impact of our work on customers how it might remove a problem our customers face today and ensure we are measuring our customers continued progress and success on their journey to find meaning through search The Vectara Team lives and works with a great set of values every day They balance accountability for getting outstanding results with being a supportive work friendly environment Head of Staff We are a developer first product led growth company Our GTM is focused on making it easier for developers to learn the magic of neural search as a new powerful interface to their data and to help them succeed in applying it in an incredible number of ways both understood and brand new to the world We have a culture that combines passion and humbleness with an understanding of putting our customers success first Controller People at Vectara are energized by the potential of our unique technology and value This energy shows in how we interact with each other and our customers We have a tendency to generate excitement for the company and its possibilities in whomever we interact with You might say we are a team of entrepreneurs because well we are Every member of Vectara is an ambassador of the Vectara brand Join us and experience what it is like to work with a world class team who interact with developers and help us turn them into Vectara s next biggest fans It is super rewarding to work with such a talented team of engineers I feel like I am learning and growing every day Software Engineer We hire the best talent and pay accordingly We want every employee to feel invested in the company s success Vectara offers 100 paid medical dental and vision beginning on your first day Vectara offers access to flexible spending accounts FSAs and a health savings account HSA to help employees save money Enjoy generous paid time off PTO plus paid sick time holidays and company rest days We provide professional development and training opportunities We want this to be a fun place to work and we think creatively about how to achieve that Become a part of an outstanding team Check out these open roles for more information Vectara welcomes all We value the collective wisdom of people from different backgrounds experiences abilities and perspectives We never discriminate on the basis of race religion national origin gender identity or expression sexual orientation age or marital veteran or disability status Vectara has a positive and supportive culture we look for people who are inventive and work to be a little better every single day We seek to be smart humble hardworking and above all curious After all we are on a mission to find meaning 2024 Vectara Inc All rights reserved Prior to founding Vectara Amr served as the VP of Developer Relations for Google Cloud Amr co founded Cloudera where he led the development of enterprise tools to ingest and extract value from big data Before Cloudera Amr served as VP of product intelligence engineering at Yahoo after Yahoo acquired his first startup Aptiva a search engine company Amr holds a PhD in Electrical Engineering from Stanford University and a MA from Cairo University Prior to founding Vectara Amin served as a senior engineer at Google Research for 10 years Amin led the development of question answering and neural information retrieval systems that have been launched into several Google products His 20 years of search industry experience include working with Fortune 500 companies startups and state and federal governments Amin holds a BS in Computer Science and Mathematics from Bowling Green State University","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"4e3dd728-db50-432a-8762-4103a2644ccd":{"id_":"4e3dd728-db50-432a-8762-4103a2644ccd","metadata":{"url":"https://docs.vectara.com/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"fb1e2989-fb0f-449a-a74d-7fb2bc77a82d","metadata":{"url":"https://docs.vectara.com/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"gggB81gs1/sucG0U7iU+5HqmKN4c0Ady7gMFPm1hM6Y=","text":"Vectara Docs  Powerful Semantic SearchEasy to UseDesigned by Experts  Developer documentation for Vectara s Semantic Search Platform Our semantic search based on the latest Neural IR research returns results that keyword search often misses Semantic search is not just about finding data but about understanding data and helping you answer questions about your data Our intuitive cloud based API makes it easy to index and query your textual data making it easy to create generative AI advanced applications like our AskNews demo quickly Dive into our API Playground to experiment with Vectara s REST APIs directly from your browser We are experts in language understanding and machine learning with over twenty five years of industry experience Our cutting edge solutions are built on this extensive expertise to deliver optimal performance and reliability","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"5d50a89c-3f78-4223-888a-bb8768edd63b":{"id_":"5d50a89c-3f78-4223-888a-bb8768edd63b","metadata":{"url":"https://vectara.com/legal/terms-of-service/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"178f629a-6c72-41d4-99d2-7bababc7fa38","metadata":{"url":"https://vectara.com/legal/terms-of-service/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"OPrxQT720hw3kcV/tao8rrSj8bd+6LLlkxYyGbGDBro=","text":"Terms of Service   PlatformSolutionsResourcesPricingCompany 1 1 Vectara Inc herein referred to as the Company we us or our provides and makes available this web site the Site All use of the Site is subject to the terms and conditions contained in these Website Terms and Conditions this Agreement Please read this Agreement carefully By accessing browsing or otherwise using the Site you acknowledge that you have read understood and agree to be bound by this Agreement If you do not accept the terms and conditions of this Agreement you shall not access browse or use the Site You understand and agree that your use of our Software as a Service machine learning powered text search platform Company Products shall not be governed by this Agreement but rather by your company s or organization s agreement with the Company covering such Company Products However please note that your access to and use of the Site and any Company Products is also subject to the Company s Privacy Policy located at vectara com legal privacy policy 1 2 You understand and agree that we may change this Agreement at any time without prior notice You may read a current effective copy of this Agreement at any time by selecting the Terms of Use link on the Site The revised terms and conditions will become effective at the time of posting Any use of the Site after such date shall constitute your acceptance of such revised terms and conditions If any change to this Agreement is not acceptable to you your sole remedy is to cease accessing browsing and otherwise using the Site 2 1 This Site contains material including but not limited to software text graphics and images collectively referred to as the Content We may own the Content or portions of the Content may be made available to us through arrangements that we have with third parties The Content is protected by United States and foreign intellectual property laws Unauthorized use of the Content may result in violation of copyright trademark and other laws You have no rights in or to the Content and you will not copy the Content and will only access and use the Content for your personal purposes You may not sell transfer assign license sublicense or modify the Content or reproduce display publicly perform make a derivative version of distribute or otherwise use the Content in any way for any public or commercial purpose The use or posting of any of the Content on any other web site or computer network for any purpose is expressly prohibited If you violate any part of this Agreement your right to access and or use the Content and Site shall automatically terminate 2 2 The trademarks service marks and logos of the Company the Company Trademarks used and displayed on this Site are registered and unregistered trademarks or service marks of the Company Other company product and service names located on the Site may be trademarks or service marks owned by third parties the Third Party Trademarks and collectively with the Company Trademarks the Trademarks Nothing on this Site or in this Agreement should be construed as granting by implication estoppel or otherwise any license or right to use any Trademark displayed on this Site without the prior written consent of the Company specific for each such use The Trademarks may not be used to disparage the Company or the applicable third party the Company s or third party s products or services or in any manner using commercially reasonable judgment that may damage any goodwill in the Trademarks Use of any Trademarks as part of a link to or from any web site is prohibited without the Company s prior written consent All goodwill generated from the use of any Company Trademark shall inure to the Company s benefit 2 3 You agree not to a take any action that imposes an unreasonable load on the Site s infrastructure b use any device software or routine to interfere or attempt to interfere with the proper working of the Site or any activity being conducted on the Site c attempt to decipher decompile disassemble or reverse engineer any of the software comprising or making up the Site d delete or alter any material posted on the Site by the Company or any other person or entity or e frame or link to any of the materials or information available on the Site 2 4 The Site contains links to third party web sites External Sites These links are provided solely as aconvenience to you and not as an endorsement by us of the content on such External Sites The content of such External Sites is developed and provided by others You should contact a representative of those External Sites if you have any concerns regarding such links or any content located on such External Sites We are not responsible for the content of any linked External Sites and do not make any representations regarding the content or accuracy of any materials on such External Sites You should take precautions when downloading files from all web sites to protect your computer from viruses and other destructive programs If you decide to access any External Sites you do so at your own risk 2 5 Certain elements of the Site are protected by trade dress trademark unfair competition and other state and federal laws and may not be copied or imitated in whole or in part by any means including but not limited to the use of framing or mirrors except as otherwise expressly permitted by Section 2 1 of the Agreement None of the Content for this Site may be retransmitted without the express written consent from the Company for each and every instance 2 6 You may from time to time provide suggestions comments for enhancements or functionality or other feedback Feedback to us with respect to the Site or Content We shall have full discretion to determine whether or not to proceed with the development or implementation of any Feedback You hereby grants Company a royalty free fully paid up worldwide transferable sublicenseable irrevocable perpetual license to a copy distribute transmit display perform and create derivative works of the Feedback and b use the Feedback and or any subject matter thereof including without limitation the right to develop manufacture have manufactured market promote sell have sold offer for sale have offered for sale import have imported rent provide and or lease products or services which practice or embody or are configured for use in practicing the Feedback and or any subject matter of the Feedback 3 1 THE COMPANY ITS AFFILIATES THEIR RESPECTIVE OFFICERS DIRECTORS EMPLOYEES AGENTS SUPPLIERS OR LICENSORS COLLECTIVELY THE COMPANY PARTIES MAKE NO WARRANTIES ORREPRESENTATIONS ABOUT THE SITE OR CONTENT INCLUDING BUT NOT LIMITED TO ITS ACCURACY RELIABILITY COMPLETENESS TIMELINESS OR RELIABILITY THE COMPANY PARTIES SHALL NOT BESUBJECT TO LIABILITY FOR THE TRUTH ACCURACY OR COMPLETENESS OF THE SITE OR CONTENT OR ANY OTHER INFORMATION CONVEYED TO THE USER OR FOR ERRORS MISTAKES OR OMISSIONSTHEREIN OR FOR ANY DELAYS OR INTERRUPTIONS OF THE DATA OR INFORMATION STREAM FROMW","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"1119e777-fcdd-4daa-af24-a7066261601d":{"id_":"1119e777-fcdd-4daa-af24-a7066261601d","metadata":{"url":"https://vectara.com/legal/terms-of-service/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"26cd343f-5f65-492b-966c-fba62257e970","metadata":{"url":"https://vectara.com/legal/terms-of-service/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"rN6ypBLNibPyBTQfLuTH/HVDEme+DmjKRHw8rET+4+s=","text":"HATEVER CAUSE YOU AGREE THAT YOU USE THE SITE AND THE CONTENT AT YOUR OWN RISK THE COMPANY PARTIES DO NOT WARRANT THAT THE SITE WILL OPERATE ERROR FREE OR THAT THE SITE ITS SERVER OR THE CONTENT ARE FREE OF COMPUTER VIRUSES OR SIMILAR CONTAMINATION OR DESTRUCTIVE FEATURES IF YOUR USE OF THE SITE OR THE CONTENT RESULTS IN THE NEED FOR SERVICING OR REPLACING EQUIPMENT OR DATA NO COMPANY PARTY SHALL BE RESPONSIBLE FOR THOSE COSTS THE SITE AND CONTENT ARE PROVIDED ON AN AS IS AND AS AVAILABLE BASIS WITHOUT ANY WARRANTIES OF ANY KIND THE COMPANY PARTIES DISCLAIM ALL WARRANTIES INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF TITLE MERCHANTABILITY NONINFRINGEMENT OF THIRD PARTIES RIGHTS AND FITNESS FOR PARTICULAR PURPOSE 3 2 IN NO EVENT SHALL ANY COMPANY PARTY BE LIABLE FOR ANY DAMAGES WHATSOEVER INCLUDING WITHOUT LIMITATION INCIDENTAL AND CONSEQUENTIAL DAMAGES LOST PROFITS OR DAMAGES RESULTING FROM LOST DATA OR BUSINESS INTERRUPTION RESULTING FROM THE USE OR INABILITY TO USE THE SITE AND THE CONTENT WHETHER BASED ON WARRANTY CONTRACT TORT INCLUDING NEGLIGENCE OR ANY OTHER LEGAL THEORY EVEN IF SUCH COMPANY PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES 3 3 SOME STATES DO NOT ALLOW THE DISCLAIMER OR EXCLUSION OF CERTAIN WARRANTIES OR THE LIMITATION OR EXCLUSION OF LIABILITY FOR INCIDENTAL OR CONSEQUENTIAL DAMAGES ACCORDINGLY IN SUCH STATES SOME OF THE ABOVE LIMITATIONS MAY NOT APPLY TO YOU OR BEENFORCEABLE WITH RESPECT TO YOU AND THE LIABILITY OF THE COMPANY PARTIES SHALL BE LIMITED TO THE GREATEST EXTENT PERMITTED BY LAW 3 4 IF YOU ARE FROM NEW JERSEY THE FOREGOING SECTIONS 3 1 AND 3 2 AND SECTION 4 BELOWARE INTENDED TO BE ONLY AS BROAD AS IS PERMITTED UNDER THE LAWS OF THE STATE OF NEWJERSEY IF ANY PORTION OF THESE SECTIONS IS HELD TO BE INVALID UNDER THE LAWS OF THE STATE OF NEW JERSEY THE INVALIDITY OF SUCH PORTION SHALL NOT AFFECT THE VALIDITY OF THE REMAINING PORTIONS OF THE APPLICABLE SECTIONS To the extent permitted under applicable law you agree to defend indemnify and holdharmless the Company Parties from and against any claims actions or demands including without limitation reasonable legal and accounting fees arising or resulting from your breach of this Agreement or your access to use or misuse of the Content or Site The Company shall provide notice to you of any such claim suit or proceeding The Company reserves the right to assume the exclusive defense and control of any matter which is subject to indemnification under this section In such case you agree to cooperate with any reasonable requests assisting the Company s defense of such matter 5 1 The Company reserves the right in its sole discretion to restrict suspend or terminate this Agreement and your access to all or any part of the Site or the Content at any time and for any reason without prior notice or liability The Company reserves the right to change suspend or discontinue all or any part of the Site or the Content at any time without prior notice or liability 5 2 Sections 2 Use of the Site 3 Limitation of Liability and Warranty 4 Indemnification 5 Termination of Agreement and 8 Miscellaneous shall survive the termination of this Agreement 6 1 This Site is hosted in the United States We make no claims concerning whether the Content may bedownloaded viewed or be appropriate for use outside of the United States If you access the Site or the Content from outside of the United States you do so at your own risk Whether inside or outside of the United States you are solely responsible for ensuring compliance with the laws of your specific jurisdiction 6 2 The United States controls the export of products and information You expressly agree to comply with such restrictions and not to export or re export any of the Content to countries or persons prohibited under the export control laws By downloading the Content you are expressly agreeing that you are not in a country where such export is prohibited or are a person or entity for which such export is prohibited You are solely responsible for compliance with the laws of your specific jurisdiction regarding the import export or re export of the Content The Content is provided with RESTRICTED RIGHTS Use duplication or disclosure by the Government is subject to the restrictions contained in 48 CFR 52 227 19 and 48 CFR 252 227 7013 et seq or its successor Use of the Site or Content by the Government constitutes acknowledgement of our proprietary rights in the Site and Content This Agreement is governed by the internal substantive laws of the State of California without respect to its conflict of laws provisions You expressly agree to submit to the exclusive personal jurisdiction of the state and federal courts located in the City of San Jose in the State of California If any provision of this Agreement is found to be invalid by any court having competent jurisdiction the invalidity of such provision shall not affect the validity of the remaining provisions of this Agreement which shall remain in full force and effect Failure of the Company to act on or enforce any provision of the Agreement shall not be construed as a waiver of that provision or any other provision in this Agreement No waiver shall be effective against the Company unless made in writing and no such waiver shall be construed as a waiver in any other or subsequent instance Except as expressly agreed by the Company and you this Agreement constitutes the entire Agreement between you and the Company with respect to the subject matter and supercedes all previous or contemporaneous agreements whether written or oral between the parties with respect to the subject matter The section headings are provided merely for convenience and shall not be given any legal import This Agreement will inure to the benefit of our successors assigns licensees and sublicensees Any information submitted or provided by you to the Site might be publicly accessible Important and private information should be protected by you Data Processing Agreement 2024 Vectara Inc All rights reserved Prior to founding Vectara Amr served as the VP of Developer Relations for Google Cloud Amr co founded Cloudera where he led the development of enterprise tools to ingest and extract value from big data Before Cloudera Amr served as VP of product intelligence engineering at Yahoo after Yahoo acquired his first startup Aptiva a search engine company Amr holds a PhD in Electrical Engineering from Stanford University and a MA from Cairo University Prior to founding Vectara Amin served as a senior engineer at Google Research for 10 years Amin led the development of question answering and neural information retrieval systems that have been launched into several Google products His 20 years of search industry experience include working with Fortune 500 companies startups and state and federal governments Amin holds a BS in Computer Science and Mathematics from Bowling Green State University","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"ad4f6a05-cc2c-4f56-9103-24a3c2303234":{"id_":"ad4f6a05-cc2c-4f56-9103-24a3c2303234","metadata":{"url":"https://vectara.com/blog/vectara-and-airbyte/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"009fea4e-97a2-4990-bef5-54d07ae8a042","metadata":{"url":"https://vectara.com/blog/vectara-and-airbyte/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"zX0TiuclAx7b4vRNfJd40BlWXFxqY8maUM8Xii4ATDQ=","text":"Vectara and Airbyte IntroductionVectara and Airbyte Better TogetherExample RAG based on Documents in Google DriveQuerying the dataSummaryRecommended ContentVectara Airbyte Destination connector Setting up VectaraSetting up an Airbyte pipeline Google Drive to Vectara PlatformSolutionsResourcesPricingCompany Blog Post Application Development  Automating data ingestion for 350 sources into Vectara s GenAI platform using the Airbyte destination connector  January 17 2024 by Ofer Mendelevitch When building a GenAI or semantic search application with Vectara one of the most important considerations is how to architect your data ingestion pipeline that transfers data from your systems into Vectara in a way that scales well is robust to system failures and provides a mechanism for incremental updates We recently added a Vectara destination connector to Airbyte s list of destination connectors which makes this process quick and easy In this blog post we will review this new Vectara connector explain how to set up an ETL flow from one of Airbyte s source connectors and provide an end to end example for ingesting text from documents in a Google Drive into Vectara using Airbyte Vectara offers powerful generative AI capabilities for developers via an easy to use API Often referred to as RAG in a box Vectara s platform simplifies the development of GenAI applications by taking care of the heavy lifting required for building RAG retrieval augmented generation applications document chunking embedding vector storage state of the art retrieval and summarization are all handled behind the scenes and in a scalable and secure fashion When building a GenAI application there are two main data flows Vectara s API provides native support for indexing text and uploading files however it is often up to the user to take care of the details In particular the application developer is required to build the code that would crawl the source data convert it into text and ingest it into Vectara using the API With the complexity of data sources in the enterprise this can easily become more complex than originally anticipated and hard to maintain over time Furthermore as the source data updates the developer needs to ensure incremental updates are performed properly Enter Airbyte an open source tool for data movement with 350 connectors designed specifically to help address the data ingestion problem Besides offering a large breadth of connectors for APIs and databases that work out of the box Airbyte solves common data integration problems like incremental syncs schema evolution and sync observability in a single place in a consistent manner for all your data integration needs Using Airbyte s capabilities for data movement at scale using the Vectara destination connector makes it easier for GenAI developers to build scalable enterprise grade GenAI applications with immediate access to a growing list of data connectors to deal with various enterprise needs Google Drive is pretty ubiquitous although other competing solutions like Microsoft s OneDrive Box com and Dropbox are also pretty good and especially for those using Google Suite it s common to have all your documents hosted there But how many times did you want to find that document you know exists in your Google Drive but you just can t remember where it is Or maybe you want to ask a question and have a response based on all the relevant documents hosted on your Google Drive I know I have So let s build an example of a GenAI application that allows you to do just that using Airbyte to index your Google Drive into Vectara and build a question answering application using that data Our first step would be to sign up for an account on Vectara if you don t already have one You can create a corpus for your application but the Vectara connector also includes the capability to generate this target corpus for you and all you need is a corpus name Note that the Vectara destination adds a special metadata field to the Vectara corpus called _ab_stream which is used internally in the ingest flow Vectara s free account can include up to 50MB of text extracted from your Google Drive documents If you need to index larger amounts of text you can add a credit card to increase your Vectara account data quota to the size you need or contact sales for the Vectara Scale plan For this blog post I ll use a publicly available Google Drive folder that includes seven of William Shakespeare texts However in order to simulate a real life situation where the folder is private I ve made a copy of it to my local Google Drive folder under the name shakespeare To install Airbyte on an EC2 instance we follow the instructions in this guide Usually we would need to follow this step by step guide to set up the right permissions for the Google Drive folder to ensure we have the right credentials set up allowing the Google Drive source connector to work properly To setup the connection we go to the main Airbyte dashboard screen at localhost 8000  Now let s set up the connection between our Google Drive and Vectara following these 3 simple steps That s it Once you finish configuring the connection and it s enabled Airbyte will automatically sync the contents of the source Google folder with Vectara Airbyte will make sure your Vectara corpus stays in sync with your source data with minimal overhead and notify you in case the data replication can t be completed Now all the content of these seven wonderful works of Shakespeare are ingested into Vectara let s do some querying In Vectara s Console the query tab can be use to run some sample queries Let s try Who is Juliet  Or we can ask Who is King Lear to get the following  Of course you can use Vectara s query API to run your own queries and integrate them into your GenAI App Or use the vectara answer tool to build a full functional question answering application In this blog post we ve seen the power of using Airbyte s powerful data movement platform together with Vectara s trusted platform to build GenAI applications We ve provided an example for moving data from Google Drive to Vectara but of course many other data sources are available via Airbyte such as Salesforce Airtable Asana BigQuery or Elastic just to name a few For more complex setups it can make sense to first move raw data into a warehouse to perform transformations before eventually loading them into Vectara this post about ELTP architectures goes into greater detail of such a setup We encourage you to try this new exciting integration with your own data and let us know how it works we re always excited to hear about your GenAI projects And as always if you have any questions please feel free to join the Vectara Discord server or Airbyte s Slack and let us know Ofer Mendelevitch leads developer relations at Vectara He has extensive hands on experience in machine learning data science and big data systems across multiple industries and has focused on developing products using la","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"e437808c-2b0c-46f0-abc4-07cec1a16a3e":{"id_":"e437808c-2b0c-46f0-abc4-07cec1a16a3e","metadata":{"url":"https://vectara.com/blog/vectara-and-airbyte/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"27c91996-e8ad-41f4-b2ec-12db8deefc40","metadata":{"url":"https://vectara.com/blog/vectara-and-airbyte/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"QlqZLU+bgFeyTBLxTM3T7wV61V13NNb1G7dr31BDeGg=","text":"rge language models since 2019 Prior to Vectara he built and led data science teams at Syntegra Helix Lendup Hortonworks and Yahoo Ofer holds a B Sc in computer science from Technion and M Sc in EE from Tel Aviv university and is the author of Practical Data Science with Hadoop Addison Wesley Research August 8 2023 by Ofer Mendelevitch Tallat Shafaat 7 min Read Retrieval and Search December 6 2023 by Ofer Mendelevitch 8 min Read Data ingestion May 16 2023 by Ofer Mendelevitch 5 min Read docs The documentation for Vectara s destination connector for AirByte 2024 Vectara Inc All rights reserved Prior to founding Vectara Amr served as the VP of Developer Relations for Google Cloud Amr co founded Cloudera where he led the development of enterprise tools to ingest and extract value from big data Before Cloudera Amr served as VP of product intelligence engineering at Yahoo after Yahoo acquired his first startup Aptiva a search engine company Amr holds a PhD in Electrical Engineering from Stanford University and a MA from Cairo University Prior to founding Vectara Amin served as a senior engineer at Google Research for 10 years Amin led the development of question answering and neural information retrieval systems that have been launched into several Google products His 20 years of search industry experience include working with Fortune 500 companies startups and state and federal governments Amin holds a BS in Computer Science and Mathematics from Bowling Green State University","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"3aedcfab-35cc-4e90-875d-9da978343d8b":{"id_":"3aedcfab-35cc-4e90-875d-9da978343d8b","metadata":{"url":"https://vectara.com/faqs/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"78eeb410-e211-450c-860c-a82a806fbfb9","metadata":{"url":"https://vectara.com/faqs/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"B5pllmton8Vtx/GAQf5IgtzaukYR95IdeIJ+9fShyE0=","text":"Vectara FAQsAbout VectaraSoftwareDeploymentPricingSecurity and PrivacyGeneral  What is Vectara How does Vectara work How can I benefit from Vectara What are the use cases for Vectara How is Vectara different from other search solutions What countries is Vectara available in How do I become a Vectara partner How do I apply for a job at Vectara How do I get up and running with Vectara How long does it take to implement Vectara What file types does Vectara support What languages does Vectara support Can I index from any data source Can I search across multiple indexes What are the deployment options for Vectara What resources do I need to deploy Vectara Does Vectara have any scalability limits I am having trouble setting up Vectara What should I doWhat are Vectara s SLAs What are the different options for buying Vectara Is there a free plan What is Vectara s pricing model How do you count queries in your pricing model What is the definition of account size in the pricing model How does billing work Which payment methods and currencies are accepted by Vectara Is there any commitment once I start paying for Vectara What happens if I exceed my committed plan usage How do I switch from one plan to another How can I change my account details and billing information How does Vectara handle and process personal and customer sensitive information Does Vectara implement any encryption standards for information processing Does Vectara have a security program Does Vectara have a privacy policy Does Vectara have a disaster recovery plan I have a question that is not listed here How can I get an answer PlatformSolutionsResourcesPricingCompany TABLE OF CONTENTS  How does Vectara work How can I benefit from Vectara What are the use cases for Vectara How is Vectara different from other search solutions What countries is Vectara available in How do I become a Vectara partner How do I apply for a job at Vectara  How long does it take to implement Vectara What file types does Vectara support What languages does Vectara support Can I index from any data source Can I search across multiple indexes  What resources do I need to deploy Vectara Does Vectara have any scalability limits I am having trouble setting up Vectara What should I do What are Vectara s SLAs  Is there a free plan What is Vectara s pricing model How do you count queries in your pricing model What is the definition of account size in the pricing model How does billing work Which payment methods and currencies are accepted by Vectara Is there any commitment once I start paying for Vectara What happens if I exceed my committed plan usage How do I switch from one plan to another How can I change my account details and billing information  Does Vectara implement any encryption standards for information processing Does Vectara have a security program Does Vectara have a privacy policy Does Vectara have a disaster recovery plan Back to top Vectara is a search engine powered by large language models and the latest neural network and information retrieval technologies to deliver search results with high efficacy and low latency Developers and engineers can use Vectara to add advanced semantic search capabilities in their sites and applications Vectara can understand queries in natural language phrases and questions regardless of how the question is asked Vectara uses LLMs and neural networks to apply complete vector encoding for all indexed content and all queries allowing for a high efficacy semantic response Vectara combines native neural indexing for all content statistical keyword algorithms and re rankers to maximize the relevance of results across a wide spectrum of queries and data classes You can benefit from Vectara if Vectara has many core features in its Search as a Service platform and we have unique features that address limitations of current in market systems Some of Vectara s unique differentiators include Vectara is a cloud based search engine running on AWS infrastructure See the AWS Regional Services page for more details Please contact us at partnerships vectara com You can check all our available openings on our Careers page All you need to do is sign up with a company email address You will then get access to the Vectara console to get started with ingesting documents and testing the search engine For more information on setting up Vectara you can check out our Getting Started and Docs Setting up and implementing Vectara in production can be done on the same day Index your first document and issue your first batch of queries in under 5 minutes Vectara supports PDF Microsoft Word Microsoft Powerpoint Open Office HTML JSON XML email in RFC822 text RTF ePUB and Common Mark You can index and search on in the world s most commonly written 11 languages You can index data from any text document in a supported file format via Vectara APIs or the file upload feature within the Vectara console Yes you can issue a single query or multiple queries in parallel to one or multiple indexes Vectara is a fully managed cloud platform maintained by the Vectara team and deployed on AWS Like other multi tenant SaaS services it employs a release process designed to ensure features ship faster the product can scale seamlessly when any client load increases and it leverages enterprise grade security provided in part by AWS This eliminates the responsibility for server maintenance upgrades and capacity provisioning No additional or specialized engineering hardware or infrastructure resources are needed to successfully deploy Vectara Vectara was developed to make it easy for web and application developers to integrate semantic search in sites and applications without the need for additional training or resources For more information on setting up Vectara you can check out our Getting Started and Docs Vectara can auto scale from small volumes of text to millions of documents without the need to manage the provisioning of additional computing infrastructure Vectara automatically adds capacity as needed to handle higher query volumes If you are having any trouble setting up Vectara or need any help with implementation you can send a message on our Vectara Discourse Community or contact Vectara support directly at support vectara com You can also visit https support vectara com Vectara offers SLAs in support availability and performance For a full list of available Vectara SLAs please review our Pricing page and order form documents There are 2 Vectara subscription plans to choose from Growth and Scale You can visit Vectara s pricing page to view the different features for each plan and determine the best one for your needs Yes Vectara s Growth plan is free with a defined query volume that resets every month and storage capacity allocation Customers who need more queries and or storage capacity can purchase additional capacity bundles on the Growth plan Vectara has a usage based pricing model that is a function of search queries processed and","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"998d3ac9-9fd9-475e-a12d-b7bf6bfc4ac3":{"id_":"998d3ac9-9fd9-475e-a12d-b7bf6bfc4ac3","metadata":{"url":"https://vectara.com/faqs/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"35d49b71-0708-4760-8978-090fa5562e0e","metadata":{"url":"https://vectara.com/faqs/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"fEXSks5W5E98N4empq0c0tyVwAVu+aasnE3msQ1cEJI=","text":"account size Visit our pricing page for more details Any queries that are issued to indexed content via the Vectara console or the Vectara API are counted towards the query count Account size is the sum of text size measured in MB within all corpora in the customer account before any replication factor is applied For Growth plan users Vectara s free tier supports up to 15 000 queries per month and 50 MB of storage capacity No credit card is required at signup and no billing will occur For Growth plan users that require additional capacity bundles of queries and account size a credit card is required For Growth plan users the Billing tab within the Vectara Console contains invoice and payment history Vectara accepts payments made through a credit card Vectara supports billing in United States Dollar USD For Vectara s Growth plan there is no commitment requirement Vectara users will receive a notification when they have consumed their monthly query allocation Users will then get an option to buy additional capacity or wait until their account resets for the next month Some upgrade and downgrade functions can be performed directly within the Vectara Console For any other requests please reach out directly to sales vectara com Account details and billing information can be accessed and changed from the Billing tab within the Vectara Console Vectara supports full client control over how data is preserved including the support of a full deletion of customer data via API Clients decide what data they transmit to the service and what data remains within the service with the exception of billing and billing contact data Vectara ensures sensitive data is encrypted and the keys are managed both logically and physically by the appropriate teams Encryption at rest is AES 256 bit symmetric key encryption Separate keys are managed per corpus and access to keys is through an account master key managed on FIPS 140 2 compliant hardware Vectara also provides the option of a customer managed account master key Encryption in transit is TLS 1 3 Vectara has a documented security program that is audited periodically for major security program objectives status of security program non conformities and risk logs The system architecture was designed to enable ready compliance with SOC 2 and ISO 27001 standards Vectara has a documented privacy policy that is reviewed periodically Vectara s privacy policy covers responsibilities under GDPR regulations CCPA regulations are not applicable to Vectara Vectara s privacy policy can be found here Vectara has policies and procedures for disaster recovery and backup Vectara has established documented implemented and maintained processes procedures and controls to ensure the required level of continuity for information security during an adverse situation Information processing facilities infrastructure and application architecture are implemented with redundancy sufficient to meet and support high availability requirements You can contact Vectara Support to get an answer to your question at support vectara com or https support vectara com 2024 Vectara Inc All rights reserved Prior to founding Vectara Amr served as the VP of Developer Relations for Google Cloud Amr co founded Cloudera where he led the development of enterprise tools to ingest and extract value from big data Before Cloudera Amr served as VP of product intelligence engineering at Yahoo after Yahoo acquired his first startup Aptiva a search engine company Amr holds a PhD in Electrical Engineering from Stanford University and a MA from Cairo University Prior to founding Vectara Amin served as a senior engineer at Google Research for 10 years Amin led the development of question answering and neural information retrieval systems that have been launched into several Google products His 20 years of search industry experience include working with Fortune 500 companies startups and state and federal governments Amin holds a BS in Computer Science and Mathematics from Bowling Green State University","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"78e9b0e8-58aa-42db-ab27-6dc157b90926":{"id_":"78e9b0e8-58aa-42db-ab27-6dc157b90926","metadata":{"url":"https://vectara.com/about-vectara/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"78e0fb2e-63bd-42d4-bf10-4c6304e41e91","metadata":{"url":"https://vectara.com/about-vectara/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"qNVlSAkU2cikZ1qAMBzfsdatp+cNvG/gNgoTEuF29fE=","text":"About VectaraOur FoundersOur InvestorsSee what people are saying about us We re Hiring The Future of SearchWhat Problem Do We Solve About Vectara Zafer Ipekci CEO MetusErin Loy VP Software Engineering ApexChatEric Colson Fmr Chief Algorithms Officer Stitch FixMike Olson Cofounder ClouderaShane Feldberg Founder Feld VenturesBradley Horowitz VP of Product GoogleRob Solomon Board Director GoFundMe orgStewart Butterfield CEO Slack TechnologiesOmar Tawakol General Manager Webex CiscoSaqib E Awan Founder Managing Director GTM CapitalAdam Ghobarah Founder Top Harvest CapitalEmad Mostaque Founder Stability AI PlatformSolutionsResourcesPricingCompany Vectara s mission is to help the world find meaning through search The company uses the latest innovations in artificial intelligence and neural network technologies for natural language processing to deliver unparalleled search relevance The way people search is changing We believe that search in every application SaaS app mobile app ecommerce site media property website enterprise data lake and beyond will be powered by semantic search within the next five years As AI powers search search itself will provide the primary response for almost every question by both human and application requests for information retrieval related content recommendations and user intent across document image video and voice Until recently search technologies have been keyword based With advances in the use of AI and natural language processing Vectara is expanding the role and application of search Vectara s semantic search platform delivers dramatic improvement in understanding questions and answering with more relevant information Vectara is different from any search you have used prior Vectara is a semantic search software company that enables customers to understand exactly what their users are asking no matter how they ask to provide extraordinarily relevant results Our 100 neural search technology provides radical increases in search relevance when compared with keyword based algorithms Vectara s complete search pipeline is available through an API first platform that allows developers to easily add semantic search within their sites and applications Co Founder CEO Co Founder CTO Co Founder Chief Architect Additionally many technology pioneers have invested in Vectara including founders of WhatsApp Slack and Palantir Search relevance across different media types is a difficult engineering problem Vectara is empowering us to differentiate from our current competitors in product performance and reliability  Vectara helps us understand the intent behind the thousands of requests our agents handle every day to provide faster responses and more accurate information NLP powered search is improving the way we serve our customers and their end users I am very excited for how Vectara enables better search on product reviews This will create tremendous value for both consumers and ecommerce companies Vectara is opening the door to transformational change in search technology not only delivering more accurate and relevant results but making cutting edge neural technology accessible to almost any developer  Vectara s neural search is the future of how people will interact with content They are solving a major problem the founders are incredibly well suited for the task and their early customer feedback is very promising Vectara is the first platform that harnesses NLP s transformative power and makes it practical and useful empowering developers everywhere with the most advanced AI available  Vectara represents a huge leap forward in search technology Offering neural search as a service the platform provides a complete solution that gives developers API access to the most advanced NLP AI site level search in just minutes Most people rely on search to navigate websites and applications including our product but the underlying technology has not significantly evolved in decades By applying cutting edge neural networks Vectara is bringing much needed innovation to improve product search Vectara is neural search as a service The platform provides a complete search pipeline from extraction and indexing to retrieval rerank and calibration Every platform element is API addressable Developers can embed the most advanced NLP for app and site search in minutes When Vectara presented their capabilities to our audience of hundreds of CIOs and CTOs from Global 2000 firms customer reaction was incredibly positive Many signed up for POCs after seeing the product I knew right away that we found an incredible company and team in Vectara The Vectara team is exceptional in their ability to take the latest advances in natural language understanding and seamless multilingual support and bring it to mainstream users as a turnkey search solution The team s technical capability and customer focus are impressive I am delighted to be personally included as one of the angel investors for Vectara Their ML researchers truly understand how to leverage large language models as many of them worked on these technologies at Google AI research If you are interested in exploring roles on the Vectara team take a look at our open positions 2024 Vectara Inc All rights reserved Prior to founding Vectara Tallat served as a Senior Software Engineer for Google Search and Google Ads He was a core member of the Google Knowledge Graph indexing team where he designed and implemented systems that processed petabytes of data and serviced up to 200 000 queries per second Tallat holds a PhD in Distributed Systems from KTH Sweden Prior to founding Vectara Amr served as the VP of Developer Relations for Google Cloud Amr co founded Cloudera where he led the development of enterprise tools to ingest and extract value from big data Before Cloudera Amr served as VP of product intelligence engineering at Yahoo after Yahoo acquired his first startup Aptiva a search engine company Amr holds a PhD in Electrical Engineering from Stanford University and a MA from Cairo University Prior to founding Vectara Amin served as a senior engineer at Google Research for 10 years Amin led the development of question answering and neural information retrieval systems that have been launched into several Google products His 20 years of search industry experience include working with Fortune 500 companies startups and state and federal governments Amin holds a BS in Computer Science and Mathematics from Bowling Green State University","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"5d24852c-ef13-4c6a-8349-8485c8c42438":{"id_":"5d24852c-ef13-4c6a-8349-8485c8c42438","metadata":{"url":"https://vectara.com/legal/privacy-policy/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"1bfa114c-4e1d-498c-9799-91a2bd80d8f3","metadata":{"url":"https://vectara.com/legal/privacy-policy/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"K4d5wvXRQs+2LAn0Npg4BL7R5ZXlI0f6VenS3ciSNSY=","text":"Privacy Policy Personal Information We CollectHow We Use Personal InformationHow We Share Personal InformationYour ChoicesOther Sites and ServicesData SecurityChildrenChanges to This Privacy PolicyHow to Contact Us  PlatformSolutionsResourcesPricingCompany Vectara Inc Vectara we us or our provides a SaaS based machine learning powered text search platform the Services This Privacy Policy describes how we handle personal information that we collect about our customers including when they use our Services or visit our website https vectara com We treat all personal information covered by this Privacy Policy as pertaining to individuals acting as business representatives and not in their individual or household capacity This Privacy Policy does not apply to our handling of personal information that we process on behalf of our customers as a service provider Information you provide to us may include Account information and contact details such as your company s name your first and last name username and password email address mailing address and phone number Payment information including your payment account details which we may obtain from your financial institution when you choose to connect your account to the Services Communications that we exchange with you including when you contact us with questions feedback or otherwise Marketing data such as your preferences for receiving our marketing communications and details about your engagement with them Other data not specifically listed here which we will use as described in this Privacy Policy or as otherwise disclosed at the time of collection Third party sources We may combine personal information we receive from you with personal information we obtain from other sources such as Automatic data collection We our service providers and our advertising partners may automatically log information about you your computer or mobile device and your interaction over time with our website and the Services in your capacity as our customer s authorized user our communications and other online services such as Device data such as your computer s or mobile device s operating system type and version manufacturer and model browser type screen resolution RAM and disk size CPU usage device type e g phone tablet IP address unique identifiers including identifiers used for advertising purposes language settings mobile device carrier radio network information e g WiFi LTE 3G and general location information such as city state or geographic area Online activity data such as pages or screens you viewed how long you spent on a page or screen the website you visited before browsing to the website navigation paths between pages or screens information about your activity on a page or screen access times and duration of access and whether you have opened our marketing emails or clicked links within them We use the following tools for automatic data collection Cookies which are text files that websites store on a visitor s device to uniquely identify the visitor s browser or to store information or settings in the browser for the purpose of helping you navigate between pages efficiently remembering your preferences enabling functionality helping us understand user activity and patterns and facilitating online advertising Local storage technologies like HTML5 that provide cookie equivalent functionality but can store larger amounts of data including on your device outside of your browser in connection with specific applications Web beacons also known as pixel tags or clear GIFs which are used to demonstrate that a webpage or email was accessed or opened or that certain content was viewed or clicked We use your personal information for the following purposes To operate our Services Research and development As part of these activities we may create aggregated de identified or other anonymous data from personal information we collect We may use this anonymous data and share it with third parties for our lawful business purposes including to analyze and improve the Services and promote our business Marketing and advertising including for Compliance and protection including to We may share your personal information with Affiliates Our corporate parent subsidiaries and affiliates for purposes consistent with this Privacy Policy Service providers Companies and individuals that provide services on our behalf or help us operate our Services or our business such as banking partners hosting information technology customer support email delivery and website analytics services Advertising partners Third party advertising companies including for the interest based advertising purposes described above that may collect information on our website through cookies and other automated technologies Professional advisors Professional advisors such as lawyers auditors bankers and insurers where necessary in the course of the professional services that they render to us Authorities and others Law enforcement government authorities and private parties as we believe ingood faith to be necessary or appropriate for the compliance and protection purposes described above Business transferees Acquirers and other relevant participants in business transactions or negotiations for such transactions involving a corporate divestiture merger consolidation acquisition reorganization sale or other disposition of all or any portion of the business or assets of or equity interests in Vectara or our affiliates including in connection with a bankruptcy or similar proceedings Opt out of marketing communications You may opt out of marketing related communications by following the opt out or unsubscribe instructions contained in the marketing communications we send you Online tracking opt out There are a number of ways to limit online tracking which we have summarized below Note that because these opt out mechanisms are specific to the device or browser on which they are exercised you will need to opt out on every browser and device that you use Do Not Track Some Internet browsers may be configured to send Do Not Track signals to the online services that you visit We currently do not respond to Do Not Track or similar signals To find out more about Do Not Track please visit https www allaboutdnt com Our Services may contain links to websites and other online services operated by third parties In addition our content may be integrated into web pages or other online services that are not associated with us These links and integrations are not an endorsement of or representation that we are affiliated with any third party We do not control websites or online services operated by third parties and we are not responsible for their actions We employ a number of technical organizational and physical safeguards designed to protect the personal information we collect However no security measures are failsafe and we cannot guarantee the security of your personal information The Services are not intended","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"494cb98a-2fc9-4e19-9fc0-d58b5a6ea9cb":{"id_":"494cb98a-2fc9-4e19-9fc0-d58b5a6ea9cb","metadata":{"url":"https://vectara.com/legal/privacy-policy/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"975bc65f-29dc-4ba8-a523-bccb60b25ccd","metadata":{"url":"https://vectara.com/legal/privacy-policy/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"NfN2ea1o+yxbzmh8K0G7tRV2Wnx0iHXhKkHkf30ej6o=","text":"for use by children under 13 years of age If we learn that we have collected personal information through our Services from a child under 13 without the consent of the child s parent or guardian as required by law we will delete it We reserve the right to modify this Privacy Policy at any time If we make material changes to this Privacy Policy we will notify you by updating the date of this Privacy Policy and posting it on the website You can reach us by email at privacy vectara com or at the following mailing address Vectara Inc 395 Page Mill Road 275Palo Alto CA 94306 2024 Vectara Inc All rights reserved Prior to founding Vectara Amr served as the VP of Developer Relations for Google Cloud Amr co founded Cloudera where he led the development of enterprise tools to ingest and extract value from big data Before Cloudera Amr served as VP of product intelligence engineering at Yahoo after Yahoo acquired his first startup Aptiva a search engine company Amr holds a PhD in Electrical Engineering from Stanford University and a MA from Cairo University Prior to founding Vectara Amin served as a senior engineer at Google Research for 10 years Amin led the development of question answering and neural information retrieval systems that have been launched into several Google products His 20 years of search industry experience include working with Fortune 500 companies startups and state and federal governments Amin holds a BS in Computer Science and Mathematics from Bowling Green State University","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"dc30c416-45d8-4aae-a567-6f88a34abab4":{"id_":"dc30c416-45d8-4aae-a567-6f88a34abab4","metadata":{"url":"https://linkedin.com/company/vectara"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"fd31c2be-4ec8-4417-9109-3f7c3ba37ec3","metadata":{"url":"https://linkedin.com/company/vectara"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"1SFFw7r4sXuTiZtbGBG+5+0otdHK8D+7aIlgTVKJ+sM=","text":"Vectara Software Development About us Products Locations Employees at Vectara Updates Account Executive Small Midsize Businesses SMB Join now to see what you are missing Similar pages Funding Sign in to see who you already know at Vectara Welcome back Palo Alto CA 6 862 followers GenAI Conversational Search Discovery Platform Wei Zheng Amr Awadallah Tony Pecora Mansour Karam Vectara Palm Coast FL GitHub vectara hallucination leaderboard Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents Introducing Vectara s Custom Prompt Engine Vector Database Do You Really Need One Vectara Introduces Game Changing GenAI Chat Module Turbocharging Conversational AI Development Techedgeai Cloudera ZIR AI Pinecone Databricks Vectra AI Google Race Capital Snowflake Cohere Stability AI Vectara is The Trusted GenAI Platform for All Builders Retrieval Augmented Generation as a Service RAGaaS Enterprise Search Software Founder CEO Vectara The Trusted GenAI Platform Chief of Staff at Vectara High Tech Entrepreneur Executive Seed Investor github com https vectara com https vectara com https www techedgeai com Vectara is The Trusted GenAI Platform for All Builders Retrieval Augmented Generation as a Service RAGaaS to Power Your Busiiness Put GenAI into Action Vectara is an end to end platform for product builders to embed powerful generative AI features into their applications with extraordinary results Built on a solid hybrid search core Vectara delivers the shortest path to a correct answer action through a safe secure and trusted entry point Vectara is a platform for companies with moderate to no AI experience that solves use cases including conversational AI question answering semantic app search and research analysis Vectara provides an end to end SaaS solution abstracting the complex ML Operations pipeline Extract Encode Index Retrieve Re Rank Summarize Vectara is built for product managers and developers with an easily leveraged API that gives full access to the platform s powerful features Vectara s Grounded Generation allows businesses to quickly safely and affordably integrate best in class conversational AI and question answering into their application with zero shot precision Vectara never trains on your data allowing businesses to embed generative AI capabilities without the risk of data or privacy violations External link for Vectara Vectara is a GenAI conversational search and discovery platform that allows businesses to have intelligent conversations utilizing their own data think ChatGPT but for your data Developer first the platform provides an easy to use API and gives developers access to cutting edge NLU Natural Language Understanding technology with industry leading relevance The platform ensures data security and privacy with strong encryption while ensuring no customer data is used for training models With Vectara s Grounded Generation businesses can quickly and affordably integrate best in class search and question answering into their application knowledge base website chatbot or support helpdesk Visit Vectara com for more information 395 Page Mill Road Ste 275 Palo Alto CA 94306 US 6 862 followers HHEM Flash Update Does Google Gemma hallucinate more or less than Gemini Orca Cohere or Anthropic Claude2 See how they compare on the opensource Hughes Hallucination Evaluation Model Leaderboard https bit ly 48qk5Wu 6 862 followers Vectara s Personal API Key a simpler way for developers to explore and prototype Bypass OAuth complexity and integrate with external applications easily Ideal for rapid prototyping or when OAuth 2 0 support is pending DeveloperToolsGet started today https bit ly 3OKXkpf 6 862 followers Executive Coach Head of Staff Vectara Former Manager Exec Business Partners Google Vectara is growing and Shawn Clink is hiring Know anyone who might be interested 6 862 followers Vectara has seamlessly integrated into the LangFlow ecosystem making it simpler than ever to transform your data into an interactive chatbot Check out the step by step tutorial on creating your own RAG Chatbot Ready to bring your data to life and give it a voice Dive into our tutorial and start building today https bit ly 3wkkPPD 6 862 followers We re dedicated to trust and transparency in AI providing you with the most current and comprehensive leaderboard for large language models Recently Google unveiled their latest open LLM named Gemma which has been added to our leaderboard It currently ranks in the middle exhibiting a hallucination rate of 7 5 Check out our leaderboard and see for yourself how Gemma and other models compare https lnkd in gKKc2H6j 6 862 followers Thrilled to introduce Vectara s newest feature the Custom Prompt Engine Take your Retrieval Augmented Generation RAG system to the next level by designing custom prompts that utilize relevant text and metadata presenting a tailored answer to your GenAI needs With the Custom Prompt Engine we built an RFI question answering bot right inside Google Sheets using fewer than 70 lines of Javascript This is only the start we can t wait to see the amazing tools our users will build using this new functionality Learn more in our blog https bit ly 49JIase Ready to try it out Get in touch with our GenAI experts today https bit ly 49JIaIK 6 862 followers Hot Take You don t really need a vector database With the rise of RAG and platforms integrating vector search capabilities the landscape is shifting Vectara is at the forefront offering an all in one solution that simplifies the process for developers bypassing the complex DIY route and heading straight to comprehensive solutions Learn more in our latest blog https bit ly 49zF1eQ 6 862 followers New Episode Release Dive into the latest episode of the Vectara UX Series In Episode 2 we explore the simplicity of integrating a semantic search UI with react search in your react applications Learn what react search is and how it can enhance your codebase today Watch now using the link below https bit ly 48mI20YSign up today for a free Vectara account https bit ly 49nfPId GenAI RAGaaS VectaraVision 6 862 followers Don t miss out on this insightful article by TechEdge AI Explore in depth insights into Vectara Chat s key features functionalities and details that make our chat platform stand out 392 followers Vectara Introduces Game Changing GenAI Chat Module Turbocharging Conversational AI Development Read the full story https lnkd in gxtQqTwqShane Connelly Tallat M Shafaat techedge techedgeai ai conversationalai aidevelopment genai llm aimodule machinelearning automation innovation technology 6 862 followers Exciting to see this year s State of Data and AI Survey by Airbyte Find out more on how we are working with Airbyte to connect AI to data 20 768 followers Calling all Data Practitioners Airbyte is running an industry wide survey and we want to hear from you The State of Data AI 2024 survey is a vendor agnostic survey about the tools you use in","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"d8696cc5-8b75-4039-80c6-789cf727bffc":{"id_":"d8696cc5-8b75-4039-80c6-789cf727bffc","metadata":{"url":"https://linkedin.com/company/vectara"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"a7fabe5f-7715-4dee-bff2-c21d488ced0a","metadata":{"url":"https://linkedin.com/company/vectara"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"n0ZCvzD5QKqR5J9hMuD2xQMakBHrI0Unw6PU+ys/uJM=","text":"your day to day For every submission Airbyte will donate 5 to a charity of your choice between The Visiola Foundation or The Ocean Cleanup Take 5 minutes do some good and let your voice be heard https lnkd in gAhivXNt Software Development Santa Clara California Software Development Cupertino CA Software Development New York NY Software Development San Francisco CA Computer and Network Security San Jose CA Software Development Mountain View CA Venture Capital and Private Equity Principals Software Development Software Development Toronto Ontario Research Services London England Last Round US 28 5M Investors Agree Join LinkedIn By clicking Continue you agree to LinkedIn s User Agreement Privacy Policy and Cookie Policy   or By clicking Continue you agree to LinkedIn s User Agreement Privacy Policy and Cookie Policy New to LinkedIn Join now or By clicking Continue you agree to LinkedIn s User Agreement Privacy Policy and Cookie Policy New to LinkedIn Join now","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"ce2d8e26-57a7-4ab1-83d8-aa76fda12701":{"id_":"ce2d8e26-57a7-4ab1-83d8-aa76fda12701","metadata":{"url":"https://docs.vectara.com/#__docusaurus_skipToContent_fallback"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"626692c2-1206-44d0-89ea-2de0146be0b0","metadata":{"url":"https://docs.vectara.com/#__docusaurus_skipToContent_fallback"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"DZH5emwjasNpHjTTxYP+mEL5TCjeO6nyWDmg67fNlWA=","text":"Vectara Docs  Powerful Semantic SearchEasy to UseDesigned by Experts  Developer documentation for Vectara s Semantic Search Platform Our semantic search based on the latest Neural IR research returns results that keyword search often misses Semantic search is not just about finding data but about understanding data and helping you answer questions about your data Our intuitive cloud based API makes it easy to index and query your textual data making it easy to create generative AI advanced applications like our AskNews demo quickly Dive into our API Playground to experiment with Vectara s REST APIs directly from your browser We are experts in language understanding and machine learning with over twenty five years of industry experience Our cutting edge solutions are built on this extensive expertise to deliver optimal performance and reliability","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"12f40804-9e02-4ac3-8223-0d0b953310eb":{"id_":"12f40804-9e02-4ac3-8223-0d0b953310eb","metadata":{"url":"https://docs.vectara.com//"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b2139a3b-124f-44cd-bf44-6382bde34e4e","metadata":{"url":"https://docs.vectara.com//"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"K8GcxQdsKRpOIojeyjMgfrzoqqTZgeTnJAW8yH4w7PE=","text":"Vectara Docs  Powerful Semantic SearchEasy to UseDesigned by Experts  Developer documentation for Vectara s Semantic Search Platform Our semantic search based on the latest Neural IR research returns results that keyword search often misses Semantic search is not just about finding data but about understanding data and helping you answer questions about your data Our intuitive cloud based API makes it easy to index and query your textual data making it easy to create generative AI advanced applications like our AskNews demo quickly Dive into our API Playground to experiment with Vectara s REST APIs directly from your browser We are experts in language understanding and machine learning with over twenty five years of industry experience Our cutting edge solutions are built on this extensive expertise to deliver optimal performance and reliability","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"3ab2ff9a-5638-4b1e-a8b0-5801afe28aa0":{"id_":"3ab2ff9a-5638-4b1e-a8b0-5801afe28aa0","metadata":{"url":"https://docs.vectara.com//docs/rest-api/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"8e80c7bb-ebbd-4952-8988-912d5940efa0","metadata":{"url":"https://docs.vectara.com//docs/rest-api/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"cdRGrUSDoyXWx4LTb+aKTM51IaORvbaoSrQ1tmT7fUU=","text":"Vectara API Introduction AdminService IndexService QueryService DocumentService ChatService   Play around with Vectara s REST APIs Vectara provides an end to end platform for creating GenAI products using 17 items 4 items 1 items 1 items 5 items","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"baf0aef5-baea-4140-9ea8-782929f3a20a":{"id_":"baf0aef5-baea-4140-9ea8-782929f3a20a","metadata":{"url":"https://docs.vectara.com/docs/rest-api/vectara-rest-api"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"78801986-5a14-45e6-bd84-1cf2ec6de956","metadata":{"url":"https://docs.vectara.com/docs/rest-api/vectara-rest-api"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"29/P2GmcTrgzNf2tp3r4sxsFedvBKKlerCZOtUm0Zvw=","text":"Vectara REST API Authentication ContactTerms of Service  Vectara provides an end to end platform for creating GenAI products usinga simple to use API You can sign up for an account andthen view several API Recipes with example queriesand parameter values The Vectara API Playground lets you experiment with REST endpoints fromyour browser Select an endpoint to view its definition including therequired or optional headers body responses and sample commands On theright side of each endpoint page like ReadCorpus you manuallyenter your API Key or OAuth Bearer Token customer_id and then anyrequired body parameters like the corpusID before sending the APIrequest Vectara has three kinds of API keys the Personal API Key Index API Keys and Query API Keys The Personal API Key enables administrative tasksincluding creating deleting and listing corpora and managing Index andQuery API keys for accessible corpora reading usage data updatingcorpora filters executing queries and indexing Query API Keys are usedfor read only querying operations while Index API Keys provide read andwrite access The OAuth operations authenticate with a Bearer Token via theOAuth 2 0 client credentials grant Review the OAuth 2 0 section abouthow to generate the token OAuth2 access to Vectara Token URL https vectara prod YOUR_VECTARA_CUSTOMER_ID auth us west 2 amazoncognito com oauth2 token","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"aa7db0b5-7597-4349-bb0f-b72ff85cf163":{"id_":"aa7db0b5-7597-4349-bb0f-b72ff85cf163","metadata":{"url":"https://docs.vectara.com//docs/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"f45f1ed9-a691-42f6-b258-9fa1e3d11c59","metadata":{"url":"https://docs.vectara.com//docs/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"oRU4HvNi9vAtqXS4njOOKsTeAc/31PsSedc40CajFn4=","text":"The Vectara Platform Welcome to the Answer Engine Developer focused API first Secure Solve the Hallucination Problem Language Agnostic Not sure where to start Ready to Dive In Check Out Our API Playground  Vectara is an end to end platform for product builders to embed powerful generative AI capabilities into applications with extraordinary results Vectara offers significant improvements over traditional searches by understanding the context and meaning of your data This revolutionary technology enables Vectara to drive insights and provide more accurate responses to user queries assisting decision making processes User data remains secure because Vectaranever trains on customer data The Vectara team envisions a future where generative AI powers everyapplication to deliver contextually accurate responses and give the rightanswers and actions Vectara is built on a solid hybrid search coreto enable better generative outcomes Traditional search technologies focuson keywords which limit their ability to understand complex queries Vectaradeploys advanced zero shot models and conversational searchcapabilities to understand interpret and respond to user queries with remarkableprecision Vectara summarizes search results on complex queries while providing factualcitations from your data Vectara provides the best hybrid searchcore and superior language understanding for ingestion and retrieval Vectaracan become your answer engine Designed for developers with an API first approach Vectara isthe optimal choice to integrate generative AI search into yourapplications This complete end to end platform provides easy ingestion andsimple APIs The Vectara Generative AI platform enablesdevelopers with the flexibility to build a wide range of applications withpowerful search experiences The Vectara platform never trains on customer data which enablesbusinesses to embed generative AI capabilities without the risk of data orprivacy violations Vectara provides support for customer managedkeys encryption at rest and during transit client configurable dataretention and more If you re ready to dive into our APIs make your way to our API Playground This interactive environment allows you to experiment with Vectara s REST APIsdirectly from your browser Tailored for developers the API Playgroundoffers a hands on experience to understand and demonstrate our capabilities AI content generators often create hallucinations false informationoutside of the raw factual data they make stuff up These hallucinationslead to inaccurate and misleading responses Vectara addressesthis problem through Retrieval Augmented Generation RAG meaning it grounds the searchresults in the uploaded data By focusing on facts and reducinghallucinations Vectara enhances trust in AI powered decision making Use Vectara to search across multiple languages eliminating languagebarriers and enabling users to find what they need regardless of thelanguage they use This cross language approach provides a seamlesssearch experience for users around the world The best answer may bewritten in German but a user asked the question in Spanish The Vectara platform is more than just an AI product It isa pioneer in the realm of neural search leading the way to harness thepower of your data Vectara wants to transform the waydevelopers interact with data and unlock a world of insights at theirfingertips Welcome to the future of information interaction If you don t have a Vectara account yet register forone here Here are some other ideas","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"7188a5b6-4af0-4682-8b70-4b94447c4298":{"id_":"7188a5b6-4af0-4682-8b70-4b94447c4298","metadata":{"url":"https://docs.vectara.com//docs/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"ac0f5a1c-8f3b-45d6-8bb8-a3d51a7a5e13","metadata":{"url":"https://docs.vectara.com//docs/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"oRU4HvNi9vAtqXS4njOOKsTeAc/31PsSedc40CajFn4=","text":"The Vectara Platform Welcome to the Answer Engine Developer focused API first Secure Solve the Hallucination Problem Language Agnostic Not sure where to start Ready to Dive In Check Out Our API Playground  Vectara is an end to end platform for product builders to embed powerful generative AI capabilities into applications with extraordinary results Vectara offers significant improvements over traditional searches by understanding the context and meaning of your data This revolutionary technology enables Vectara to drive insights and provide more accurate responses to user queries assisting decision making processes User data remains secure because Vectaranever trains on customer data The Vectara team envisions a future where generative AI powers everyapplication to deliver contextually accurate responses and give the rightanswers and actions Vectara is built on a solid hybrid search coreto enable better generative outcomes Traditional search technologies focuson keywords which limit their ability to understand complex queries Vectaradeploys advanced zero shot models and conversational searchcapabilities to understand interpret and respond to user queries with remarkableprecision Vectara summarizes search results on complex queries while providing factualcitations from your data Vectara provides the best hybrid searchcore and superior language understanding for ingestion and retrieval Vectaracan become your answer engine Designed for developers with an API first approach Vectara isthe optimal choice to integrate generative AI search into yourapplications This complete end to end platform provides easy ingestion andsimple APIs The Vectara Generative AI platform enablesdevelopers with the flexibility to build a wide range of applications withpowerful search experiences The Vectara platform never trains on customer data which enablesbusinesses to embed generative AI capabilities without the risk of data orprivacy violations Vectara provides support for customer managedkeys encryption at rest and during transit client configurable dataretention and more If you re ready to dive into our APIs make your way to our API Playground This interactive environment allows you to experiment with Vectara s REST APIsdirectly from your browser Tailored for developers the API Playgroundoffers a hands on experience to understand and demonstrate our capabilities AI content generators often create hallucinations false informationoutside of the raw factual data they make stuff up These hallucinationslead to inaccurate and misleading responses Vectara addressesthis problem through Retrieval Augmented Generation RAG meaning it grounds the searchresults in the uploaded data By focusing on facts and reducinghallucinations Vectara enhances trust in AI powered decision making Use Vectara to search across multiple languages eliminating languagebarriers and enabling users to find what they need regardless of thelanguage they use This cross language approach provides a seamlesssearch experience for users around the world The best answer may bewritten in German but a user asked the question in Spanish The Vectara platform is more than just an AI product It isa pioneer in the realm of neural search leading the way to harness thepower of your data Vectara wants to transform the waydevelopers interact with data and unlock a world of insights at theirfingertips Welcome to the future of information interaction If you don t have a Vectara account yet register forone here Here are some other ideas","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"863460dc-16aa-4450-9d20-91bcc64077a9":{"id_":"863460dc-16aa-4450-9d20-91bcc64077a9","metadata":{"url":"https://docs.vectara.com//docs/api-reference/indexing-apis/indexing"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b352e411-6841-4f0c-bfd3-71e90c82793a","metadata":{"url":"https://docs.vectara.com//docs/api-reference/indexing-apis/indexing"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"3g2qzjBE6Viv1YRrt2H0hHjLZgGCDbxMEuhVSqbF+Rc=","text":"Standard Indexing API Definition Document Object Definition History REST Example gRPC Example Index Document Request and Response Section within a Document First inhabitants Spanish rule Standard Indexing REST Endpoint Index Request and Response Index Document Request and Response Document Format Section within a Document Custom Dimensions Use Cases Scale only  The first step in using Vectara is to index a set of relateddocuments or content into a corpus Indexing a document enables you to makedata available for search and retrieval more efficiently The StandardIndexing API is recommended for applications where documents already have aclear and consistent structure Our Standard Indexing capability transforms this structured data into aformat that enables the data to become easily searchable in just a fewseconds We also support a variety of data formats by allowing you to specifymultiple document attributes and metadata The request that adds data into a corpus provides essential information aboutthe document you want to index A document is a piece of coherent textualmatter This index request requires the following parameters The response includes a status message and a StorageQuota messageindicating how much quota was consumed An ALREADY_EXISTS status codeindicates how much quota would have been consumed The storage quota object returns the number of characters consumed and thenumber of metadata characters consumed The total quota consumed is simply thesum of both values A document object encapsulates the information about the data that you wantto index A document in Vectara is very flexible because it represent ashort tweet or book with thousands of pages This object has a document_idwhich must be unique among all the documents in the same corpus The documentmay optionally speciify a title description and metadata The core ofthe document is also structured in sections that can include uniqueidentifiers titles strings metadata and so on The custom_dims field provides default values for the correspondingsection fields should they fail to define them explicitly Most importantly section defines the actual textual matter A section represents an organizational subunit within a document Itsdefinition is recursive since a section can be composed of further sections The actual textual content which is at least a single sentence but might spanseveral paragraphs or more is stored in text Like a document it mayoptionally specify a title which semantically corresponds to a sectionheader or chapter title Sections provide flexibility and it s possible that a section specifies atitle but relegates the text to subsections For instance consider thefollowing simple document excerpt from Wikipedia Settled by successive waves of arrivals during at least the last 13 000years 41 California was one of the most culturally and linguistically diverseareas in pre Columbian North America Various estimates of the native populationrange from 100 000 to 300 000 42 The indigenous peoples of California includedmore than 70 distinct ethnic groups of Native Americans ranging from large settled populations living on the coast to groups in the interior Californiagroups also were diverse in their political organization with bands tribes villages and on the resource rich coasts large chiefdoms such as the Chumash Pomo and Salinan Trade intermarriage and military alliances fostered manysocial and economic relationships among the diverse groups The first Europeans to explore the California coast were the members of aSpanish sailing expedition led by Portuguese captain Juan Rodr guez Cabrillo they entered San Diego Bay on September 28 1542 and reached at least as farnorth as San Miguel Island Privateer and explorer Francis Drake exploredand claimed an undefined portion of the California coast in 1579 landing northof the future city of San Francisco The first Asians to set foot on whatwould be the United States occurred in 1587 when Filipino sailors arrived inSpanish ships at Morro Bay Sebasti n Vizca no explored andmapped the coast of California in 1602 for New Spain sailing as far north asCape Mendocino This could be represented as a top level section titled History and no text It would contain two sections First inhabitants and Spanish rule that bothspecify text The part metadata held in metadata_json is returned in search queryresults It can contain for example information that links the item to recordsin other systems Finally custom_dims allows you to specify additional factors that can beused at query time to control the ranking of results The dimensions must bedefined ahead of time for the corpus or else they ll be ignored The request body provides essential information about the document you want toindex The Index request requires the following parameters The response from the server includes a status code and the amount of quotaconsumed You can find the full Standard Indexing gRPC definition at indexing proto The reply does not block The information in the request is not necessarilyavailable in the index when the RPC returns In most cases it becomesavailable within a second Here is an example response Custom dimensions are a powerful Vectara capability Customdimensions enable you to attach numeric factors to every item in the index which affect its final ranking during searches Some example use cases include Define the authoritativeness of the content For example content with 100upvotes can be ranked higher than content with no upvotes and 10 downvotes Indicate the source of the content If there are N sources this is usually done by defining N customdimensions and treating them as boolean 0 1 fields This allows weighting results based on source or even excluding certainsources altogether For example content from a government FAQ would be ratedhigher than content from a user forum Define the geography in which content is relevant Indicate the publication date which makes it easy to weight more recentresults higher For more information on how to use custom dimensions refer to theCustom Dimensions Usage Documentation","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"c5dcec38-08ed-4443-ad9a-230bc629f008":{"id_":"c5dcec38-08ed-4443-ad9a-230bc629f008","metadata":{"url":"https://readme.fireworks.ai/page/application-status"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"973d54d0-0b1c-49ca-88e9-453fb76db7f5","metadata":{"url":"https://readme.fireworks.ai/page/application-status"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"5ir4E5M2r87yNAXK2u/mEeaA3FnzKZw718H3O0/Nubw=","text":"Application Status    The application is currently running smoothly and all systems are operational If you experience any issues or have any questions please contact us at email protected or open a bug report on Discord","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"1ae14d8f-161a-4e2e-88cb-df0a2cb7d166":{"id_":"1ae14d8f-161a-4e2e-88cb-df0a2cb7d166","metadata":{"url":"https://readme.fireworks.ai/docs"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b3cb8b37-8d47-4214-9ed3-db60cac7c22a","metadata":{"url":"https://readme.fireworks.ai/docs"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"7XOb47Yq04DTw9Yuuq4FDddF7cuf0XI+nZTIpvUbxV0=","text":"Quickstart GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks ai is a lightning fast inference platform that helps you serve generative AI models Check out the following guides to see what else you can do with Fireworks Have fun If you have any questions please reach out to us on Discord or Twitter Updated 18 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"143c458e-3219-4bab-a4a1-9400f5f0a5d5":{"id_":"143c458e-3219-4bab-a4a1-9400f5f0a5d5","metadata":{"url":"https://fireworksai.readme.io/docs/fine-tuning"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"9d4c8dbf-e301-45f4-b4f6-f5f770e36f5d","metadata":{"url":"https://fireworksai.readme.io/docs/fine-tuning"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"RRV/V3a54HzQ2SGUBEr6XugfGwiXF3PxGOjLkOrE62U=","text":"LLM recipes GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationTuningGenerationModel evaluationSetting up your environment   We provide a collection of recipes that simplify the process of model fine tuning and evaluation Individual recipes and workflows combining them into end to end model development flows are available in the Fireworks Cookbook repository Each recipe comes with dedicated documentation providing more in depth information Recipes are categorized according to their respective use cases Here is the list of currently supported use cases See Docker container for LLM development Updated 4 months ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"f597c57c-08e0-4ca5-918a-6d6130cafc81":{"id_":"f597c57c-08e0-4ca5-918a-6d6130cafc81","metadata":{"url":"https://readme.fireworks.ai/docs"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"8a51de70-c04f-492e-9fb7-8296dc8c0cd2","metadata":{"url":"https://readme.fireworks.ai/docs"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"7XOb47Yq04DTw9Yuuq4FDddF7cuf0XI+nZTIpvUbxV0=","text":"Quickstart GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks ai is a lightning fast inference platform that helps you serve generative AI models Check out the following guides to see what else you can do with Fireworks Have fun If you have any questions please reach out to us on Discord or Twitter Updated 18 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"176ebd24-afe0-448a-95e3-62744b294dae":{"id_":"176ebd24-afe0-448a-95e3-62744b294dae","metadata":{"url":"https://readme.com?ref_src=hub&project=fireworksai"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"f9513578-1cb4-47d3-ab51-34260fef1127","metadata":{"url":"https://readme.com?ref_src=hub&project=fireworksai"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"3zwXcHI6MmX0JPkWb7j7Gfc3gfZwtI367kwqdpLCjew=","text":"Developer hubs that meet your users where they are Behind every API call is an API user So why do your docs treat them all the same ReadMe transforms static API documentation into real time interactive developer hubs More featuresA better experience for your developers at every stepEverything you need to build the best developer hubsDocumentationMetricsCommunityEnterpriseReady for a developer hub developers love Faster Time to First Call when just getting started Devs can explore what s possible in your guides then dive right into the Try It playground to make their first call in a snap Get on the same page when things go wrong Shareable API request logs make it easy for everyone to understand the issue and quickly troubleshoot Make the right updates with data driven insights Understand API usage and how that s changing over time so your team can focus on the highest impact improvements Keep users in the loop on the latest and greatest Help users take advantage of the latest updates and prevent breaking changes before they happen  ReadMe transforms your API docs into interactive hubs that help developers succeed Trusted by more than 3000 leading developer experience teams Easy to edit guides to help developers get started Real time API logs for troubleshooting and insights Changelog and more to keep users in the loop Manage multiple APIs in one hub and unlock advanced permissions with ReadMe Enterprise Explore Enterprise Join the 3000 companies putting developer experience first Sign up for a free 14 day trial of the full platform Your API Reference is free forever","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"9848cb7a-9dde-4389-8b9d-db1c516ac456":{"id_":"9848cb7a-9dde-4389-8b9d-db1c516ac456","metadata":{"url":"https://fireworksai.readme.io/docs/model-upload"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"12e1f3f0-c273-443b-8983-8132a843bb43","metadata":{"url":"https://fireworksai.readme.io/docs/model-upload"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"yw6WDXKXU9bYHWZRr8W2n9BrDUmqUlkUtVTtCHx/v04=","text":"Deploying fine tuned modelsInstalling firectlSigning inUploading a fine tuned modelDeploying your modelTesting your modelUsing the APICleaning upDeployment limits GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   This guide will walk you through uploading your own fine tuned model The firectl command line interface CLI will be used to manage your LLMmodels Run the following command to sign into Fireworks Confirm that you have successfully signed in by listing your account You should see your account ID Make sure to review the requirements for a fine tuned model Sample configs for supported models are available here To upload a fine tuned model located at tmp falcon 7b addon run Once uploaded you can see your model with To deploy the model for inference run Once your model is deployed you can query it on the model page You should see your model s response streamed below You can also directly query the model using the v1 completions API Now that you are finished with the guide you can undeploy the models to avoid accruing charges on your account You can also delete the model from your account Non enterprise accounts are limited to a maximum of 100 deployed models Updated 28 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"69323f5d-0867-43bc-b8c5-21c0e2deac35":{"id_":"69323f5d-0867-43bc-b8c5-21c0e2deac35","metadata":{"url":"https://readme.fireworks.ai/docs"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"6fca0e9b-574a-40b9-9260-90c99bc42223","metadata":{"url":"https://readme.fireworks.ai/docs"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"7XOb47Yq04DTw9Yuuq4FDddF7cuf0XI+nZTIpvUbxV0=","text":"Quickstart GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks ai is a lightning fast inference platform that helps you serve generative AI models Check out the following guides to see what else you can do with Fireworks Have fun If you have any questions please reach out to us on Discord or Twitter Updated 18 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"fc7ba94e-0d4e-4179-ba0e-0887565b8eec":{"id_":"fc7ba94e-0d4e-4179-ba0e-0887565b8eec","metadata":{"url":"https://readme.fireworks.ai/docs"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b1acb4b5-ff78-45ec-972f-ad23869ab5b9","metadata":{"url":"https://readme.fireworks.ai/docs"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"7XOb47Yq04DTw9Yuuq4FDddF7cuf0XI+nZTIpvUbxV0=","text":"Quickstart GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks ai is a lightning fast inference platform that helps you serve generative AI models Check out the following guides to see what else you can do with Fireworks Have fun If you have any questions please reach out to us on Discord or Twitter Updated 18 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"ac30b56b-53e5-4e7f-a557-2b69af1e0b78":{"id_":"ac30b56b-53e5-4e7f-a557-2b69af1e0b78","metadata":{"url":"https://readme.fireworks.ai#content"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"602f0ea9-450f-42b4-954c-688cec13f879","metadata":{"url":"https://readme.fireworks.ai#content"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"ntvT2cKf5d/wdB9waFqJTy0J0Gq/vpyN4aBu/9W5fpE=","text":"Quickstart GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks ai is a lightning fast inference platform that helps you serve generative AI models Check out the following guides to see what else you can do with Fireworks Have fun If you have any questions please reach out to us on Discord or Twitter Updated 18 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"2aa00229-d870-471b-aaee-db52618fd5f5":{"id_":"2aa00229-d870-471b-aaee-db52618fd5f5","metadata":{"url":"https://readme.fireworks.ai/docs/quickstart"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"dfd24d05-5e1e-4786-8ca0-cc9fe243b7c5","metadata":{"url":"https://readme.fireworks.ai/docs/quickstart"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"uda9y+XLKgQnkqqYs+3D5vjkCoOdIrUgJ/UfXEbKW7M=","text":"Quickstart GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks ai is a lightning fast inference platform that helps you serve generative AI models Check out the following guides to see what else you can do with Fireworks Have fun If you have any questions please reach out to us on Discord or Twitter Updated 18 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"a33559d1-f885-4b49-ac94-ff9322c776c8":{"id_":"a33559d1-f885-4b49-ac94-ff9322c776c8","metadata":{"url":"https://readme.fireworks.ai/docs"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"c2a28405-3b7f-4b93-b4aa-f0439afe2916","metadata":{"url":"https://readme.fireworks.ai/docs"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"7XOb47Yq04DTw9Yuuq4FDddF7cuf0XI+nZTIpvUbxV0=","text":"Quickstart GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks ai is a lightning fast inference platform that helps you serve generative AI models Check out the following guides to see what else you can do with Fireworks Have fun If you have any questions please reach out to us on Discord or Twitter Updated 18 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"50260da7-a7f1-4c1b-ae5f-e5bec3c58587":{"id_":"50260da7-a7f1-4c1b-ae5f-e5bec3c58587","metadata":{"url":"https://readme.fireworks.ai/docs/custom-base-models"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"8917e377-5da6-4baa-b076-c08c01cddf9f","metadata":{"url":"https://readme.fireworks.ai/docs/custom-base-models"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"hbO50Ux2Xu0QW93/rDmsN1d784w/9enzcoFHLiWtsjs=","text":"Custom base modelsUploading from your local computerFireworks json file for custom base modelsUploading from other locations GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   To upload a custom base model from your local computer you need to create the fireworks json file for Fireworks to understand your model and then run the following command Fireworks container requires a few bits of metadata to be added to the checkpoint These need to be stored in fireworks json file in the model checkpoint directory next to config json Define a JSON file with the following fields If your model is prohibitively large or otherwise not conveniently accessible from your local computer please contact your Fireworks representative for alternative upload methods Updated 4 months ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"b4be2a22-0e13-4cdd-8226-5cb1c08f3e6f":{"id_":"b4be2a22-0e13-4cdd-8226-5cb1c08f3e6f","metadata":{"url":"https://readme.fireworks.ai/docs/publishing-models"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"db7b4c33-0ef8-4813-b1d1-09d1e360016a","metadata":{"url":"https://readme.fireworks.ai/docs/publishing-models"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"lfmbPKOu4lhNuq+ZrXtmuO6rf8sE2w9nK861ylCkhNw=","text":"Publishing modelsMaking your models publicMaking your models private GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   By default a model you create and deploy can only be queried by an API key associate with your account You can also designate your model as public which allows it to be queried using any API key To let your model be publicly accessible pass the public flag when creating the model You can also update an existing model to be public NOTE The model must be undeployed before changing your models publicity New models are created private by default If you want to turn a public model back to private run Updated 4 months ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"8574a785-8024-4aa6-8f9f-8cb8de8b7728":{"id_":"8574a785-8024-4aa6-8f9f-8cb8de8b7728","metadata":{"url":"https://readme.fireworks.ai/docs/private-deployments"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"a65df53a-841a-45b2-b9ff-fd3508ebeea0","metadata":{"url":"https://readme.fireworks.ai/docs/private-deployments"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"J5isge2Im44slOeEkrbHZoi3xP9q1tEAZzFORVEnbMo=","text":"Private deployments GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationCreating a private deploymentDeploying modelsDeleting a private deployment Querying the model  Important Before creating a private deployment work with your Fireworks representative to confirm your capacity needs To create a private deployment run The second argument is the fully qualified name of the base model used in the deployment You may use any publicly available base model which includes those in the fireworks account or your own custom base model Make note of the deployment ID the suffix after accounts deployments You will need this ID to deploy models to this deployment NOTE A base model can have multiple deployments You must specify which deployment is used when querying a custom base model by explicitly deploying it to a specific deployment ID Once the deployment is created you can now deploy models to it Any PEFT addons you deploy must use the base model specified when you created the deployment You may also deploy the base model if it s not already deployed elsewhere to the deployment Querying a model in a private deployment is identical to using a public deployment Simply specify the model name in your API request See Programmatic access for details To delete a deployment run WARNING This will automatically undeploy all models deployed to this deployment Updated 4 months ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"679336e9-5c9e-45bf-a320-ff3d49b02cca":{"id_":"679336e9-5c9e-45bf-a320-ff3d49b02cca","metadata":{"url":"https://readme.fireworks.ai/docs/signing-in"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"bd420889-a24c-48b6-b926-c50abbea90b2","metadata":{"url":"https://readme.fireworks.ai/docs/signing-in"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"zmJmV6eBa2CpZbwW8gwFhOLrOGgek6jxnKVJNUwpjUw=","text":"Authentication GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   To sign into your enterprise account run This will prompt you to sign in with your organization s SSO Updated 4 months ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"dca51ac5-776c-448e-a385-a0de94278038":{"id_":"dca51ac5-776c-448e-a385-a0de94278038","metadata":{"url":"https://readme.fireworks.ai/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"97ab240a-43e6-40ea-93f2-81aa7b7ddf3c","metadata":{"url":"https://readme.fireworks.ai/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"uTIy/oaJCYHr42LBTFBdvQ0fC6GuXWos8EtkuL5IYg0=","text":"Quickstart GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks ai is a lightning fast inference platform that helps you serve generative AI models Check out the following guides to see what else you can do with Fireworks Have fun If you have any questions please reach out to us on Discord or Twitter Updated 18 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"fc76ec82-fe11-41f3-a580-cd7fca7fc68e":{"id_":"fc76ec82-fe11-41f3-a580-cd7fca7fc68e","metadata":{"url":"https://readme.fireworks.ai/"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"38225612-53c3-4900-9fba-ef79662a0a26","metadata":{"url":"https://readme.fireworks.ai/"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"uTIy/oaJCYHr42LBTFBdvQ0fC6GuXWos8EtkuL5IYg0=","text":"Quickstart GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks ai is a lightning fast inference platform that helps you serve generative AI models Check out the following guides to see what else you can do with Fireworks Have fun If you have any questions please reach out to us on Discord or Twitter Updated 18 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"9947f08f-b3da-412d-9a96-636343301cb9":{"id_":"9947f08f-b3da-412d-9a96-636343301cb9","metadata":{"url":"https://readme.fireworks.ai/reference/image_generationaccountsfireworksmodelsstable-diffusion-xl-1024-v1-0"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"dcdc4d6b-f368-4ab6-87db-a8f76b317a05","metadata":{"url":"https://readme.fireworks.ai/reference/image_generationaccountsfireworksmodelsstable-diffusion-xl-1024-v1-0"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"cW81d28xfKZziqFRfCGZKBnZjy/kcJJvkPzQ2fTAIyc=","text":"image_generation accounts fireworks models stable diffusion xl 1024 v1 0 Fireworks REST APIPython Client LibraryPeFT AddonFirectl CLI ReferenceFireworks REST APIPython Client LibraryPeFT AddonFirectl CLI Reference   Official API reference for image generation workloads can be found on the corresponding models pages under the API tab","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"5a051be1-adbc-4fc4-a382-49f67c7bd017":{"id_":"5a051be1-adbc-4fc4-a382-49f67c7bd017","metadata":{"url":"https://readme.fireworks.ai/docs/structured-output-grammar-based"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"8c7ddcfc-86d7-4e95-a885-a73a888d6513","metadata":{"url":"https://readme.fireworks.ai/docs/structured-output-grammar-based"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"YIIpUVK0mKmy5yJyV9U6A4sd9LYPv1r6HunLj7UNSZw=","text":"Grammar modeWhat is grammar based structured output Why grammar based structured output End to end examplesAdvanced examplesSyntax GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationJapanese and ChineseC code generationBackgroundBasicsNon terminals and terminalsCharacters and character rangesSequences and alternativesRepetition and optional symbolsComments and newlinesThe root rule PrerequisitesStep 1 Configure the Fireworks ai clientStep 2 Define the output grammarStep 3 Specify your output grammar in your chat completions request  Grammar mode is the ability to specify a forced output schema for any Fireworks model via an extended BNF formal grammar GBNF format This method is popularly used to constrain model outputs in llama cpp What is a formal grammar It s a way to define rules to declare strings to be valid or invalid See the Syntax for Describing Grammars below for more info Similar to our JSON mode format you provide response_format field in the request like type grammar grammar For best results we still recommend that you do some prompt engineering and describe the desired output to the model to guide decision making This guide provides a step by step example of creating a structured output response with grammar using the Fireworks ai API The example uses Python and the OpenAI library to define the schema for the output Before you begin ensure you have the following Python installed on your system openai libraries installed You can install them using pip Next select the model you want to use In this example we use mixtral 8x7b instruct but all fireworks models support this feature You can find your favorite model and get structured response out of it You can use either Fireworks ai or OpenAI SDK with this feature Using OpenAI SDK with your API key and the base URL Replace Your_API_Key with your actual API key Define a grammar to restrict the specified output Let s say you have a model that is a classifier and classifies patient request into a few predefined classes Then you can ask the model to only respond within these classes and for the response we will only get one of the 5 classes we specified in this case the model output is Note that we still have done some prompt engineering to instruct the model about possible diagnoses in free form Alternatively we may have used one of the fine tuned models for the medical domain Make a request to the Fireworks ai API to get a structured response In your request specify the output schema you used in step 3 For example we are pretending The model will reply in Japanese And since the grammar is actually more lenient than Japanese and covers Chinese as well we can also just prompt the model to be a fluent Chinese speaker And you can see here that we are trying something a little difficult asking a Japanese tour guide to speak Chinese But with the help from the grammar the model replied in Chinese with the same grammar specified Without the help from the grammar here is the model reply in a mix of Chinese and English This is one of the community contribution on llama cpp You can hook that with our Mixtral model and try to come up with a good solution for a coding problem you have In this case we get a cute little valid C program as the output Bakus Naur Form BNF is a notation for describing the syntax of formal languages like programming languages file formats and protocols Fireworks API uses an extension of BNF with a few modern regex like features inspired by Llama cpp s implementation In BNF we define production rules that specify how a non terminal rule name can be replaced with sequences of terminals characters specifically Unicode code points and other non terminals The basic format of a production rule is nonterminal sequence Consider an example of a small chess notation grammar Non terminal symbols rule names stand for a pattern of terminals and other non terminals They are required to be a dashed lowercase word like move castle or check mate Terminals are actual characters code points They can be specified as a sequence like 1 or O O or as ranges like 1 9 or NBKQR Terminals support the full range of Unicode Unicode characters can be specified directly in the grammar for example hiragana or with escapes 8 bit xXX 16 bit uXXXX or 32 bit UXXXXXXXX Character ranges can be negated with Dot symbol matches any character The order of symbols in a sequence matter For example in 1 move move n the 1 must come before the first move etc Alternatives denoted by give different sequences that are acceptable For example in move pawn nonpawn castle move can be a pawn move a nonpawn move or a castle Parentheses can be used to group sequences which allows for embedding alternatives in a larger rule or applying repetition and optional symbols below to a sequence Comments can be specified with Newlines are allowed between rules and between symbols or sequences nested inside parentheses Additionally a newline after an alternate marker will continue the current rule even outside of parentheses In a full grammar the root rule always defines the starting point of the grammar In other words it specifies what the entire output must match Updated 3 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"388bb201-6f75-4007-9916-3144b9fff928":{"id_":"388bb201-6f75-4007-9916-3144b9fff928","metadata":{"url":"https://readme.fireworks.ai/docs/quickstart"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"645a2d07-8cb9-4b5e-8598-7171e8a6d13b","metadata":{"url":"https://readme.fireworks.ai/docs/quickstart"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"uda9y+XLKgQnkqqYs+3D5vjkCoOdIrUgJ/UfXEbKW7M=","text":"Quickstart GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks ai is a lightning fast inference platform that helps you serve generative AI models Check out the following guides to see what else you can do with Fireworks Have fun If you have any questions please reach out to us on Discord or Twitter Updated 18 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"34af5a46-eb28-468e-aea8-8162a7590301":{"id_":"34af5a46-eb28-468e-aea8-8162a7590301","metadata":{"url":"https://readme.fireworks.ai/docs/querying-text-models"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"e2b67160-2db1-48a2-a5be-63ce8f54dc23","metadata":{"url":"https://readme.fireworks.ai/docs/querying-text-models"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"WtcNnmYWQJWKuZ0VtyBZS4pSwLSrS2BZ+Ka4+RBAk6g=","text":"Querying text modelsUsing the web consoleUsing the APIAdvanced optionsAppendix GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationCompletions APIChat Completions APIGetting usage infoStreamingAsync modeSampling optionsDebugging optionsTokenization Overriding the system promptMultiple choicesTemperatureTop pTop kRepetition penaltyLogprobsEcho  Fireworks ai offers an OpenAI compatible REST API for querying text models There are several ways to interact with it All Fireworks models can be accessed through the web console at https app fireworks ai Clicking on a model will take you to the playground where you can enter a prompt along with additional request parameters Non chat models will use the completions API which passes your input directly into the model Models with a conversation config are considered chat models also known as instruct models By default chat models will use the chat completions API which will automatically format your input with the conversation style of the model Advanced users can revert back to the completions API by unchecking the Use chat template option Text models generate text based on the provided input prompt All text models support this basic completions API Using this API the model will successively generate new tokens until either the maximum number of output tokens has been reached or if the model s special end of sequence EOS token has been generated NOTE Llama family models will automatically prepend the beginning of sequence BOS token to your prompt input This is to be consistent with the original implementation Here are some examples of calling the completions API Models with a conversation config have the chat completions API enabled These models are typically tuned with a specific conversation styles for which they perform best For example Llama chat models use the following template INST system_prompt user_message_1 INST Some templates like llama chat can support multiple chat messages as well In general we recommend users use the chat completions API whenever possible to avoid common prompt formatting errors Even small errors like misplaced whitespace may result in poor model performance Here are some examples of calling the chat completions API A conversation style may include a default system prompt For example the llama chat style uses the default Llama prompt You are a helpful respectful and honest assistant Always answer as helpfully as possible while being safe Your answers should not include any harmful unethical racist sexist toxic dangerous or illegal content Please ensure that your responses are socially unbiased and positive in nature For styles that support a system prompt you may override this prompt by setting the first message with role system For example To completely omit the system prompt you can set content to the empty string The process of generating a conversation formatted prompt will depend on the conversation style used To verify the exact prompt used turn on echo The returned object will contain a usage field containing See the API reference for the completions and chat completions APIs for a detailed description of these options By default results are returned to the client once the generation is finished Another option is to stream the results back which is useful for chat use cases where the client can incrementally see results as each token is generated Here is an example with the completions API and one with the chat completions API The Python client library also supports asynchronous mode for both completion and chat completion The API auto regressively generates text based on choosing the next token using the probability distribution over the space of tokens By default the API will return a single generation choice per request You can create multiple generations by setting the n parameter to the number of desired choices The returned choices array will contain the result of each generation Temperature allows you to configure how much randomness you want in the generated text A higher temperature leads to more creative results On the other hand setting a temperature of 0 will allow you generate deterministic results which is useful for testing and debugging Top p also called nucleus sampling is an alternative to sampling with temperature where the model considers the results of the tokens with top_p probability mass So 0 1 means only the tokens comprising the top 10 probability mass are considered Top k is another sampling method where the k most probable tokens are filtered and the probability mass is redistributed among tokens LLMs are sometimes prone to repeat a single character or a sentence Using a frequency and presence penalty can reduce the likelihood of sampling repetitive sequences of tokens They work by directly modifying the model s logits un normalized log probabilities with an additive contribution logits j c j frequency_penalty c j 0 1 0 presence_penalty where Setting the logprobs parameter will return the log probabilities of the logprobs 1 most likely tokens the chosen token plus logprobs additional tokens The log probabilities will be returned in a LogProbs object for each choice When used in conjunction with echo this option can be set to see how the model tokenized your input Setting the echo option to true will cause the API to return the prompt along with the generated text This can be used in conjunction with the chat completions API to verify the prompt template used It can also be used in conjunction with logprobs to see how the model tokenized your input Language models read and write text in chunks called tokens In English a token can be as short as one character or as long as one word e g a or apple and in some languages tokens can be even shorter than one character or even longer than one word Different model families use different tokenizers The same text might be translated to different numbers of tokens depending on the model It means that generation cost may vary per model even if the model size is the same For the Llama model family you can use this tool to estimate token counts The actual number of tokens used in prompt and generation is returned in the usage field of the API response Updated 6 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"17df9856-d5c0-4528-9c89-98164571cab1":{"id_":"17df9856-d5c0-4528-9c89-98164571cab1","metadata":{"url":"https://readme.fireworks.ai/docs/adding-users"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"dfcaca91-cb29-4b24-8406-51bf76a7547d","metadata":{"url":"https://readme.fireworks.ai/docs/adding-users"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"Cwxiss4QPSZQHhPWy3pbrtAb0RKV+KXcx6XmyQAox8k=","text":"Adding users GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   To add a new user to your enterprise Fireworks account run Updated 4 months ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"5ac9d64e-f96a-4c4b-ae23-00ff89c1de0d":{"id_":"5ac9d64e-f96a-4c4b-ae23-00ff89c1de0d","metadata":{"url":"https://readme.fireworks.ai/page/pricing"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"2436f4c4-1027-4f37-9558-38addfe2bf01","metadata":{"url":"https://readme.fireworks.ai/page/pricing"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"1/gaggumtnSxR1XbapX5lYBpyHqyeKdydV6uhnwHT8c=","text":"Pricing Developer pricingDeveloper PROEnterprise pricing Text language chat code modelsImage modelsMulti modal modelsEmbeddingsFine tuning jobs  Fireworks is currently available in three tiers Per token pricing is applied only for non dedicated deployments Contact us for dedicated deployment pricing options Input tokens are determined from the prompt you supply in the request Output tokens are the completions generated by the model See Tokens section for more details For image generation models like SDXL we charge based on the number of inference steps denoising iterations For multi modal models like LLaVA each image is treated as 576 prompt tokens The pricing is otherwise identical to text models We will not be charging for embedding models until March Here is our pricing plan when it kicks in The Fireworks fine tuning service is currently in an experimental alpha stage so usage is 100 free Your account will be automatically enrolled in the Developer PRO tier by adding a valid payment method Developer PRO tier is currently prepaid only You pay in advance to gain credits that can be used anywhere on the Fireworks platform Credits will be deducted as you use our services An automatic top up option will be available soon Please contact us at email protected for a custom quote","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"89056845-b06e-41e0-a5f4-1d986daa8eee":{"id_":"89056845-b06e-41e0-a5f4-1d986daa8eee","metadata":{"url":"https://readme.fireworks.ai/edit/quickstart"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"f7a1f857-16a0-475c-8875-d0bcb15e4b89","metadata":{"url":"https://readme.fireworks.ai/edit/quickstart"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"LfGa4/QjcmmN2ghz1fwP+pFw7X/WYgaApLyZv3IWTQU=","text":"GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Visit app fireworks ai Click the Sign In button in the top navigation bar Click Continue with Google and authenticate with your Google account A new Fireworks developer account will be provisioned for you the first time you sign in Next we ll provision a new API key Click on API Keys in the left navigation bar Click on New API Key and give your new API key a name Now we re ready to use a model Click on View Models in the left navigation bar Click on any of the available models we ll use Llama 2 70B Chat Enter in a prompt and click Generate Completion Check out the following guides to see what else you can do with Fireworks Fine tuning your own model Deploying a fine tuned model Calling a model programatically Calling Fireworks from LangChain API reference for image generation models Have fun If you have any questions please reach out to us on Discord or Twitter","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"2c438ff7-ea37-49bc-8b85-137171143b65":{"id_":"2c438ff7-ea37-49bc-8b85-137171143b65","metadata":{"url":"https://readme.fireworks.ai/page/pricing"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"e8739b92-2a28-486e-8c28-c383e3b3d8c4","metadata":{"url":"https://readme.fireworks.ai/page/pricing"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"1/gaggumtnSxR1XbapX5lYBpyHqyeKdydV6uhnwHT8c=","text":"Pricing Developer pricingDeveloper PROEnterprise pricing Text language chat code modelsImage modelsMulti modal modelsEmbeddingsFine tuning jobs  Fireworks is currently available in three tiers Per token pricing is applied only for non dedicated deployments Contact us for dedicated deployment pricing options Input tokens are determined from the prompt you supply in the request Output tokens are the completions generated by the model See Tokens section for more details For image generation models like SDXL we charge based on the number of inference steps denoising iterations For multi modal models like LLaVA each image is treated as 576 prompt tokens The pricing is otherwise identical to text models We will not be charging for embedding models until March Here is our pricing plan when it kicks in The Fireworks fine tuning service is currently in an experimental alpha stage so usage is 100 free Your account will be automatically enrolled in the Developer PRO tier by adding a valid payment method Developer PRO tier is currently prepaid only You pay in advance to gain credits that can be used anywhere on the Fireworks platform Credits will be deducted as you use our services An automatic top up option will be available soon Please contact us at email protected for a custom quote","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"6edf880f-b794-418e-822e-d2f9d65f3ea8":{"id_":"6edf880f-b794-418e-822e-d2f9d65f3ea8","metadata":{"url":"https://readme.fireworks.ai/docs/querying-embeddings-models"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"5f52b930-8ec4-41b9-92fa-b0ed02fab0ee","metadata":{"url":"https://readme.fireworks.ai/docs/querying-embeddings-models"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"//MJ7H8rk5JhEqTZYyTzhywRZdpaJhk/6XJ2pT7Lrqo=","text":"Querying embedding modelsEmbedding documentsQuerying documentsList of available models GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks hosts one embedding model the E5 Mistral model Specifically we host the intfloat e5 mistral 7b instruct variant a highly adaptable language model that s currently the state of the art model on the Hugging Face leaderboard It has 32 layers and an embedding size of 4096 making it well suited for complex embedding tasks Our embeddings service is OpenAI compatible Use OpenAI s embeddings guide and OpenAI s embeddings documentation for more detailed information on our embedding model usage The embedding model inputs text and outputs a vector list of floating point numbers to use for tasks like similarity comparisons and search This code embeds the text Spiderman was a particularly entertaining movie with and returns the following Unlike most embedding models the E5 Mistral model we use is unique because it allows users to give instructions with their query This enables users to use the same document store for multiple tasks with higher accuracy Lets say I previously used the embedding model to embed many movie reviews that I stored in a vector database I now want to create a movie recommender that takes in a user query and outputs recommendations based on this data The code below demonstrates how to embed the user query and system prompt To view this example end to end and see how to use a MongoDB vector store and Fireworks hosted generation model for RAG see our full guide Updated 4 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"fac28d77-95b0-46b9-aa75-005787270ebc":{"id_":"fac28d77-95b0-46b9-aa75-005787270ebc","metadata":{"url":"https://readme.fireworks.ai/docs/querying-vision-language-models"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"5c790809-01d8-48f4-a227-73e00a0ed486","metadata":{"url":"https://readme.fireworks.ai/docs/querying-vision-language-models"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"y/Gx2z40AOE1n/25AOGQx9zRTwXgvB65/4qFtNgdyfo=","text":"Querying vision language modelsUsing the web consoleUsing the APIAdvanced optionsManaging imagesCalculating costFAQ GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationChat Completions APICompletions APIAPI LimitationsModel LimitationsCan I fine tune the image capabilities with FireLlava Can FireLlava generate images What type of files can I upload Is there a limit to the size of the image I can upload What is the retention policy for the images I upload How do rate limits work with FireLlava Can FireLlava understand image metadata Overriding the system prompt  Please refer to https readme fireworks ai docs querying text models for accessing the web console Both completions API and chat completions API are supported However we would recommend users to stick to chat completions API for simplicity All vision language models should have a conversation config and have chat completions API enabled These models are typically tuned with a specific conversation styles for which they perform best For example FireLLaVA models use the following template SYSTEM system message USER user message ASSISTANT The substring is a special token that we insert into the prompt to allow the model to figure out where to put the image In general we recommend users use the chat completions API whenever possible to avoid common prompt formatting errors Even small errors like misplaced whitespace may result in poor model performance When used as a text only language model you can also just call API without the nested message content field see https readme fireworks ai docs querying text models for details Here are some examples of calling the chat completions API In the above example we are providing images by providing the URL to the images Alternatively you can also provide the string representation of the base64 encoding of the images prefixed with MIME types For example A conversation style may include a default system prompt For example the llava chat style uses the default Llava prompt A chat between a curious user and an artificial intelligence The assistant gives helpful detailed and polite answers to the user s questions For styles that support a system prompt you may override this prompt by setting the first message with role system For example To completely omit the system prompt you can set content to the empty string For more details please refer to the Overriding the system prompt section of https readme fireworks ai docs querying text models Advanced users can also query the completions API directly Users will need to manually insert the image token where appropriate and supply the list of images as an ordered list this is true for FireLLaVA model but may be subject to change for future vision language models For example Right now we impose certain limit on the completions API and chat completions API as follows At the moment FireLLaVA is the only VLM available and owing to the nature of the model and how it was trained model performance may degrade when there are multiple images in the same conversation Please refer to https readme fireworks ai docs querying text models for more advanced options and generation parameters The Chat Completions API is not stateful That means you have to manage the messages including images you pass to the model yourself However we try to cache the image download as much as we can to save latency on model download For long running conversations we suggest passing images via URL s instead of base64 The latency of the model can also be improved by downsizing your images ahead of time to be less than the maximum size they are expected them to be For FireLLaVA 13B each image is treated as 576 prompt tokens The pricing is otherwise identical to a 13B text models For more information please refer to our our pricing page here We currently do not charge separately for high resolution images and low resolution images they all cost 576 tokens Not right now but we are working on integrating FireLlava with fine tuning If you are interested please reach out to us via Discord No We have a list of models deployed on our platform that is StableDiffusion based Please give these models a try and let us know how it goes We currently support png jpg jpeg gif bmp tiff and ppm format images Currently our API is restricted to 10MB for the whole request so the image sent through request in base64 encoding will need to be smaller than 10MB when converted to base64 encoding If you are using URLs then each image need to be smaller than 5MB We do not persist the images longer than the server lifetime and will be deleted automatically FireLlava is rate limited like all of our other LLM models which depends on which tier of rate limiting you are at For more information please check out https readme fireworks ai page pricing No If you have image metadata that you want the model to understand please provide them through the prompt Updated about 1 month ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"119c1cff-dc17-403d-8090-f5fd7cf34423":{"id_":"119c1cff-dc17-403d-8090-f5fd7cf34423","metadata":{"url":"https://readme.fireworks.ai/docs/querying-text-models#using-the-api"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"49c496c9-e2c9-4133-a2fa-e0eaed054013","metadata":{"url":"https://readme.fireworks.ai/docs/querying-text-models#using-the-api"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"5DAZWMZIyCGze8frCVdpZYreQRkW0lrvdcHFTiSRXrY=","text":"Querying text modelsUsing the web consoleUsing the APIAdvanced optionsAppendix GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationCompletions APIChat Completions APIGetting usage infoStreamingAsync modeSampling optionsDebugging optionsTokenization Overriding the system promptMultiple choicesTemperatureTop pTop kRepetition penaltyLogprobsEcho  Fireworks ai offers an OpenAI compatible REST API for querying text models There are several ways to interact with it All Fireworks models can be accessed through the web console at https app fireworks ai Clicking on a model will take you to the playground where you can enter a prompt along with additional request parameters Non chat models will use the completions API which passes your input directly into the model Models with a conversation config are considered chat models also known as instruct models By default chat models will use the chat completions API which will automatically format your input with the conversation style of the model Advanced users can revert back to the completions API by unchecking the Use chat template option Text models generate text based on the provided input prompt All text models support this basic completions API Using this API the model will successively generate new tokens until either the maximum number of output tokens has been reached or if the model s special end of sequence EOS token has been generated NOTE Llama family models will automatically prepend the beginning of sequence BOS token to your prompt input This is to be consistent with the original implementation Here are some examples of calling the completions API Models with a conversation config have the chat completions API enabled These models are typically tuned with a specific conversation styles for which they perform best For example Llama chat models use the following template INST system_prompt user_message_1 INST Some templates like llama chat can support multiple chat messages as well In general we recommend users use the chat completions API whenever possible to avoid common prompt formatting errors Even small errors like misplaced whitespace may result in poor model performance Here are some examples of calling the chat completions API A conversation style may include a default system prompt For example the llama chat style uses the default Llama prompt You are a helpful respectful and honest assistant Always answer as helpfully as possible while being safe Your answers should not include any harmful unethical racist sexist toxic dangerous or illegal content Please ensure that your responses are socially unbiased and positive in nature For styles that support a system prompt you may override this prompt by setting the first message with role system For example To completely omit the system prompt you can set content to the empty string The process of generating a conversation formatted prompt will depend on the conversation style used To verify the exact prompt used turn on echo The returned object will contain a usage field containing See the API reference for the completions and chat completions APIs for a detailed description of these options By default results are returned to the client once the generation is finished Another option is to stream the results back which is useful for chat use cases where the client can incrementally see results as each token is generated Here is an example with the completions API and one with the chat completions API The Python client library also supports asynchronous mode for both completion and chat completion The API auto regressively generates text based on choosing the next token using the probability distribution over the space of tokens By default the API will return a single generation choice per request You can create multiple generations by setting the n parameter to the number of desired choices The returned choices array will contain the result of each generation Temperature allows you to configure how much randomness you want in the generated text A higher temperature leads to more creative results On the other hand setting a temperature of 0 will allow you generate deterministic results which is useful for testing and debugging Top p also called nucleus sampling is an alternative to sampling with temperature where the model considers the results of the tokens with top_p probability mass So 0 1 means only the tokens comprising the top 10 probability mass are considered Top k is another sampling method where the k most probable tokens are filtered and the probability mass is redistributed among tokens LLMs are sometimes prone to repeat a single character or a sentence Using a frequency and presence penalty can reduce the likelihood of sampling repetitive sequences of tokens They work by directly modifying the model s logits un normalized log probabilities with an additive contribution logits j c j frequency_penalty c j 0 1 0 presence_penalty where Setting the logprobs parameter will return the log probabilities of the logprobs 1 most likely tokens the chosen token plus logprobs additional tokens The log probabilities will be returned in a LogProbs object for each choice When used in conjunction with echo this option can be set to see how the model tokenized your input Setting the echo option to true will cause the API to return the prompt along with the generated text This can be used in conjunction with the chat completions API to verify the prompt template used It can also be used in conjunction with logprobs to see how the model tokenized your input Language models read and write text in chunks called tokens In English a token can be as short as one character or as long as one word e g a or apple and in some languages tokens can be even shorter than one character or even longer than one word Different model families use different tokenizers The same text might be translated to different numbers of tokens depending on the model It means that generation cost may vary per model even if the model size is the same For the Llama model family you can use this tool to estimate token counts The actual number of tokens used in prompt and generation is returned in the usage field of the API response Updated 6 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"b4eddfe2-ade0-4b5b-80fb-1d90d7f2f531":{"id_":"b4eddfe2-ade0-4b5b-80fb-1d90d7f2f531","metadata":{"url":"https://readme.fireworks.ai/docs/enabling-chat-completions"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b4152c4a-32d0-45a2-b607-cc92c5fe4241","metadata":{"url":"https://readme.fireworks.ai/docs/enabling-chat-completions"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"SUg0qkewmUFyOvq0j3PWqUR91HA0Lk39kRg0+GtfSj4=","text":"Enabling the chat completions APISetting a conversation config GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationStyles alpacachatmlllama chatllama infillmistral chatpassthrough  By default text models only have the completions API enabled To enable the chat completions API the model must configure a conversation config which tells the API how to format the prompt from the list of messages The conversation config is set in your model s fireworks json and is immutable after the model is uploaded It has one required field style which configures the conversation style See below for a list of valid styles Here is an example configuration for a Llama chat model messages must contain exactly one message from the user role The generated prompt will be This style uses the ChatML system prompt You are a helpful assistant The completion prompt template for the following messages will be The messages must adhere to the ChatML format as specified here The first message must be a system prompt and the last message must be a user prompt User and assistant prompts must interleave each other By default this style uses the default LLaMA system prompt You are a helpful respectful and honest assistant Always answer as helpfully as possible while being safe Your answers should not include any harmful unethical racist sexist toxic dangerous or illegal content Please ensure that your responses are socially unbiased and positive in nature A custom possibly empty system prompt may be specified in the first message in the request by setting the role to system The first non system message must be from the user role then alternate with the assistant role and end with the user role See generation py for a reference implementation messages must contain exactly two messages the first from the prefix role and the second from the suffix role The generated prompt will be See https huggingface co mistralai Mistral 7B Instruct v0 1 instruction format messages must contain exactly one message whose content is passed to the model directly role and name are ignored Updated about 1 month ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"97a85191-e7c9-4528-84ad-8b1542419e7f":{"id_":"97a85191-e7c9-4528-84ad-8b1542419e7f","metadata":{"url":"https://python.langchain.com/docs/integrations/providers/fireworks"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"5eca59ea-4f94-4f41-a927-5b363d79dfe4","metadata":{"url":"https://python.langchain.com/docs/integrations/providers/fireworks"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"xWaEScTl+Pf6uQUjPiM62+XDmJwexn6oaKx2IvR6F4Y=","text":"Fireworks Installation and setup Authentication Using the Fireworks LLM module   This page covers how to use Fireworks models withinLangchain Install the Fireworks integration package Get a Fireworks API key by signing up at fireworks ai Authenticate by setting the FIREWORKS_API_KEY environment variable There are two ways to authenticate using your Fireworks API key Setting the FIREWORKS_API_KEY environment variable Setting fireworks_api_key field in the Fireworks LLM module Fireworks integrates with Langchain through the LLM module In this example wewill work the mixtral 8x7b instruct model For a more detailed walkthrough see here","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"878fe730-d424-437f-a9e8-b2bbd85c7272":{"id_":"878fe730-d424-437f-a9e8-b2bbd85c7272","metadata":{"url":"https://readme.fireworks.ai/docs/setting-up-sso"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"eefc81b0-c9f9-4da5-88d4-7ad048a9f44b","metadata":{"url":"https://readme.fireworks.ai/docs/setting-up-sso"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"Dh1AxA1aReeoXK/T9Jri/dAdum6mrOhkNeXikTozIx0=","text":"Setting up SSOGoogle WorkspaceOpenID Connect OIDC providerSAML 2 0 provider GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks uses single sign on SSO as the primary mechanism to authenticate with the platform Coordinate with your Fireworks ai representative to enable the integration Fireworks supports the following SSO implementations If your organization utilizes Google Workspace identity management Fireworks ai is pre registered as an OIDC application with Google Please provide the Fireworks ai representative with the domain name of your organization Updated 4 months ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"3c1b5f97-a91d-49fb-9cd7-4e43bcb9b376":{"id_":"3c1b5f97-a91d-49fb-9cd7-4e43bcb9b376","metadata":{"url":"https://readme.fireworks.ai/reference"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"cbe331eb-013c-4a3c-b918-b34d4899bfb5","metadata":{"url":"https://readme.fireworks.ai/reference"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"sAuO7BvqjPQWVXOhHAV+RzGpOs82czCcJJ1ecpLaKK8=","text":"POST chat completions Fireworks REST APIPython Client LibraryPeFT AddonFirectl CLI ReferenceFireworks REST APIPython Client LibraryPeFT AddonFirectl CLI Reference   Creates a model response for the given chat conversation","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"beb28723-c4ed-49ad-a0c9-8042ba29fa91":{"id_":"beb28723-c4ed-49ad-a0c9-8042ba29fa91","metadata":{"url":"https://readme.fireworks.ai/reference"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"a6db588d-8c1c-4e9f-94fb-b93a8c20af20","metadata":{"url":"https://readme.fireworks.ai/reference"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"sAuO7BvqjPQWVXOhHAV+RzGpOs82czCcJJ1ecpLaKK8=","text":"POST chat completions Fireworks REST APIPython Client LibraryPeFT AddonFirectl CLI ReferenceFireworks REST APIPython Client LibraryPeFT AddonFirectl CLI Reference   Creates a model response for the given chat conversation","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"93f90fc2-df27-4073-959e-1e6d10f2a7b2":{"id_":"93f90fc2-df27-4073-959e-1e6d10f2a7b2","metadata":{"url":"https://readme.fireworks.ai/docs/openai-compatibility"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"b56077a1-bc70-4676-a292-1d381fa4616b","metadata":{"url":"https://readme.fireworks.ai/docs/openai-compatibility"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"dWTRc4s5nY9KQPVK1yW99kgHTxX9oRvtnzsQvPOvmZ0=","text":"OpenAI compatibilityOpenAI Python client library GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationSpecify endpoint and API keyUsageAPI compatibility CompletionChat CompletionDifferencesToken usage for streaming responsesNot supported options  The Fireworks ai LLM API is OpenAI compatible You can use OpenAI Python client library to interact with Fireworks This makes migration of existing applications already using OpenAI particularly easy You can override parameters for the entire application using environment variables or by setting these values in Python Alternatively you may specify these parameters for a single request useful if you mix calls to OpenAI and Fireworks in the same process Use OpenAI s SDK how you d normally would Just ensure that the model parameter refers to one of Fireworks models Simple completion API that doesn t modify provided prompt in any way Works best for models fine tuned for conversation e g llama chat variants The following options have minor differences OpenAI API returns usage stats number of tokens in prompt and completion for non streaming responses but doesn t for the streaming ones see forum post Fireworks ai returns usage stats in both cases For streaming responses the usage field is returned in the very last chunk on the response i e the one having finish_reason set For example Note that if you re using OpenAI SDK they usage field won t be listed in the SDK s structure definition But it can be accessed directly For example The following options are not yet supported Please reach out to us on Discord if you have a use case requiring one of these Updated 2 months ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"4c74392b-4197-46ea-911f-f418a4fe34ef":{"id_":"4c74392b-4197-46ea-911f-f418a4fe34ef","metadata":{"url":"https://readme.fireworks.ai/docs/structured-response-formatting"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"bfb8fcb6-22af-4c4a-ab42-b7d5db989e30","metadata":{"url":"https://readme.fireworks.ai/docs/structured-response-formatting"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"BZ0gve+OW1XAeEQLKwpQFWErdb6p0XKbBobQCt5oWxU=","text":"JSON mode GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationWhat is JSON mode Why JSON responses End to end exampleStructured response modesSimilar features PrerequisitesStep 1 Import librariesStep 2 Configure the Fireworks ai clientStep 3 Define the output schemaStep 4 Specify your output schema in your chat completions requestStep 5 Display the resultJSON schema constructs  JSON mode enables you to provide a JSON schema to force any Fireworks language model to respond in Clarity and Precision Responding in JSON ensures that the output from the LLM is clear precise and easy to parse This is particularly beneficial in scenarios where the response needs to be further processed or analyzed by other systems Ease of Integration JSON being a widely used format allows for easy integration with various platforms and applications This interoperability is essential for developers looking to incorporate AI capabilities into their existing systems without extensive modifications This guide provides a step by step example of how to create a structured output response using the Fireworks ai API The example uses Python and the pydantic library to define the schema for the output Before you begin ensure you have the following Python installed on your system openai and pydantic libraries installed You can install them using pip Next select the model you want to use In this example we use mixtral 8x7b instruct but all fireworks models support this feature You can find your favorite model and get a JSON response out of it Start by importing the required libraries You can use either Fireworks ai or OpenAI SDK with this feature Using OpenAI SDK with your API key and the base URL Replace Your_API_Key with your actual API key Define a Pydantic model to specify the schema of the output For example This model defines a simple schema with a single field winner If you are not familiar with pydantic please check the documentation here Pydantic emits JSON Schema and you can find more informations about it here Make a request to the Fireworks ai API to get a JSON response In your request specify the output schema you used in step 3 For example to ask who won the US presidential election in 2012 Finally print the result This will display the response in the format defined by the Result schema We get just one nice json response And you can parse that as a plain JSON and hook it up with the rest of your system Current we enforce a structure with a grammar based state machine to make sure that the LLMs would always generate all the fields in the schema If your provided output schema is not a valid json schema we will fail the response Fireworks support the following variants Important when using JSON mode it s crucial also to instruct the model to produce JSON and describe the desired schema via a system or user message Without this the model may generate an unending stream of whitespace until the generation reaches the token limit resulting in a long running and seemingly stuck request To get the best outcome you need to include the schema in both the prompt and the schema Technically it means that when using JSON with the given schema mode the model doesn t automatically see the schema passed in the response_format field Adherence to the schema is forced upon the model during sampling So for best results you need to include the desired schema in the prompt in addition to specifying it as response_format You may need to experiment with the best way to describe the schema in the prompt depending on the model besides JSON schema describing it in plain English might work well too e g extract name and address of the person in JSON format Note that the message content may be partially cut off if finish_reason length which indicates the generation exceeded max_tokens or the conversation exceeded the max context length In this case the return value might not be a valid JSON Structured response modes work for both Completions and Chat Completions APIs If you use Function Calling JSON mode is enabled automatically and function schema is added to the prompt So none of the comments above apply Fireworks supports a subset of JSON schema specification Supported Fireworks API doesn t error out on unsupported constructs They just won t be enforced Not yet supported constraints include Note JSON specification allows for arbitrary field names to appear in an object with the properties constraint unless additionalProperties false or unevaluatedProperties false is provided It s a poor default for LLM constrained generation since any hallucination would be accepted Thus Fireworks treats any schema with properties constraint as if it had unevaluatedProperties false An example of response_format field with the schema accepting an object with two fields a required string and an optional integer Check out our function calling model if you re interested in use cases like Check out grammar mode if you want structured output specified not through JSON but rather through an arbitrary grammar limit output to specific words character limits or character types etc Updated 3 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"1f1fc105-555d-4905-8d69-9ba378658215":{"id_":"1f1fc105-555d-4905-8d69-9ba378658215","metadata":{"url":"https://app.fireworks.ai"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"3994219d-204f-4c3d-bb04-883bcc5a813d","metadata":{"url":"https://app.fireworks.ai"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"4KHGnSl1z7wWRlkj9a0kQHG7ugbRfkwckTGWMhU2AxA=","text":"The production AI platform built for developers Models curated and optimized by FireworksThe fastest and most uncompromising AI platform Level up with Fireworks AI Enterprise   Fireworks partners with the world s leading generative AI researchers to serve the best models at the fastest speeds Companies of all sizes trust Fireworks to power their production AI use cases Mixtral MoE 8x7B Instruct Mistral MoE 8x7B Instruct v0 1 model with Sparse Mixture of Experts Fine tuned for instruction following FireFunction V1 Fireworks open source function calling model Llama 2 70B Chat A fine tuned version of Llama 2 70B optimized for dialogue applications using Reinforcement Learning from Human Feedback RLHF and perform comparably to ChatGPT according to human evaluations Mistral 7B Instruct The Mistral 7B Instruct v0 1 Large Language Model LLM is a instruct fine tuned version of the Mistral 7B v0 1 generative text model using a variety of publicly available conversation datasets Industry Leading Performance Independently benchmarked to have the top speed of all inference providers Enterprise Scale Throughput FireLLaVA the first commercially permissive OSS LLaVA model State of the art Models Use powerful models curated by Fireworks or our in house trained multi modal and function calling models Battle Tested for Reliability Built for Developers Our OpenAI compatible API makes it easy to start building with Fireworks Get dedicated deployments for your models to ensure uptime and speed Fireworks is proudly compliant with HIPAA and SOC2 and offers secure VPC and VPN connectivity Meet your needs with data privacy own your data and your models 2024 Fireworks AI All rights reserved Pages Company Legal","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"0e7acf09-ecb8-45bd-8420-bc2b2400f35e":{"id_":"0e7acf09-ecb8-45bd-8420-bc2b2400f35e","metadata":{"url":"https://readme.fireworks.ai/docs/fine-tuning"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"0dca1bad-c566-4221-a72e-9379222e2ebb","metadata":{"url":"https://readme.fireworks.ai/docs/fine-tuning"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"qJql8L00RuWivzougcSLByj5XyzHEi6blKogEsoGktE=","text":"LLM recipes GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationTuningGenerationModel evaluationSetting up your environment   We provide a collection of recipes that simplify the process of model fine tuning and evaluation Individual recipes and workflows combining them into end to end model development flows are available in the Fireworks Cookbook repository Each recipe comes with dedicated documentation providing more in depth information Recipes are categorized according to their respective use cases Here is the list of currently supported use cases See Docker container for LLM development Updated 4 months ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"d9ca8673-c7d1-478a-8ddb-8dcf8cd2f665":{"id_":"d9ca8673-c7d1-478a-8ddb-8dcf8cd2f665","metadata":{"url":"https://readme.fireworks.ai/docs/model-upload"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"802b62f1-ebbb-47b8-a268-1943d32ef4f0","metadata":{"url":"https://readme.fireworks.ai/docs/model-upload"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"ydv8n2IC7X2Vp2WfrgBXxETm+5btsKw+9amh9OGL1hI=","text":"Deploying fine tuned modelsInstalling firectlSigning inUploading a fine tuned modelDeploying your modelTesting your modelUsing the APICleaning upDeployment limits GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   This guide will walk you through uploading your own fine tuned model The firectl command line interface CLI will be used to manage your LLMmodels Run the following command to sign into Fireworks Confirm that you have successfully signed in by listing your account You should see your account ID Make sure to review the requirements for a fine tuned model Sample configs for supported models are available here To upload a fine tuned model located at tmp falcon 7b addon run Once uploaded you can see your model with To deploy the model for inference run Once your model is deployed you can query it on the model page You should see your model s response streamed below You can also directly query the model using the v1 completions API Now that you are finished with the guide you can undeploy the models to avoid accruing charges on your account You can also delete the model from your account Non enterprise accounts are limited to a maximum of 100 deployed models Updated 28 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"be77356c-2786-4ac7-a5a1-e8ed759feb16":{"id_":"be77356c-2786-4ac7-a5a1-e8ed759feb16","metadata":{"url":"https://readme.fireworks.ai/docs/fine-tuning-models"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"fb324d78-9f41-4718-a0be-5c88e447a59c","metadata":{"url":"https://readme.fireworks.ai/docs/fine-tuning-models"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"FWiNDei8ltzDfUt60VFe32GqKlVQFIT7ttKKX3hRHVE=","text":"Fine tuning modelsIntroInstalling firectlSigning inPreparing your datasetPreparing your fine tuning job settingsKick off fine tuning jobDeploy for inference Clean upSupported modelsHuggingFace to JSONL GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationExample text completionclassification DatasetSetting Up Fine TuningDatasetSetting Up Fine Tuning The guide will walk you through the steps to fine tune a Fireworks supported base model The fine tuning service is currently in its pre alpha phase and is 100 free to use Please give it a try and share your feedback in the fine tuning Discord channel We utilize LoRA Low Rank Adaptation for efficient and effective fine tuning of large language models Take advantage of this opportunity to enhance your models with our cutting edge technology Fine tuning a model with a targeted dataset is crucial for several key reasons In essence fine tuning a model with a specific dataset is a pivotal step in ensuring its enhanced accuracy relevance and suitability for specific applications Let s hop on a journey of fine tuning a model The firectl command line interface CLI will be used to manage your LLM models Run the following command to sign into Fireworks Confirm that you have successfully signed in by listing your account You should see your account ID To fine tune a model we need to first upload a dataset Once uploaded this dataset can be used to create one or more fine tuning jobs A dataset consists of a single JSONL file where each line is a separate training example Limits To create dataset named by the identifier and upload the files and you can check the dataset with To use an existing HuggingFace dataset please refer to the script below for conversion Datasets are private and cannot be viewed by other accounts To kick off a fine tuning job you need to specify the settings and save it locally in a new directory Define a yaml file with following fields For this example we ll use databricks databricks dolly 15k dataset focused on instruction following Each record in this jsonl dataset consists of a category instruction an optional context and the expected response Here are a couple of sample records Next to set up how you d like to use the dataset for fine tuning the text completion tuning setting should be configured with following fields A typical settings yaml would look like Now let s try a different scenario to tune the model for multi class classification specifically We will utilize the symptom_to_diagnosis dataset available on Hugging Face This dataset is structured to associate medical symptoms with potential diagnoses Records in the dataset might look like this To fine tune a model for classifying symptoms to diagnoses configure your settings yaml file as follows A typical settings yaml for this dataset would be You can kick off a fine tuning job via firectl create fine tuning job command by passing in the settings yaml and dataset by following the example below Optionally you can also pass in a unique model id via flag to override the model name If not specified model will be named by default as after the kick off you can check the status of the recipe job with after the fine tuning is finished the tuned model will be uploaded directly and you can see the model via by default the created model is in undeployed state To further deploy or delete the model please refer to the deploy fine tuned models guide If you d like to use a dataset from huggingface in our fine tuning service the code snippet below should convert any dataset to a JSONL file Updated 2 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"352ea893-526a-468b-8268-617b39adefb3":{"id_":"352ea893-526a-468b-8268-617b39adefb3","metadata":{"url":"https://readme.fireworks.ai/docs/function-calling"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"9541def6-b62c-499d-b64b-9c61da230613","metadata":{"url":"https://readme.fireworks.ai/docs/function-calling"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"xz+XxOWSAHvnloTy5ObGUhJuFqzTd7DoJzPiEUGu2EI=","text":"Function callingIntroductionResourcesSupported ModelsExample UsageTools SpecificationTool ChoiceOpenAI CompatibilityBest PracticesFunction Calling vs JSON modeExample AppsData Policy GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Function Calling API allows a user to describe the set of tools functions available to the model and have the model intelligently choose the right set of function calls to invoke given the context This functionality allows users to build dynamic agents that can get access to real time information produce structured outputs The Function Calling API doesn t invoke any function calls instead it generates the tool calls to make in OpenAI compatible format On a high level function calling works by TL DR This example tutorial is available as a python notebook code Colab For this example let s consider a user looking for Nike s financial data We will provide the model with a tool that the model is allowed to invoke get access to the financial information of any company This results in the following response tools field is an array and each individual component contains the following two fields tool_choice parameter control whether the model is allowed to call functions or not Currently we support values auto none any or a specific function name Our function calling API is fully compatible with OpenAI including in streaming scenarios Compared to the OpenAI API we don t support parallel and nested function calling When to use function calling vs JSON mode Use function calling if the use case involves decision making or interactivity which function to call what parameters to fill or how to follow up with the user to fill in missing function arguments in a chat form JSON Grammar mode is a preferred choice for non interactive structured data extraction and allows you to explore non JSON formats too Firefunction v1 is in a free beta phase Data from Firefunction is logged and automatically deleted after 30 days to ensure product quality and prevent abuse This data will never be used to train models Please contact us at email protected if you have questions or comments Updated 29 minutes ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"6e107288-8fea-4ef8-8775-ebbafa2e1fe9":{"id_":"6e107288-8fea-4ef8-8775-ebbafa2e1fe9","metadata":{"url":"https://readme.fireworks.ai/docs"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"0e7bc347-bea5-4751-8cf7-baa14e85c9ff","metadata":{"url":"https://readme.fireworks.ai/docs"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"7XOb47Yq04DTw9Yuuq4FDddF7cuf0XI+nZTIpvUbxV0=","text":"Quickstart GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks ai is a lightning fast inference platform that helps you serve generative AI models Check out the following guides to see what else you can do with Fireworks Have fun If you have any questions please reach out to us on Discord or Twitter Updated 18 days ago","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"fe187289-1996-4748-9b8a-f9e94fd96637":{"id_":"fe187289-1996-4748-9b8a-f9e94fd96637","metadata":{"url":"https://readme.fireworks.ai/page/application-status"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"bb02705e-2bc0-483a-a096-01833500ed83","metadata":{"url":"https://readme.fireworks.ai/page/application-status"},"hash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="}},"hash":"5ir4E5M2r87yNAXK2u/mEeaA3FnzKZw718H3O0/Nubw=","text":"Application Status    The application is currently running smoothly and all systems are operational If you experience any issues or have any questions please contact us at email protected or open a bug report on Discord","textTemplate":"","metadataSeparator":"\n","type":"TEXT"}},"type":"simple_dict"}}}