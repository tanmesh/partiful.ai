{"text":"Benchmarking OpenAI Retrieval API through Assistant Agent Setup Data Define Eval Modules Construct Assistant with Built In Retrieval Benchmark Setup Golden Dataset Eval Modules Define Baseline Index RAG Pipeline Run Evals over Baseline Run Evals over Assistant API Get Results Option 1 Pull Existing Dataset Option 2 Generate New Dataset Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  This guide benchmarks the Retrieval Tool from the OpenAI Assistant API by using our OpenAIAssistantAgent We run over the Llama 2 paper and compare generation quality against a naive RAG pipeline Here we load the Llama 2 paper and chunk it We setup evaluation modules including the dataset and evaluators Here we load in a golden dataset NOTE We pull this in from Dropbox For details on how to generate a dataset please see our DatasetGenerator module If you choose this option you can choose to generate a new dataset from scratch This allows you to play around with our DatasetGenerator settings to make sure it suits your needs We define two evaluation modules correctness and semantic similarity both comparing quality of predicted response with actual response Let s construct the assistant by also passing it the built in OpenAI Retrieval tool Here we upload and pass in the file during assistant creation time We run the agent over our evaluation dataset We benchmark against a standard top k RAG pipeline k 2 with gpt 4 turbo NOTE During our time of testing November 2023 the Assistant API is heavily rate limited and can take 1 2 hours to generate responses over 60 datapoints Here we see that our basic RAG pipeline does better Take these numbers with a grain of salt The goal here is to give you a script so you can run this on your own data That said it s surprising the Retrieval API doesn t give immediately better out of the box performance","link":"https://docs.llamaindex.ai/en/stable/examples/agent/openai_retrieval_benchmark.html"}