{"text":"Querying embedding modelsEmbedding documentsQuerying documentsList of available models GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentation   Fireworks hosts one embedding model the E5 Mistral model Specifically we host the intfloat e5 mistral 7b instruct variant a highly adaptable language model that s currently the state of the art model on the Hugging Face leaderboard It has 32 layers and an embedding size of 4096 making it well suited for complex embedding tasks Our embeddings service is OpenAI compatible Use OpenAI s embeddings guide and OpenAI s embeddings documentation for more detailed information on our embedding model usage The embedding model inputs text and outputs a vector list of floating point numbers to use for tasks like similarity comparisons and search This code embeds the text Spiderman was a particularly entertaining movie with and returns the following Unlike most embedding models the E5 Mistral model we use is unique because it allows users to give instructions with their query This enables users to use the same document store for multiple tasks with higher accuracy Lets say I previously used the embedding model to embed many movie reviews that I stored in a vector database I now want to create a movie recommender that takes in a user query and outputs recommendations based on this data The code below demonstrates how to embed the user query and system prompt To view this example end to end and see how to use a MongoDB vector store and Fireworks hosted generation model for RAG see our full guide Updated 4 days ago","link":"https://readme.fireworks.ai/docs/querying-embeddings-models"}