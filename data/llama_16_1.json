{"text":"Customization Tutorial    Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes Tip If you haven t already install LlamaIndex and complete the starter tutorial If you run into terms you don t recognize check out the high level concepts In this tutorial we start with the code you wrote for the starter example and show you the most common ways you might want to customize it for your use case I want to parse my documents into smaller chunks I want to use a different vector store First you can install the vector store you want to use For example to use chromadb as the vector store you can install it using pip To learn more about all integrations available checkout LlamaHub https llamahub ai _ Then you can use it in your code StorageContext defines the storage backend for where the documents embeddings and indexes are stored You can learn more about storage and how to customize it I want to retrieve more context when I query as_query_engine builds a default retriever and query engine on top of the index You can configure the retriever and query engine by passing in keyword arguments Here we configure the retriever to return the top 5 most similar documents instead of the default of 2 You can learn more about retrievers and query engines I want to use a different LLM You can learn more about customizing LLMs I want to use a different response mode You can learn more about query engines and response modes I want to stream the response back You can learn more about streaming responses I want a chatbot instead of Q A Learn more about the chat engine Next Steps want a thorough walkthrough of almost everything you can configure Get started with Understanding LlamaIndex want more in depth understanding of specific modules Check out the module guides in the left nav","link":"https://docs.llamaindex.ai/en/stable/getting_started/customization.html"}