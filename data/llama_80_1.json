{"text":"Query Engine with Pydantic Outputs Setup Create the Index Query Engine OpenAI Create the Index Query Engine Non OpenAI Beta Accumulate Examples Beta Create our Pydanitc Output Object  Getting Started Use Cases Understanding Optimizing Module Guides API Reference Community Contributing Changes  Every query engine has support for integrated structured responses using the following response_modes in RetrieverQueryEngine refine compact tree_summarize accumulate beta requires extra parsing to convert to objects compact_accumulate beta requires extra parsing to convert to objects In this notebook we walk through a small example demonstrating the usage Under the hood every LLM response will be a pydantic object If that response needs to be refined or summarized it is converted into a JSON string for the next response Then the final response is returned as a pydantic object NOTE This can technically work with any LLM but non openai is support is still in development and considered beta If you re opening this Notebook on colab you will probably need to install LlamaIndex Download Data When using OpenAI the function calling API will be leveraged for reliable structured outputs When using an LLM that does not support function calling we rely on the LLM to write the JSON itself and we parse the JSON into the proper pydantic object Accumulate with pydantic objects requires some extra parsing This is still a beta feature but it s still possible to get accumulate pydantic objects In accumulate responses are separated by a default separator and prepended with a prefix","link":"https://docs.llamaindex.ai/en/stable/examples/query_engine/pydantic_query_engine.html"}