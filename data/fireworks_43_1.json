{"text":"OpenAI compatibilityOpenAI Python client library GuidesSTRUCTURED RESPONSESEnterprise GuidesDocumentationSpecify endpoint and API keyUsageAPI compatibility CompletionChat CompletionDifferencesToken usage for streaming responsesNot supported options  The Fireworks ai LLM API is OpenAI compatible You can use OpenAI Python client library to interact with Fireworks This makes migration of existing applications already using OpenAI particularly easy You can override parameters for the entire application using environment variables or by setting these values in Python Alternatively you may specify these parameters for a single request useful if you mix calls to OpenAI and Fireworks in the same process Use OpenAI s SDK how you d normally would Just ensure that the model parameter refers to one of Fireworks models Simple completion API that doesn t modify provided prompt in any way Works best for models fine tuned for conversation e g llama chat variants The following options have minor differences OpenAI API returns usage stats number of tokens in prompt and completion for non streaming responses but doesn t for the streaming ones see forum post Fireworks ai returns usage stats in both cases For streaming responses the usage field is returned in the very last chunk on the response i e the one having finish_reason set For example Note that if you re using OpenAI SDK they usage field won t be listed in the SDK s structure definition But it can be accessed directly For example The following options are not yet supported Please reach out to us on Discord if you have a use case requiring one of these Updated 2 months ago","link":"https://readme.fireworks.ai/docs/openai-compatibility"}